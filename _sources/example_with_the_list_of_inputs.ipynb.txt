{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example : k times repetition with the list of k input files \n",
    "\n",
    "DeepBiome package takes microbiome abundance data as input and uses the phylogenetic taxonomy to guide the decision of the optimal number of layers and neurons in the deep learning architecture.\n",
    "\n",
    "To use DeepBiome, you can experiment (1) __k times repetition__ or (2) __k fold cross-validation__.\n",
    "For each experiment, we asuume that the dataset is given by\n",
    "- __A list of k input files for k times repetition.__\n",
    "- __One input file for k fold cross-validation.__\n",
    "\n",
    "This notebook contains an example of (1) __k times repetition__ for the deep neural netowrk using deepbiome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load library\n",
    "\n",
    "First, we have to load deepbiome package. The deepbiome package is build on the tensorflow and keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "from pkg_resources import resource_filename\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deepbiome import deepbiome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "\n",
    "In this example, we assume that we have __a list of k input files for k times repetition.__\n",
    "\n",
    "DeepBiome needs 4 data files as follows:\n",
    "1. __the tree information__\n",
    "1. __the lists of the input files__ (each file has all sample's information for one repetition)\n",
    "1. __the list of the names of input files__ \n",
    "1. __y__\n",
    "\n",
    "In addition, we can set __the training index for each repetition__. If we set the index file, DeepBiome builds the training set for each repetition based on each fold index in the index file. If not, DeepBiome will generate the index file locally.\n",
    "\n",
    "\n",
    "Eath data should have the csv format. Below is the example of each file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the tree information\n",
    "\n",
    "First we need a file about the phylogenetic tree information. This tree information file should have the format below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genus</th>\n",
       "      <th>Family</th>\n",
       "      <th>Order</th>\n",
       "      <th>Class</th>\n",
       "      <th>Phylum</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Streptococcus</td>\n",
       "      <td>Streptococcaceae</td>\n",
       "      <td>Lactobacillales</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tropheryma</td>\n",
       "      <td>Cellulomonadaceae</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Veillonella</td>\n",
       "      <td>Veillonellaceae</td>\n",
       "      <td>Selenomonadales</td>\n",
       "      <td>Negativicutes</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actinomyces</td>\n",
       "      <td>Actinomycetaceae</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flavobacterium</td>\n",
       "      <td>Flavobacteriaceae</td>\n",
       "      <td>Flavobacteriales</td>\n",
       "      <td>Flavobacteria</td>\n",
       "      <td>Bacteroidetes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Prevotella</td>\n",
       "      <td>Prevotellaceae</td>\n",
       "      <td>Bacteroidales</td>\n",
       "      <td>Bacteroidia</td>\n",
       "      <td>Bacteroidetes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Porphyromonas</td>\n",
       "      <td>Porphyromonadaceae</td>\n",
       "      <td>Bacteroidales</td>\n",
       "      <td>Bacteroidia</td>\n",
       "      <td>Bacteroidetes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Parvimonas</td>\n",
       "      <td>Clostridiales_Incertae_Sedis_XI</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fusobacterium</td>\n",
       "      <td>Fusobacteriaceae</td>\n",
       "      <td>Fusobacteriales</td>\n",
       "      <td>Fusobacteria</td>\n",
       "      <td>Fusobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Propionibacterium</td>\n",
       "      <td>Propionibacteriaceae</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gemella</td>\n",
       "      <td>Bacillales_Incertae_Sedis_XI</td>\n",
       "      <td>Bacillales</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rothia</td>\n",
       "      <td>Micrococcaceae</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Granulicatella</td>\n",
       "      <td>Carnobacteriaceae</td>\n",
       "      <td>Lactobacillales</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Neisseria</td>\n",
       "      <td>Neisseriaceae</td>\n",
       "      <td>Neisseriales</td>\n",
       "      <td>Betaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lactobacillus</td>\n",
       "      <td>Lactobacillaceae</td>\n",
       "      <td>Lactobacillales</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Megasphaera</td>\n",
       "      <td>Veillonellaceae</td>\n",
       "      <td>Selenomonadales</td>\n",
       "      <td>Negativicutes</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Catonella</td>\n",
       "      <td>Lachnospiraceae</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Atopobium</td>\n",
       "      <td>Coriobacteriaceae</td>\n",
       "      <td>Coriobacteriales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Campylobacter</td>\n",
       "      <td>Campylobacteraceae</td>\n",
       "      <td>Campylobacterales</td>\n",
       "      <td>Epsilonproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Capnocytophaga</td>\n",
       "      <td>Flavobacteriaceae</td>\n",
       "      <td>Flavobacteriales</td>\n",
       "      <td>Flavobacteria</td>\n",
       "      <td>Bacteroidetes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Solobacterium</td>\n",
       "      <td>Erysipelotrichaceae</td>\n",
       "      <td>Erysipelotrichales</td>\n",
       "      <td>Erysipelotrichia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Moryella</td>\n",
       "      <td>Lachnospiraceae</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TM7_genera_incertae_sedis</td>\n",
       "      <td>TM7_genera_incertae_sedis</td>\n",
       "      <td>TM7_genera_incertae_sedis</td>\n",
       "      <td>TM7_genera_incertae_sedis</td>\n",
       "      <td>TM7</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Staphylococcus</td>\n",
       "      <td>Staphylococcaceae</td>\n",
       "      <td>Bacillales</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Filifactor</td>\n",
       "      <td>Peptostreptococcaceae</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Oribacterium</td>\n",
       "      <td>Lachnospiraceae</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Burkholderia</td>\n",
       "      <td>Burkholderiaceae</td>\n",
       "      <td>Burkholderiales</td>\n",
       "      <td>Betaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sneathia</td>\n",
       "      <td>Leptotrichiaceae</td>\n",
       "      <td>Fusobacteriales</td>\n",
       "      <td>Fusobacteria</td>\n",
       "      <td>Fusobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Treponema</td>\n",
       "      <td>Spirochaetaceae</td>\n",
       "      <td>Spirochaetales</td>\n",
       "      <td>Spirochaetes</td>\n",
       "      <td>Spirochaetes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Moraxella</td>\n",
       "      <td>Moraxellaceae</td>\n",
       "      <td>Pseudomonadales</td>\n",
       "      <td>Gammaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Haemophilus</td>\n",
       "      <td>Pasteurellaceae</td>\n",
       "      <td>Pasteurellales</td>\n",
       "      <td>Gammaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Selenomonas</td>\n",
       "      <td>Veillonellaceae</td>\n",
       "      <td>Selenomonadales</td>\n",
       "      <td>Negativicutes</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Corynebacterium</td>\n",
       "      <td>Corynebacteriaceae</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Rhizobium</td>\n",
       "      <td>Rhizobiaceae</td>\n",
       "      <td>Rhizobiales</td>\n",
       "      <td>Alphaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Bradyrhizobium</td>\n",
       "      <td>Bradyrhizobiaceae</td>\n",
       "      <td>Rhizobiales</td>\n",
       "      <td>Alphaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Methylobacterium</td>\n",
       "      <td>Methylobacteriaceae</td>\n",
       "      <td>Rhizobiales</td>\n",
       "      <td>Alphaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>OD1_genera_incertae_sedis</td>\n",
       "      <td>OD1_genera_incertae_sedis</td>\n",
       "      <td>OD1_genera_incertae_sedis</td>\n",
       "      <td>OD1_genera_incertae_sedis</td>\n",
       "      <td>OD1</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Finegoldia</td>\n",
       "      <td>Clostridiales_Incertae_Sedis_XI</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Microbacterium</td>\n",
       "      <td>Microbacteriaceae</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Sphingomonas</td>\n",
       "      <td>Sphingomonadaceae</td>\n",
       "      <td>Sphingomonadales</td>\n",
       "      <td>Alphaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Chryseobacterium</td>\n",
       "      <td>Flavobacteriaceae</td>\n",
       "      <td>Flavobacteriales</td>\n",
       "      <td>Flavobacteria</td>\n",
       "      <td>Bacteroidetes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Bacteroides</td>\n",
       "      <td>Bacteroidaceae</td>\n",
       "      <td>Bacteroidales</td>\n",
       "      <td>Bacteroidia</td>\n",
       "      <td>Bacteroidetes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Bdellovibrio</td>\n",
       "      <td>Bdellovibrionaceae</td>\n",
       "      <td>Bdellovibrionales</td>\n",
       "      <td>Deltaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Streptophyta</td>\n",
       "      <td>Chloroplast</td>\n",
       "      <td>Chloroplast</td>\n",
       "      <td>Chloroplast</td>\n",
       "      <td>Cyanobacteria_Chloroplast</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Lachnospiracea_incertae_sedis</td>\n",
       "      <td>Lachnospiraceae</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Paracoccus</td>\n",
       "      <td>Rhodobacteraceae</td>\n",
       "      <td>Rhodobacterales</td>\n",
       "      <td>Alphaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Fastidiosipila</td>\n",
       "      <td>Ruminococcaceae</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Pseudonocardia</td>\n",
       "      <td>Pseudonocardiaceae</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Genus                           Family  \\\n",
       "0                   Streptococcus                 Streptococcaceae   \n",
       "1                      Tropheryma                Cellulomonadaceae   \n",
       "2                     Veillonella                  Veillonellaceae   \n",
       "3                     Actinomyces                 Actinomycetaceae   \n",
       "4                  Flavobacterium                Flavobacteriaceae   \n",
       "5                      Prevotella                   Prevotellaceae   \n",
       "6                   Porphyromonas               Porphyromonadaceae   \n",
       "7                      Parvimonas  Clostridiales_Incertae_Sedis_XI   \n",
       "8                   Fusobacterium                 Fusobacteriaceae   \n",
       "9               Propionibacterium             Propionibacteriaceae   \n",
       "10                        Gemella     Bacillales_Incertae_Sedis_XI   \n",
       "11                         Rothia                   Micrococcaceae   \n",
       "12                 Granulicatella                Carnobacteriaceae   \n",
       "13                      Neisseria                    Neisseriaceae   \n",
       "14                  Lactobacillus                 Lactobacillaceae   \n",
       "15                    Megasphaera                  Veillonellaceae   \n",
       "16                      Catonella                  Lachnospiraceae   \n",
       "17                      Atopobium                Coriobacteriaceae   \n",
       "18                  Campylobacter               Campylobacteraceae   \n",
       "19                 Capnocytophaga                Flavobacteriaceae   \n",
       "20                  Solobacterium              Erysipelotrichaceae   \n",
       "21                       Moryella                  Lachnospiraceae   \n",
       "22      TM7_genera_incertae_sedis        TM7_genera_incertae_sedis   \n",
       "23                 Staphylococcus                Staphylococcaceae   \n",
       "24                     Filifactor            Peptostreptococcaceae   \n",
       "25                   Oribacterium                  Lachnospiraceae   \n",
       "26                   Burkholderia                 Burkholderiaceae   \n",
       "27                       Sneathia                 Leptotrichiaceae   \n",
       "28                      Treponema                  Spirochaetaceae   \n",
       "29                      Moraxella                    Moraxellaceae   \n",
       "30                    Haemophilus                  Pasteurellaceae   \n",
       "31                    Selenomonas                  Veillonellaceae   \n",
       "32                Corynebacterium               Corynebacteriaceae   \n",
       "33                      Rhizobium                     Rhizobiaceae   \n",
       "34                 Bradyrhizobium                Bradyrhizobiaceae   \n",
       "35               Methylobacterium              Methylobacteriaceae   \n",
       "36      OD1_genera_incertae_sedis        OD1_genera_incertae_sedis   \n",
       "37                     Finegoldia  Clostridiales_Incertae_Sedis_XI   \n",
       "38                 Microbacterium                Microbacteriaceae   \n",
       "39                   Sphingomonas                Sphingomonadaceae   \n",
       "40               Chryseobacterium                Flavobacteriaceae   \n",
       "41                    Bacteroides                   Bacteroidaceae   \n",
       "42                   Bdellovibrio               Bdellovibrionaceae   \n",
       "43                   Streptophyta                      Chloroplast   \n",
       "44  Lachnospiracea_incertae_sedis                  Lachnospiraceae   \n",
       "45                     Paracoccus                 Rhodobacteraceae   \n",
       "46                 Fastidiosipila                  Ruminococcaceae   \n",
       "47                 Pseudonocardia               Pseudonocardiaceae   \n",
       "\n",
       "                        Order                      Class  \\\n",
       "0             Lactobacillales                    Bacilli   \n",
       "1             Actinomycetales             Actinobacteria   \n",
       "2             Selenomonadales              Negativicutes   \n",
       "3             Actinomycetales             Actinobacteria   \n",
       "4            Flavobacteriales              Flavobacteria   \n",
       "5               Bacteroidales                Bacteroidia   \n",
       "6               Bacteroidales                Bacteroidia   \n",
       "7               Clostridiales                 Clostridia   \n",
       "8             Fusobacteriales               Fusobacteria   \n",
       "9             Actinomycetales             Actinobacteria   \n",
       "10                 Bacillales                    Bacilli   \n",
       "11            Actinomycetales             Actinobacteria   \n",
       "12            Lactobacillales                    Bacilli   \n",
       "13               Neisseriales         Betaproteobacteria   \n",
       "14            Lactobacillales                    Bacilli   \n",
       "15            Selenomonadales              Negativicutes   \n",
       "16              Clostridiales                 Clostridia   \n",
       "17           Coriobacteriales             Actinobacteria   \n",
       "18          Campylobacterales      Epsilonproteobacteria   \n",
       "19           Flavobacteriales              Flavobacteria   \n",
       "20         Erysipelotrichales           Erysipelotrichia   \n",
       "21              Clostridiales                 Clostridia   \n",
       "22  TM7_genera_incertae_sedis  TM7_genera_incertae_sedis   \n",
       "23                 Bacillales                    Bacilli   \n",
       "24              Clostridiales                 Clostridia   \n",
       "25              Clostridiales                 Clostridia   \n",
       "26            Burkholderiales         Betaproteobacteria   \n",
       "27            Fusobacteriales               Fusobacteria   \n",
       "28             Spirochaetales               Spirochaetes   \n",
       "29            Pseudomonadales        Gammaproteobacteria   \n",
       "30             Pasteurellales        Gammaproteobacteria   \n",
       "31            Selenomonadales              Negativicutes   \n",
       "32            Actinomycetales             Actinobacteria   \n",
       "33                Rhizobiales        Alphaproteobacteria   \n",
       "34                Rhizobiales        Alphaproteobacteria   \n",
       "35                Rhizobiales        Alphaproteobacteria   \n",
       "36  OD1_genera_incertae_sedis  OD1_genera_incertae_sedis   \n",
       "37              Clostridiales                 Clostridia   \n",
       "38            Actinomycetales             Actinobacteria   \n",
       "39           Sphingomonadales        Alphaproteobacteria   \n",
       "40           Flavobacteriales              Flavobacteria   \n",
       "41              Bacteroidales                Bacteroidia   \n",
       "42          Bdellovibrionales        Deltaproteobacteria   \n",
       "43                Chloroplast                Chloroplast   \n",
       "44              Clostridiales                 Clostridia   \n",
       "45            Rhodobacterales        Alphaproteobacteria   \n",
       "46              Clostridiales                 Clostridia   \n",
       "47            Actinomycetales             Actinobacteria   \n",
       "\n",
       "                       Phylum    Domain  \n",
       "0                  Firmicutes  Bacteria  \n",
       "1              Actinobacteria  Bacteria  \n",
       "2                  Firmicutes  Bacteria  \n",
       "3              Actinobacteria  Bacteria  \n",
       "4               Bacteroidetes  Bacteria  \n",
       "5               Bacteroidetes  Bacteria  \n",
       "6               Bacteroidetes  Bacteria  \n",
       "7                  Firmicutes  Bacteria  \n",
       "8                Fusobacteria  Bacteria  \n",
       "9              Actinobacteria  Bacteria  \n",
       "10                 Firmicutes  Bacteria  \n",
       "11             Actinobacteria  Bacteria  \n",
       "12                 Firmicutes  Bacteria  \n",
       "13             Proteobacteria  Bacteria  \n",
       "14                 Firmicutes  Bacteria  \n",
       "15                 Firmicutes  Bacteria  \n",
       "16                 Firmicutes  Bacteria  \n",
       "17             Actinobacteria  Bacteria  \n",
       "18             Proteobacteria  Bacteria  \n",
       "19              Bacteroidetes  Bacteria  \n",
       "20                 Firmicutes  Bacteria  \n",
       "21                 Firmicutes  Bacteria  \n",
       "22                        TM7  Bacteria  \n",
       "23                 Firmicutes  Bacteria  \n",
       "24                 Firmicutes  Bacteria  \n",
       "25                 Firmicutes  Bacteria  \n",
       "26             Proteobacteria  Bacteria  \n",
       "27               Fusobacteria  Bacteria  \n",
       "28               Spirochaetes  Bacteria  \n",
       "29             Proteobacteria  Bacteria  \n",
       "30             Proteobacteria  Bacteria  \n",
       "31                 Firmicutes  Bacteria  \n",
       "32             Actinobacteria  Bacteria  \n",
       "33             Proteobacteria  Bacteria  \n",
       "34             Proteobacteria  Bacteria  \n",
       "35             Proteobacteria  Bacteria  \n",
       "36                        OD1  Bacteria  \n",
       "37                 Firmicutes  Bacteria  \n",
       "38             Actinobacteria  Bacteria  \n",
       "39             Proteobacteria  Bacteria  \n",
       "40              Bacteroidetes  Bacteria  \n",
       "41              Bacteroidetes  Bacteria  \n",
       "42             Proteobacteria  Bacteria  \n",
       "43  Cyanobacteria_Chloroplast  Bacteria  \n",
       "44                 Firmicutes  Bacteria  \n",
       "45             Proteobacteria  Bacteria  \n",
       "46                 Firmicutes  Bacteria  \n",
       "47             Actinobacteria  Bacteria  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_information = pd.read_csv(resource_filename('deepbiome', 'tests/data/genus48_dic.csv'))\n",
    "tree_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file has `.csv` format below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genus,Family,Order,Class,Phylum,Domain\n",
      "Streptococcus,Streptococcaceae,Lactobacillales,Bacilli,Firmicutes,Bacteria\n",
      "Tropheryma,Cellulomonadaceae,Actinomycetales,Actinobacteria,Actinobacteria,Bacteria\n",
      "Veillonella,Veillonellaceae,Selenomonadales,Negativicutes,Firmicutes,Bacteria\n",
      "Actinomyces,Actinomycetaceae,Actinomycetales,Actinobacteria,Actinobacteria,Bacteria\n",
      "Flavobacterium,Flavobacteriaceae,Flavobacteriales,Flavobacteria,Bacteroidetes,Bacteria\n",
      "Prevotella,Prevotellaceae,Bacteroidales,Bacteroidia,Bacteroidetes,Bacteria\n",
      "Porphyromonas,Porphyromonadaceae,Bacteroidales,Bacteroidia,Bacteroidetes,Bacteria\n",
      "Parvimonas,Clostridiales_Incertae_Sedis_XI,Clostridiales,Clostridia,Firmicutes,Bacteria\n",
      "Fusobacterium,Fusobacteriaceae,Fusobacteriales,Fusobacteria,Fusobacteria,Bacteria\n",
      "Propionibacterium,Propionibacteriaceae,Actinomycetales,Actinobacteria,Actinobacteria,Bacteria\n",
      "Gemella,Bacillales_Incertae_Sedis_XI,Bacillales,Bacilli,Firmicutes,Bacteria\n",
      "Rothia,Micrococcaceae,Actinomycetales,Actinobacteria,Actinobacteria,Bacteria\n",
      "Granulicatella,Carnobacteriaceae,Lactobacillales,Bacilli,Firmicutes,Bacteria\n",
      "Neisseria,Neisseriaceae,Neisseriales,Betaproteobacteria,Proteobacteria,Bacteria\n",
      "Lactobacillus,Lactobacillaceae,Lactobacillales,Bacilli,Firmicutes,Bacteria\n",
      "Megasphaera,Veillonellaceae,Selenomonadales,Negativicutes,Firmicutes,Bacteria\n",
      "Catonella,Lachnospiraceae,Clostridiales,Clostridia,Firmicutes,Bacteria\n",
      "Atopobium,Coriobacteriaceae,Coriobacteriales,Actinobacteria,Actinobacteria,Bacteria\n",
      "Campylobacter,Campylobacteraceae,Campylobacterales,Epsilonproteobacteria,Proteobacteria,Bacteria\n",
      "Capnocytophaga,Flavobacteriaceae,Flavobacteriales,Flavobacteria,Bacteroidetes,Bacteria\n",
      "Solobacterium,Erysipelotrichaceae,Erysipelotrichales,Erysipelotrichia,Firmicutes,Bacteria\n",
      "Moryella,Lachnospiraceae,Clostridiales,Clostridia,Firmicutes,Bacteria\n",
      "TM7_genera_incertae_sedis,TM7_genera_incertae_sedis,TM7_genera_incertae_sedis,TM7_genera_incertae_sedis,TM7,Bacteria\n",
      "Staphylococcus,Staphylococcaceae,Bacillales,Bacilli,Firmicutes,Bacteria\n",
      "Filifactor,Peptostreptococcaceae,Clostridiales,Clostridia,Firmicutes,Bacteria\n",
      "Oribacterium,Lachnospiraceae,Clostridiales,Clostridia,Firmicutes,Bacteria\n",
      "Burkholderia,Burkholderiaceae,Burkholderiales,Betaproteobacteria,Proteobacteria,Bacteria\n",
      "Sneathia,Leptotrichiaceae,Fusobacteriales,Fusobacteria,Fusobacteria,Bacteria\n",
      "Treponema,Spirochaetaceae,Spirochaetales,Spirochaetes,Spirochaetes,Bacteria\n",
      "Moraxella,Moraxellaceae,Pseudomonadales,Gammaproteobacteria,Proteobacteria,Bacteria\n",
      "Haemophilus,Pasteurellaceae,Pasteurellales,Gammaproteobacteria,Proteobacteria,Bacteria\n",
      "Selenomonas,Veillonellaceae,Selenomonadales,Negativicutes,Firmicutes,Bacteria\n",
      "Corynebacterium,Corynebacteriaceae,Actinomycetales,Actinobacteria,Actinobacteria,Bacteria\n",
      "Rhizobium,Rhizobiaceae,Rhizobiales,Alphaproteobacteria,Proteobacteria,Bacteria\n",
      "Bradyrhizobium,Bradyrhizobiaceae,Rhizobiales,Alphaproteobacteria,Proteobacteria,Bacteria\n",
      "Methylobacterium,Methylobacteriaceae,Rhizobiales,Alphaproteobacteria,Proteobacteria,Bacteria\n",
      "OD1_genera_incertae_sedis,OD1_genera_incertae_sedis,OD1_genera_incertae_sedis,OD1_genera_incertae_sedis,OD1,Bacteria\n",
      "Finegoldia,Clostridiales_Incertae_Sedis_XI,Clostridiales,Clostridia,Firmicutes,Bacteria\n",
      "Microbacterium,Microbacteriaceae,Actinomycetales,Actinobacteria,Actinobacteria,Bacteria\n",
      "Sphingomonas,Sphingomonadaceae,Sphingomonadales,Alphaproteobacteria,Proteobacteria,Bacteria\n",
      "Chryseobacterium,Flavobacteriaceae,Flavobacteriales,Flavobacteria,Bacteroidetes,Bacteria\n",
      "Bacteroides,Bacteroidaceae,Bacteroidales,Bacteroidia,Bacteroidetes,Bacteria\n",
      "Bdellovibrio,Bdellovibrionaceae,Bdellovibrionales,Deltaproteobacteria,Proteobacteria,Bacteria\n",
      "Streptophyta,Chloroplast,Chloroplast,Chloroplast,Cyanobacteria_Chloroplast,Bacteria\n",
      "Lachnospiracea_incertae_sedis,Lachnospiraceae,Clostridiales,Clostridia,Firmicutes,Bacteria\n",
      "Paracoccus,Rhodobacteraceae,Rhodobacterales,Alphaproteobacteria,Proteobacteria,Bacteria\n",
      "Fastidiosipila,Ruminococcaceae,Clostridiales,Clostridia,Firmicutes,Bacteria\n",
      "Pseudonocardia,Pseudonocardiaceae,Actinomycetales,Actinobacteria,Actinobacteria,Bacteria\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(resource_filename('deepbiome', 'tests/data/genus48_dic.csv')) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the list of the name of input files\n",
    "\n",
    "In this example. we assume that input is given by the lists of files. Each file has all sample's information for one repeatition.\n",
    "If we want to use the list of the input files, we need to make a list of the names of each input file. Below is an example file for `k=1000` repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gcount_0001.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gcount_0002.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gcount_0003.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gcount_0004.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gcount_0005.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "0  gcount_0001.csv\n",
       "1  gcount_0002.csv\n",
       "2  gcount_0003.csv\n",
       "3  gcount_0004.csv\n",
       "4  gcount_0005.csv"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_input_files = pd.read_csv(resource_filename('deepbiome', 'tests/data/gcount_list.csv'), header=None)\n",
    "list_of_input_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>gcount_0996.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>gcount_0997.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>gcount_0998.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>gcount_0999.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>gcount_1000.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "995  gcount_0996.csv\n",
       "996  gcount_0997.csv\n",
       "997  gcount_0998.csv\n",
       "998  gcount_0999.csv\n",
       "999  gcount_1000.csv"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_input_files.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the lists of the input files\n",
    "\n",
    "Below is an example of each input file. This example has 1000 samples as rows, and the abandunce of each microbiome as columns. Below is an example file for `k=1000` repetition. This example is `gcount_0001.csv` for the first repetition in the list of the names of input files above. This file has the 4 samples' microbiome abandunce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Streptococcus</th>\n",
       "      <th>Tropheryma</th>\n",
       "      <th>Veillonella</th>\n",
       "      <th>Actinomyces</th>\n",
       "      <th>Flavobacterium</th>\n",
       "      <th>Prevotella</th>\n",
       "      <th>Porphyromonas</th>\n",
       "      <th>Parvimonas</th>\n",
       "      <th>Fusobacterium</th>\n",
       "      <th>Propionibacterium</th>\n",
       "      <th>...</th>\n",
       "      <th>Microbacterium</th>\n",
       "      <th>Sphingomonas</th>\n",
       "      <th>Chryseobacterium</th>\n",
       "      <th>Bacteroides</th>\n",
       "      <th>Bdellovibrio</th>\n",
       "      <th>Streptophyta</th>\n",
       "      <th>Lachnospiracea_incertae_sedis</th>\n",
       "      <th>Paracoccus</th>\n",
       "      <th>Fastidiosipila</th>\n",
       "      <th>Pseudonocardia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>841</td>\n",
       "      <td>0</td>\n",
       "      <td>813</td>\n",
       "      <td>505</td>\n",
       "      <td>5</td>\n",
       "      <td>3224</td>\n",
       "      <td>0</td>\n",
       "      <td>362</td>\n",
       "      <td>11</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1445</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>573</td>\n",
       "      <td>0</td>\n",
       "      <td>1278</td>\n",
       "      <td>82</td>\n",
       "      <td>85</td>\n",
       "      <td>69</td>\n",
       "      <td>154</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1259</td>\n",
       "      <td>0</td>\n",
       "      <td>805</td>\n",
       "      <td>650</td>\n",
       "      <td>0</td>\n",
       "      <td>1088</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>982</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>594</td>\n",
       "      <td>0</td>\n",
       "      <td>960</td>\n",
       "      <td>81</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>157</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1162</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>969</td>\n",
       "      <td>163</td>\n",
       "      <td>1515</td>\n",
       "      <td>167</td>\n",
       "      <td>4</td>\n",
       "      <td>162</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Streptococcus  Tropheryma  Veillonella  Actinomyces  Flavobacterium  \\\n",
       "0            841           0          813          505               5   \n",
       "1           1445           0            1          573               0   \n",
       "2           1259           0          805          650               0   \n",
       "3            982           0          327          594               0   \n",
       "4           1162           0          130          969             163   \n",
       "\n",
       "   Prevotella  Porphyromonas  Parvimonas  Fusobacterium  Propionibacterium  \\\n",
       "0        3224              0         362             11                 65   \n",
       "1        1278             82          85             69                154   \n",
       "2        1088              0           0             74                  0   \n",
       "3         960             81          19              9                  0   \n",
       "4        1515            167           4            162                  3   \n",
       "\n",
       "   ...  Microbacterium  Sphingomonas  Chryseobacterium  Bacteroides  \\\n",
       "0  ...               0            87                 0            0   \n",
       "1  ...               0             1                 2            0   \n",
       "2  ...               0             2                 8            1   \n",
       "3  ...             157             1                 0            4   \n",
       "4  ...               0             9                 0            0   \n",
       "\n",
       "   Bdellovibrio  Streptophyta  Lachnospiracea_incertae_sedis  Paracoccus  \\\n",
       "0             0             0                              0           0   \n",
       "1             0             0                              0           0   \n",
       "2            39             0                              0           0   \n",
       "3            60             0                              0           0   \n",
       "4             0             0                             60           0   \n",
       "\n",
       "   Fastidiosipila  Pseudonocardia  \n",
       "0               0            2133  \n",
       "1               0            3638  \n",
       "2               0            3445  \n",
       "3               0            3507  \n",
       "4               0            3945  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = pd.read_csv(resource_filename('deepbiome', 'tests/data/count/%s' % list_of_input_files.iloc[0,0]))\n",
    "x_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Streptococcus</th>\n",
       "      <th>Tropheryma</th>\n",
       "      <th>Veillonella</th>\n",
       "      <th>Actinomyces</th>\n",
       "      <th>Flavobacterium</th>\n",
       "      <th>Prevotella</th>\n",
       "      <th>Porphyromonas</th>\n",
       "      <th>Parvimonas</th>\n",
       "      <th>Fusobacterium</th>\n",
       "      <th>Propionibacterium</th>\n",
       "      <th>...</th>\n",
       "      <th>Microbacterium</th>\n",
       "      <th>Sphingomonas</th>\n",
       "      <th>Chryseobacterium</th>\n",
       "      <th>Bacteroides</th>\n",
       "      <th>Bdellovibrio</th>\n",
       "      <th>Streptophyta</th>\n",
       "      <th>Lachnospiracea_incertae_sedis</th>\n",
       "      <th>Paracoccus</th>\n",
       "      <th>Fastidiosipila</th>\n",
       "      <th>Pseudonocardia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1401</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>526</td>\n",
       "      <td>0</td>\n",
       "      <td>923</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2655</td>\n",
       "      <td>6</td>\n",
       "      <td>106</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>952</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>158</td>\n",
       "      <td>125</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>335</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>259</td>\n",
       "      <td>67</td>\n",
       "      <td>718</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>167</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>649</td>\n",
       "      <td>69</td>\n",
       "      <td>966</td>\n",
       "      <td>1227</td>\n",
       "      <td>0</td>\n",
       "      <td>508</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>550</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1119</td>\n",
       "      <td>0</td>\n",
       "      <td>2348</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>176</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Streptococcus  Tropheryma  Veillonella  Actinomyces  Flavobacterium  \\\n",
       "995           1401           4           30          526               0   \n",
       "996           2655           6          106           74               0   \n",
       "997            335           0           71          259              67   \n",
       "998            649          69          966         1227               0   \n",
       "999           1258           0            0         1119               0   \n",
       "\n",
       "     Prevotella  Porphyromonas  Parvimonas  Fusobacterium  Propionibacterium  \\\n",
       "995         923             25           0            127                  0   \n",
       "996         952             76          13            158                125   \n",
       "997         718              1           4              4                167   \n",
       "998         508              2          30            550                  0   \n",
       "999        2348             25           0            137                176   \n",
       "\n",
       "     ...  Microbacterium  Sphingomonas  Chryseobacterium  Bacteroides  \\\n",
       "995  ...               0             0                 7            0   \n",
       "996  ...               0             2                 0            0   \n",
       "997  ...               0           246                 0            0   \n",
       "998  ...               0             0                 0            0   \n",
       "999  ...               0             2                 0            0   \n",
       "\n",
       "     Bdellovibrio  Streptophyta  Lachnospiracea_incertae_sedis  Paracoccus  \\\n",
       "995             0             0                              0           0   \n",
       "996             0             0                              0           0   \n",
       "997             6             0                              0           0   \n",
       "998             0             6                              0           0   \n",
       "999             0             0                              0           0   \n",
       "\n",
       "     Fastidiosipila  Pseudonocardia  \n",
       "995               0            4470  \n",
       "996               0            2826  \n",
       "997               0            6527  \n",
       "998               0            4402  \n",
       "999               0            2585  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file has .csv format below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Streptococcus\",\"Tropheryma\",\"Veillonella\",\"Actinomyces\",\"Flavobacterium\",\"Prevotella\",\"Porphyromonas\",\"Parvimonas\",\"Fusobacterium\",\"Propionibacterium\",\"Gemella\",\"Rothia\",\"Granulicatella\",\"Neisseria\",\"Lactobacillus\",\"Megasphaera\",\"Catonella\",\"Atopobium\",\"Campylobacter\",\"Capnocytophaga\",\"Solobacterium\",\"Moryella\",\"TM7_genera_incertae_sedis\",\"Staphylococcus\",\"Filifactor\",\"Oribacterium\",\"Burkholderia\",\"Sneathia\",\"Treponema\",\"Moraxella\",\"Haemophilus\",\"Selenomonas\",\"Corynebacterium\",\"Rhizobium\",\"Bradyrhizobium\",\"Methylobacterium\",\"OD1_genera_incertae_sedis\",\"Finegoldia\",\"Microbacterium\",\"Sphingomonas\",\"Chryseobacterium\",\"Bacteroides\",\"Bdellovibrio\",\"Streptophyta\",\"Lachnospiracea_incertae_sedis\",\"Paracoccus\",\"Fastidiosipila\",\"Pseudonocardia\"\n",
      "841,0,813,505,5,3224,0,362,11,65,156,1,55,0,1,20,382,1,333,24,80,43,309,2,3,4,0,1,32,0,2,4,382,0,0,96,23,0,0,87,0,0,0,0,0,0,0,2133\n",
      "1445,0,1,573,0,1278,82,85,69,154,436,3,0,61,440,0,394,83,33,123,0,49,414,0,0,37,0,0,42,0,0,384,27,0,0,0,146,0,0,1,2,0,0,0,0,0,0,3638\n",
      "1259,0,805,650,0,1088,0,0,74,0,155,228,430,765,0,0,11,102,68,90,77,83,322,10,0,7,0,122,76,0,1,25,0,0,0,44,13,0,0,2,8,1,39,0,0,0,0,3445\n",
      "982,0,327,594,0,960,81,19,9,0,45,457,1049,0,3,450,19,170,388,147,0,0,41,63,0,1,0,0,121,0,0,1,0,0,0,0,344,0,157,1,0,4,60,0,0,0,0,3507\n",
      "1162,0,130,969,163,1515,167,4,162,3,12,0,48,73,93,259,52,0,201,85,14,14,434,2,0,0,0,0,187,0,0,188,45,0,0,0,4,0,0,9,0,0,0,0,60,0,0,3945\n",
      "1956,37,41,661,47,1555,374,7,142,19,61,226,0,30,52,0,6,480,142,148,9,575,12,0,0,0,0,3,168,0,56,50,0,0,0,98,989,0,0,12,0,0,0,0,0,0,0,2044\n",
      "1037,14,83,1595,132,305,103,174,1195,0,410,224,1,320,26,0,476,0,7,37,46,61,20,0,0,0,0,0,226,0,239,8,1,0,0,0,0,188,0,20,4,0,4,0,0,0,0,3044\n",
      "641,0,172,179,0,1312,84,9,81,376,128,223,160,0,532,155,89,355,1,282,0,0,25,0,0,43,0,9,311,0,0,0,0,0,0,0,845,0,0,8,0,0,0,0,0,0,0,3980\n",
      "852,146,504,99,2,376,116,152,67,0,120,3,23,2,34,0,127,75,240,60,42,0,9,0,15,0,62,0,13,0,197,187,396,0,0,20,51,0,0,3,0,0,0,0,0,0,0,6007\n",
      "901,3,187,1214,0,1508,675,0,107,49,318,2,393,5,3,65,4,285,79,11,0,0,4,3,0,0,1,0,729,0,0,173,0,0,0,0,254,0,0,0,0,0,1,0,29,0,0,2997\n",
      "677,60,635,45,268,2461,466,9,338,0,97,63,45,82,128,1,139,4,323,6,0,0,58,0,0,0,0,0,36,0,646,29,0,0,0,0,120,0,0,41,0,0,0,0,0,0,0,3223\n",
      "413,0,355,1258,0,583,48,107,250,5,102,204,287,4,0,18,2,145,454,11,0,79,104,25,164,1,0,100,55,0,1,33,206,0,0,10,81,0,0,0,0,0,0,0,0,0,0,4895\n",
      "351,99,17,268,0,912,4,96,847,472,67,8,440,16,0,0,247,0,165,83,0,0,0,0,4,0,0,0,275,0,21,1968,3,0,0,0,173,0,0,1,37,0,0,0,0,0,53,3373\n",
      "1829,0,32,77,0,1971,8,6,319,1,8,140,302,165,177,0,166,27,857,19,0,0,155,0,0,579,127,112,54,0,1,19,0,0,0,0,143,0,0,8,51,0,0,0,0,0,0,2647\n",
      "1089,0,4,30,0,847,51,42,48,0,211,1,24,0,81,1,687,494,758,27,0,0,123,7,0,0,0,1,310,0,6,4,780,0,0,4,1,0,0,0,0,0,2,0,0,0,0,4367\n",
      "1761,0,60,517,4,576,63,0,169,0,12,437,0,7,130,0,2,7,47,346,115,0,539,0,2,0,0,0,515,0,0,68,5,0,0,0,315,0,0,16,0,0,0,0,0,0,0,4287\n",
      "938,0,601,794,0,1836,52,6,10,0,1104,8,219,168,174,3,0,12,425,885,0,15,268,34,0,141,2,0,8,0,0,5,8,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2282\n",
      "882,8,110,975,0,1042,11,3,211,0,15,22,56,0,47,140,5,4,194,109,0,403,129,6,0,10,0,3,19,0,1,591,2,2,0,6,0,0,0,1,0,0,0,0,0,0,0,4993\n",
      "1395,0,76,60,0,916,2,466,353,13,1,159,369,0,92,2,14,528,103,133,100,4,17,0,0,0,0,0,7,0,0,6,48,0,0,44,578,0,0,0,0,0,0,0,0,0,0,4514\n",
      "1150,3,168,1265,0,1328,14,0,432,14,140,0,1,179,77,0,92,55,33,123,0,8,844,0,0,82,34,113,14,0,0,99,26,0,0,0,14,14,0,24,49,0,0,0,0,0,0,3605\n",
      "352,0,20,48,0,1035,269,1,1,53,122,1,0,0,107,0,12,414,253,483,335,3,117,0,0,1,21,37,7,0,8,13,15,0,0,186,0,0,0,18,0,57,0,0,0,0,0,6011\n",
      "1120,0,124,1198,0,2161,795,1,483,0,0,5,8,16,78,7,22,3,62,203,0,0,148,0,0,68,0,2,22,0,0,23,148,0,0,0,127,0,0,2,0,0,0,0,0,0,0,3174\n",
      "406,5,6,1191,0,2582,239,155,81,0,2,183,4,7,483,0,190,0,10,8,0,0,447,4,226,0,59,4,146,0,22,225,0,0,0,0,34,0,0,1,0,0,0,0,0,0,0,3280\n",
      "439,133,130,108,0,1511,50,180,33,0,110,86,539,0,88,145,7,11,745,487,0,40,176,43,8,103,0,6,193,0,4,306,6,0,0,0,2,1,0,0,0,0,0,0,0,0,0,4310\n",
      "513,0,130,583,2,781,3,34,201,84,3,1358,59,3,343,117,148,278,0,335,0,51,80,47,0,5,63,775,35,0,16,562,0,0,0,0,62,0,0,5,0,0,18,1,0,2,0,3303\n",
      "229,0,126,179,0,1581,61,26,1018,25,27,73,289,25,9,2,4,150,32,130,0,84,78,102,0,29,1,11,143,0,1,15,0,0,0,471,179,0,0,398,0,0,0,0,0,0,0,4502\n",
      "1378,3,327,351,3,1129,103,35,140,245,23,1,251,0,13,1128,0,0,81,16,0,0,193,31,0,0,0,255,273,0,1,15,8,0,0,0,29,0,6,0,0,0,1,0,0,0,2,3959\n",
      "701,0,402,918,0,1518,116,234,881,252,144,14,4,0,18,374,22,8,0,363,1,0,37,0,0,0,0,0,187,0,0,7,5,0,0,0,0,0,0,2,120,0,0,1,0,0,0,3671\n",
      "1116,1,630,232,0,1291,318,93,423,0,166,122,432,0,13,0,6,54,5,3,0,11,2019,0,0,51,0,0,88,0,0,91,1,0,0,34,0,0,0,0,0,0,0,1,0,0,0,2799\n",
      "1356,813,40,210,0,1337,628,0,134,0,145,203,247,1,28,5,25,0,1,373,58,0,93,2,0,80,260,0,18,0,0,196,0,2,0,0,0,0,4,0,4,0,0,0,0,0,0,3737\n",
      "470,0,167,18,41,882,318,820,189,38,521,65,58,132,25,257,43,6,973,46,132,114,17,48,0,0,51,0,4,0,0,131,76,0,0,0,106,0,0,0,2,0,0,0,0,0,0,4250\n",
      "487,16,412,945,1,535,1,0,122,0,530,392,1,182,167,1,1,44,374,61,73,9,33,0,8,0,0,9,484,0,52,66,4,0,0,0,571,0,0,1,0,0,21,0,0,0,0,4397\n",
      "994,0,1190,98,0,917,16,9,357,0,101,15,0,0,48,2,8,0,25,40,0,286,27,0,0,75,243,0,1610,0,0,387,75,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3477\n",
      "383,6,1452,690,4,775,2,58,287,0,833,0,83,1,5,0,50,400,28,120,0,16,6,0,3,274,0,0,313,0,0,0,0,0,0,4,27,0,0,0,0,0,72,0,0,0,0,4108\n",
      "179,101,59,491,0,1902,59,0,150,0,11,75,131,461,112,0,0,462,0,6,0,225,78,50,96,21,1,1,35,0,0,0,79,162,172,0,250,0,0,0,19,0,0,0,0,0,0,4612\n",
      "822,0,168,475,0,1640,600,6,247,13,138,2,51,2,64,169,24,55,360,126,0,841,94,158,0,0,0,27,448,0,0,190,0,0,0,0,5,0,0,0,22,0,0,0,0,0,0,3253\n",
      "525,50,142,278,1,1005,148,26,905,5,256,9,1536,37,6,116,48,13,116,14,23,0,0,0,0,1,1,1,8,0,0,48,7,0,0,0,289,0,0,0,0,0,0,9,0,0,0,4377\n",
      "660,14,132,252,64,597,0,206,13,23,12,233,160,0,212,21,281,160,45,102,4,133,44,0,0,88,53,0,204,0,20,127,13,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6126\n",
      "480,22,72,170,222,1481,6,2,585,211,448,503,11,48,14,527,832,0,9,192,341,272,2,0,0,50,2,0,52,0,0,86,192,0,0,0,14,0,0,0,0,0,0,1,0,0,0,3153\n",
      "741,23,189,143,3,2487,35,201,312,0,3,233,0,1,150,12,495,0,1,102,285,0,507,26,7,62,0,1,41,0,0,0,588,0,0,29,18,0,0,0,0,0,0,0,0,0,0,3305\n",
      "623,168,18,1993,0,1829,157,0,146,0,177,1,86,8,1232,0,6,0,5,14,3,0,59,0,5,0,122,8,25,0,2,202,265,0,0,0,201,0,0,0,12,0,0,0,0,0,0,2633\n",
      "835,0,79,117,17,1140,559,59,350,262,242,178,6,178,0,0,59,133,27,62,39,20,5,0,69,3,0,13,449,0,0,0,0,0,0,0,178,0,13,0,0,0,0,0,0,0,0,4908\n",
      "2196,0,614,328,0,1809,467,70,470,3,131,0,2,1,1,4,3,172,417,14,14,0,231,0,0,1,4,47,258,0,0,73,133,0,0,0,0,0,3,5,0,0,0,0,0,0,0,2529\n",
      "618,0,73,740,0,1253,21,115,508,8,124,15,221,0,88,14,33,304,733,10,258,177,305,0,0,46,0,0,342,0,13,512,0,0,0,0,117,0,0,0,0,0,0,0,0,0,0,3352\n",
      "835,7,94,769,1,1235,65,97,629,0,115,436,42,0,265,31,70,15,396,99,0,611,66,26,0,0,32,1,86,0,0,395,935,0,0,0,18,0,0,140,0,0,0,0,0,0,0,2489\n",
      "587,24,12,1374,0,2272,96,109,789,26,238,1,1,46,1,2,10,6,120,80,19,0,13,10,0,5,0,8,175,0,0,83,0,0,0,0,17,0,3,0,0,0,0,0,0,0,0,3873\n",
      "750,0,261,235,5,592,30,0,704,579,0,12,61,2,64,576,0,100,0,44,26,0,343,0,0,13,12,4,32,0,0,451,16,0,0,440,186,0,0,22,12,7,1,0,0,21,0,4399\n",
      "940,1,12,2219,2,447,203,35,451,10,76,22,183,5,78,16,0,0,194,394,68,0,14,0,0,305,7,0,567,0,1,183,410,0,0,1,27,0,0,0,0,0,0,0,0,0,0,3129\n",
      "516,2,300,127,0,831,494,18,798,0,0,589,342,53,86,922,140,5,118,207,0,2,37,31,5,77,144,0,113,0,169,34,195,0,0,0,5,0,0,60,37,216,0,0,0,0,0,3327\n",
      "449,8,130,731,8,1533,220,0,147,3,253,248,6,20,77,50,9,0,6,255,0,0,138,0,0,0,10,390,43,0,0,56,3,0,0,0,278,0,0,0,0,0,141,1,0,0,0,4787\n",
      "429,0,0,460,0,937,899,222,58,0,38,62,3,6,657,0,130,20,81,552,41,4,184,72,38,3,12,0,489,0,14,317,307,0,0,0,101,0,0,0,0,0,0,0,0,0,0,3864\n",
      "1453,368,215,377,2,1464,1,22,346,0,473,114,16,0,1,24,47,0,458,349,208,0,79,117,6,199,0,28,542,0,11,46,3,0,0,0,290,0,0,1,0,0,0,0,0,0,0,2740\n",
      "946,200,7,506,183,366,5,124,152,571,302,47,70,5,6,55,5,42,272,470,13,367,3,161,31,0,0,2,599,21,0,3,0,0,0,0,263,0,0,104,0,0,0,7,0,0,0,4092\n",
      "2039,1,288,1033,302,562,313,1,50,0,4,3,2,3,41,209,151,0,637,1031,43,5,0,0,0,1,6,101,318,0,1,20,0,0,0,0,13,0,0,7,0,0,0,0,0,0,0,2815\n",
      "707,81,332,103,436,1272,0,87,1262,0,102,69,12,22,2,2,436,507,425,177,0,0,425,0,0,0,0,0,90,2,207,272,487,0,0,477,4,0,0,0,0,0,1,0,0,0,0,2001\n",
      "534,0,94,1573,0,1752,75,0,830,7,35,0,3,5,135,16,502,65,121,4,0,0,17,1,0,314,0,262,16,0,2,274,47,0,0,162,5,4,0,3,0,185,0,0,0,0,0,2957\n",
      "1160,9,122,189,14,1479,305,158,291,9,39,4,301,0,376,0,85,0,532,32,6,621,990,0,71,61,1,4,24,0,0,160,104,0,0,0,33,0,0,58,0,0,0,27,0,7,0,2728\n",
      "564,2,31,423,0,2131,604,1,224,2,4,69,0,0,371,0,62,573,3,0,0,0,52,0,0,0,0,42,210,0,1,258,27,0,0,0,0,0,0,312,10,0,0,0,0,15,0,4009\n",
      "779,0,11,47,0,1710,568,2,188,0,255,4,2,88,75,95,786,421,698,72,7,0,31,0,21,137,0,10,354,8,0,379,103,0,0,0,126,0,0,0,0,0,0,0,0,0,0,3023\n",
      "1015,353,145,58,127,1030,47,0,39,0,410,64,22,104,212,1,260,51,86,30,8,146,78,2,0,22,22,0,19,0,43,982,382,0,0,10,0,0,26,0,154,0,0,61,0,0,0,3991\n",
      "1123,0,188,483,0,649,72,19,386,43,4,35,3,150,10,66,46,4,36,0,169,445,0,0,57,2,9,132,1286,0,0,38,12,0,0,16,22,0,0,15,0,0,0,0,0,0,0,4480\n",
      "902,0,336,744,0,1350,10,0,109,397,0,968,53,0,0,1,18,0,560,0,12,0,256,92,237,8,272,4,81,78,34,11,78,0,0,0,13,0,0,0,22,5,0,0,0,0,0,3349\n",
      "1586,0,11,473,0,2151,138,373,6,0,13,482,175,16,1,222,8,13,32,362,0,2,29,0,29,86,0,0,12,0,84,582,24,0,0,0,201,0,0,0,0,3,29,0,0,0,0,2857\n",
      "965,0,142,930,0,510,332,282,9,0,109,1,51,2,120,0,0,542,439,0,0,0,148,247,0,0,0,304,146,0,123,0,0,0,0,0,97,0,0,0,0,0,0,0,0,0,0,4501\n",
      "922,0,900,203,0,1362,21,384,26,0,523,16,42,10,42,3,29,3,9,204,0,0,13,2,57,65,0,6,108,0,140,33,43,0,0,0,70,0,0,0,0,0,0,0,0,0,0,4764\n",
      "826,0,61,84,6,1248,112,7,365,1071,193,45,0,0,1698,0,2,15,0,692,5,88,0,17,40,18,19,0,433,0,0,1,28,0,0,0,36,0,0,43,0,0,0,0,0,0,0,2847\n",
      "1138,0,504,573,3,1529,346,72,166,0,190,0,131,38,18,0,261,142,10,17,149,0,726,0,20,12,4,0,32,0,0,58,0,0,0,2,37,0,0,0,0,0,0,0,0,0,0,3822\n",
      "1610,0,17,35,0,721,597,2,588,24,32,6,187,15,31,0,15,3,531,190,34,75,10,0,0,6,0,0,250,0,0,177,44,0,0,0,78,0,0,0,0,0,25,0,0,0,0,4697\n",
      "1351,0,202,492,34,1607,639,28,197,0,55,0,155,0,174,0,10,0,158,494,47,70,32,227,0,0,0,0,445,0,0,120,1,0,0,0,42,0,0,12,0,0,0,0,0,0,0,3408\n",
      "1255,0,181,78,0,855,33,0,394,0,300,2,18,54,163,4,146,1,171,173,0,3,183,1093,0,274,288,0,336,0,0,201,13,0,0,12,24,1,0,0,0,0,0,0,0,0,0,3744\n",
      "1430,253,2,485,0,54,99,0,122,21,40,0,12,0,0,0,5,116,469,17,969,0,31,173,0,41,39,0,661,0,288,0,934,0,0,43,46,0,0,20,0,0,0,0,0,0,0,3630\n",
      "266,0,60,414,0,668,88,55,71,0,88,38,3,72,0,4,499,101,78,7,8,558,0,40,0,44,154,109,241,0,292,494,23,0,0,0,850,0,0,0,0,0,0,0,0,0,0,4675\n",
      "974,402,252,441,0,1557,40,0,188,1,231,0,2,57,1,0,655,462,468,143,239,0,60,0,0,0,0,0,56,0,0,35,10,4,0,0,254,0,0,0,0,0,0,0,0,0,173,3295\n",
      "710,503,53,94,0,1490,2,5,269,1,24,0,12,0,97,108,285,152,329,2,59,8,99,40,0,0,0,0,91,11,183,374,2,0,20,0,0,0,0,0,461,0,0,0,0,0,0,4516\n",
      "2075,309,141,83,19,1424,184,27,97,85,180,0,10,70,34,4,266,0,488,145,0,17,78,1,0,0,53,243,177,0,0,12,130,0,0,0,76,0,0,3,4,0,0,0,0,0,0,3565\n",
      "1319,3,1019,4,0,1102,1136,18,436,0,85,139,129,0,132,6,2,1,0,1,0,244,73,1,0,9,0,0,4,0,0,0,67,0,0,0,0,0,94,0,0,0,0,0,0,0,0,3976\n",
      "262,0,46,421,0,1027,501,62,377,1114,5,18,0,15,56,0,447,313,0,734,69,0,14,16,0,2,2,79,222,0,0,60,11,0,0,241,12,0,5,0,0,0,0,0,0,6,0,3863\n",
      "964,0,132,214,32,1532,130,11,1138,0,20,0,0,0,166,0,359,25,110,11,0,221,111,0,0,0,15,0,11,3,1,98,154,0,0,0,6,0,0,1,0,0,70,0,0,0,0,4465\n",
      "1813,0,110,549,20,428,1,177,131,111,0,0,5,0,3,0,0,0,318,4,7,0,52,0,0,305,0,23,441,5,124,8,829,0,0,5,10,0,4,25,0,0,0,0,0,0,0,4492\n",
      "942,3,424,292,0,658,191,37,147,0,419,50,1,542,13,0,11,6,1090,4,45,1,122,95,4,0,242,349,148,0,537,113,0,0,11,0,12,0,0,0,0,0,0,7,0,0,0,3484\n",
      "298,0,672,366,0,1176,3,8,304,253,18,432,31,0,2,50,299,6,3,32,1,618,16,258,0,0,0,0,182,0,0,1893,13,0,0,75,30,0,0,1,0,0,0,0,0,0,0,2960\n",
      "412,0,33,663,0,678,300,308,347,46,122,220,1,0,143,1,7,0,68,40,3,4,521,0,0,0,0,0,12,0,407,120,0,0,0,0,389,0,0,17,44,0,14,0,0,3,0,5077\n",
      "931,6,138,452,0,511,47,0,60,21,0,0,317,4,0,5,31,177,4,22,14,0,40,0,0,72,50,0,645,0,0,308,11,0,0,0,36,0,0,0,0,176,1,0,0,0,0,5921\n",
      "1836,605,11,72,0,2037,155,50,505,34,255,0,0,0,100,0,643,52,164,46,0,0,62,0,83,22,0,0,81,0,0,374,0,4,0,0,41,0,0,11,0,0,0,0,0,0,0,2757\n",
      "1304,2,37,119,76,1263,278,148,18,0,202,3,62,28,330,0,117,33,271,0,0,0,302,0,0,496,0,0,394,0,0,135,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,4381\n",
      "1283,90,350,408,0,764,453,4,66,0,30,0,0,60,0,12,5,30,59,225,0,25,45,0,2,0,105,5,361,0,300,259,244,0,0,115,17,0,0,7,0,0,0,0,0,0,0,4676\n",
      "675,72,166,372,27,1507,8,73,720,0,4,1,0,0,8,19,108,0,746,44,0,0,140,0,0,100,0,0,191,104,1,604,3,0,134,47,126,0,0,1008,34,0,0,0,50,0,0,2908\n",
      "2130,168,163,154,244,1733,192,19,3,57,58,1,97,0,104,34,2,8,1195,23,288,114,77,214,0,27,19,0,10,0,0,479,0,0,0,0,159,0,0,155,0,0,0,0,0,0,0,2073\n",
      "488,0,608,808,0,621,0,35,427,0,319,522,0,0,320,13,292,0,67,21,0,1,520,0,0,18,0,0,281,0,19,8,0,0,10,0,17,0,0,0,0,0,30,0,0,0,0,4555\n",
      "793,60,872,436,28,1153,0,28,18,1,0,4,145,50,216,0,4,1,93,318,44,30,22,81,0,0,0,467,72,4,0,3,0,0,0,53,99,0,0,0,413,0,0,0,0,0,0,4492\n",
      "556,0,343,804,4,1317,60,39,644,1,94,148,0,0,50,0,2,530,45,70,0,0,135,0,0,325,6,0,25,0,13,166,82,0,0,0,0,0,83,0,18,0,0,0,0,0,0,4440\n",
      "422,1433,55,365,0,513,45,9,251,10,17,31,3,400,387,0,3,88,338,4,0,95,7,8,192,180,3,6,118,0,0,369,30,0,0,240,886,0,69,11,0,0,0,0,0,5,0,3407\n",
      "818,0,411,362,332,654,174,47,137,0,223,515,15,312,614,0,255,117,58,275,0,99,159,0,0,0,83,506,380,0,1,371,48,0,0,83,108,0,0,0,7,1,0,0,0,0,0,2835\n",
      "875,0,323,50,13,1542,11,84,40,3,937,0,19,0,453,149,138,1,199,391,129,2,6,0,132,0,0,1251,75,0,0,10,0,0,0,0,2,0,15,39,0,0,1,0,0,0,0,3110\n",
      "1029,224,830,561,109,1795,17,182,40,11,1,8,2,47,88,0,516,0,4,18,5,62,0,713,0,148,0,15,204,0,3,489,1100,0,0,2,122,0,2,0,0,0,0,0,0,0,0,1653\n",
      "635,59,392,622,0,2423,4,11,294,148,60,27,164,15,38,0,7,56,572,127,5,0,922,107,5,2,0,0,232,0,172,220,62,0,0,14,0,0,0,0,0,0,0,0,0,0,0,2605\n",
      "673,13,268,526,0,2855,549,0,245,2,1,30,3,0,2,24,15,1,108,2,0,0,352,0,0,244,22,1,22,0,0,18,8,0,0,23,108,4,0,0,0,0,0,0,0,0,0,3881\n",
      "1239,7,60,1294,1,643,16,129,799,0,86,11,10,0,398,11,91,1,28,101,4,271,1,316,0,0,0,6,70,0,0,14,0,0,0,4,3,0,0,0,0,0,0,0,0,0,0,4386\n",
      "1470,0,334,281,0,975,2,225,12,0,22,631,23,0,127,7,12,334,2,36,0,0,644,27,12,5,1,0,683,0,1,276,26,0,0,5,75,0,0,1,0,0,18,0,0,0,0,3733\n",
      "641,1,135,649,0,1211,849,10,496,5,8,0,0,0,10,3,5,11,10,999,137,39,375,0,0,0,19,0,7,1,364,88,72,5,0,0,2,0,1,0,0,0,2,0,0,0,0,3845\n",
      "1163,0,786,861,0,785,86,599,291,0,239,205,3,0,47,0,37,2,4,141,158,0,240,40,0,0,7,9,39,0,0,57,20,1,0,0,364,0,0,0,0,0,0,0,0,0,0,3816\n",
      "568,2,523,348,0,1221,22,75,361,20,3,172,280,117,59,90,583,0,203,35,0,6,6,243,0,553,0,0,742,0,0,19,0,0,0,4,110,0,0,163,0,0,7,0,0,0,0,3465\n",
      "493,0,96,79,0,2091,238,72,258,0,422,146,0,36,8,104,1,5,42,49,0,1,138,0,40,0,0,2,419,0,0,56,506,0,0,0,4,0,0,7,0,0,5,0,0,0,0,4682\n",
      "395,0,46,1373,0,1394,55,0,48,34,48,0,24,124,123,336,40,23,17,1,638,0,63,7,0,0,0,0,5,0,0,109,243,0,0,0,3,0,0,2,0,0,0,0,0,0,0,4849\n",
      "694,0,6,414,0,1196,168,122,22,0,83,83,180,0,21,21,28,7,142,75,8,116,5,8,0,5,41,3,816,0,0,5,279,0,0,253,44,0,0,0,48,0,0,0,0,13,0,5094\n",
      "855,110,367,662,0,1401,4,107,29,2,20,7,101,0,108,29,358,2,53,18,7,9,639,2,0,1,4,0,202,0,0,53,8,0,0,0,244,0,0,14,0,0,0,0,0,0,0,4584\n",
      "600,0,1,148,0,1042,423,3,15,3,30,995,47,0,744,1,0,0,263,53,0,115,206,0,0,0,0,0,22,0,0,0,12,0,0,0,22,0,0,0,0,0,0,0,0,0,0,5255\n",
      "312,0,93,1111,0,1802,227,30,0,0,6,25,9,0,60,47,178,0,270,864,1,1,383,0,0,50,0,73,199,0,0,17,419,0,0,0,83,0,0,0,0,0,0,0,0,0,0,3740\n",
      "523,0,98,197,497,2342,15,0,662,0,52,222,10,12,23,37,203,0,423,444,1,0,61,39,0,18,109,2,148,0,1,32,15,0,0,0,14,0,0,177,0,0,0,0,0,0,0,3623\n",
      "297,0,175,232,0,484,0,170,35,0,358,31,2,0,2,0,6,0,104,22,168,150,42,1,0,57,0,2,409,0,1,511,551,0,0,0,16,0,53,6,0,0,13,0,0,0,0,6102\n",
      "936,26,35,73,1,958,4,0,1274,0,219,5,170,1,17,1,103,437,168,124,105,437,141,0,0,3,0,139,361,0,0,229,2,0,0,0,36,0,0,0,0,0,0,0,0,0,0,3995\n",
      "2003,0,304,329,0,852,121,9,209,5,95,135,693,0,216,0,33,6,2,1,0,2,416,38,0,111,3,0,122,0,0,43,134,0,0,0,1,0,0,0,0,0,0,17,0,0,0,4100\n",
      "724,190,18,877,0,1560,1002,725,429,75,0,0,49,17,103,0,25,0,15,52,0,0,638,0,0,24,0,0,267,15,0,146,2,0,0,1,0,33,0,0,0,0,0,0,0,0,0,3013\n",
      "646,4,332,674,0,861,381,6,897,4,145,149,38,36,108,3,1072,1,16,0,0,0,84,0,0,7,0,261,491,0,24,273,2,0,0,23,0,0,0,0,23,10,7,0,0,0,0,3422\n",
      "261,0,11,213,24,2144,35,337,178,45,423,0,127,13,72,0,0,0,2,11,196,0,13,0,0,0,0,111,261,0,0,597,284,0,0,47,10,0,0,0,1,0,0,0,2,0,0,4582\n",
      "1387,5,154,44,0,893,0,80,234,117,418,196,0,1,269,0,203,75,45,0,10,0,142,12,0,16,5,0,58,2,266,159,0,0,0,160,2,261,0,0,0,0,0,0,0,0,0,4786\n",
      "1154,31,13,1332,0,654,504,13,127,428,191,39,15,0,1,0,0,0,62,23,0,0,1,1,0,0,1,0,307,0,0,336,366,0,0,0,236,0,0,0,0,0,0,0,0,0,0,4165\n",
      "1315,5,309,698,0,759,31,8,110,26,69,245,622,232,74,0,22,0,17,7,4,0,6,6,0,2,1,272,443,0,0,93,61,0,0,0,22,0,0,0,0,0,1,0,0,0,0,4540\n",
      "1196,49,1,675,0,1126,412,5,24,15,674,157,36,0,1,1,0,0,161,79,958,3,203,0,0,0,8,0,3,2,14,242,0,0,0,0,157,0,0,1,0,184,0,0,0,0,0,3613\n",
      "267,0,228,584,58,634,450,0,240,0,137,2,202,0,48,58,20,61,75,151,2,0,424,0,0,10,35,85,16,0,805,0,846,0,0,3,4,0,0,0,0,0,49,0,0,0,0,4506\n",
      "1589,7,101,23,0,3168,54,1,20,43,124,129,14,16,107,27,0,0,2,20,0,0,25,0,0,8,0,31,2,0,0,111,131,0,0,0,0,0,0,0,0,0,0,1,0,0,0,4246\n",
      "2039,17,76,177,0,1770,15,851,320,0,366,52,314,0,39,0,208,0,9,71,17,166,26,1,0,0,8,75,133,0,0,4,0,0,0,0,121,0,0,0,0,3,8,0,0,0,0,3114\n",
      "1040,21,1246,901,412,789,809,0,76,0,0,158,7,79,55,0,31,0,157,2,118,51,191,0,11,2,0,15,188,0,678,46,1,0,0,0,162,0,0,24,2,1,0,0,0,0,0,2727\n",
      "341,70,337,88,2,322,415,2,494,4,97,107,148,0,217,781,0,27,161,421,35,0,722,5,4,0,0,31,3,0,19,281,1,0,0,0,1203,0,0,0,2,0,0,0,0,0,0,3660\n",
      "633,0,505,43,220,993,55,326,270,2,375,14,68,258,88,51,28,1,4,153,12,429,386,0,0,48,294,31,129,0,2,38,147,0,0,0,293,0,13,0,0,0,0,0,0,0,0,4091\n",
      "667,109,472,85,2,998,125,66,479,161,0,9,217,0,0,0,46,1,18,37,0,0,4,6,0,48,7,81,390,0,12,52,0,0,30,0,0,0,0,0,0,0,0,0,0,0,0,5878\n",
      "1427,0,210,376,49,1506,10,33,4,10,29,350,21,62,254,14,78,0,2,211,3,0,10,35,1188,18,0,12,34,0,25,330,261,0,0,1,4,0,0,244,7,0,0,0,0,0,0,3182\n",
      "704,0,771,390,0,2541,82,27,212,13,498,244,17,161,0,12,38,0,22,533,58,62,504,0,303,30,0,1,70,0,5,11,5,5,0,0,1,0,0,0,0,0,0,33,0,0,0,2647\n",
      "1450,3,99,303,4,385,267,83,182,55,44,0,329,11,80,1,294,130,22,3,0,6,205,0,0,1,18,8,88,25,0,6,29,0,260,0,17,0,0,420,0,0,0,0,0,0,0,5172\n",
      "431,1,118,195,227,1150,579,18,77,0,16,4,275,196,0,44,16,1011,788,13,0,0,20,8,0,4,0,1,130,0,0,32,303,327,0,0,70,0,93,3,0,0,64,0,0,0,0,3786\n",
      "150,0,757,821,1,535,48,0,108,0,132,7,288,431,1,1,56,68,122,300,14,0,276,0,0,34,6,0,438,0,2,2,0,0,0,0,63,0,0,0,0,0,0,0,0,0,0,5339\n",
      "648,0,239,947,0,935,4,0,189,12,217,0,33,0,3,0,808,6,99,612,0,29,12,0,0,11,5,94,10,0,0,23,0,0,0,0,32,0,0,2,0,0,1,0,0,0,0,5029\n",
      "2104,63,4,505,0,1186,420,95,320,54,69,157,258,307,332,162,1,20,0,111,42,16,85,78,24,0,1,1,277,0,0,39,281,0,0,0,1,0,0,0,2,0,1,0,0,0,0,2984\n",
      "684,0,895,53,159,500,199,0,264,2,342,0,12,5,1,0,329,202,446,78,66,0,419,0,0,0,0,1,726,0,91,256,121,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4149\n",
      "1041,96,176,378,0,2298,56,670,253,0,1,740,43,89,117,245,12,1,23,39,0,2,171,21,1,0,0,0,72,7,0,827,73,0,0,37,75,0,0,0,0,0,0,0,0,0,0,2436\n",
      "864,711,350,1101,0,2048,80,7,25,1,245,19,228,0,2,0,134,0,50,14,5,0,307,13,0,33,0,360,5,0,5,85,116,182,3,77,0,0,0,0,0,0,0,18,0,0,0,2912\n",
      "974,22,88,247,8,1109,115,13,131,0,17,130,6,40,686,0,95,296,32,9,0,284,81,338,0,0,328,0,109,0,25,78,1,0,0,0,65,0,0,1,0,0,0,0,0,0,0,4672\n",
      "37,0,344,263,0,647,73,0,12,16,424,0,47,0,113,1,402,112,14,0,225,2,47,0,1,107,88,0,1064,0,0,1066,1129,0,25,9,336,0,0,0,0,0,4,0,0,0,0,3392\n",
      "1616,9,104,493,98,676,83,0,101,126,4,0,81,0,101,0,428,0,248,181,70,16,3,7,0,0,52,0,91,3,8,219,495,0,0,0,0,0,0,0,96,0,2,0,13,0,0,4576\n",
      "765,349,441,285,78,2028,418,1,313,0,147,0,153,1,1,183,85,0,274,572,0,202,29,131,0,51,0,53,44,0,0,11,0,389,0,0,18,0,0,0,0,0,0,0,0,0,0,2978\n",
      "2120,0,23,538,0,786,130,1,535,0,67,103,19,2,0,196,14,0,212,119,14,0,242,0,43,1,12,126,191,0,0,38,161,0,0,0,0,0,0,138,0,0,0,0,0,0,0,4169\n",
      "161,206,261,787,5,1441,79,1059,14,92,22,17,17,4,98,96,14,24,131,278,15,0,0,40,3,0,7,0,227,49,52,85,25,0,0,0,4,0,0,0,0,0,0,23,0,11,0,4653\n",
      "369,0,1,240,0,2111,33,0,331,1,5,41,0,151,0,365,559,61,478,257,232,0,78,29,0,0,0,0,457,0,27,0,71,0,0,80,73,0,0,0,0,0,0,0,0,0,0,3950\n",
      "156,310,43,232,81,1570,164,10,212,0,29,6,494,25,5,255,0,89,10,1,21,9,225,0,0,0,0,6,149,0,0,1303,102,0,0,0,72,0,0,4,0,0,0,0,0,8,0,4409\n",
      "650,0,19,576,0,2185,18,89,265,0,50,14,16,61,48,0,76,0,24,751,0,77,81,192,0,0,0,0,362,0,0,167,395,0,0,2,0,0,0,12,0,0,0,1,0,5,0,3864\n",
      "2143,25,33,336,0,2094,40,1,109,0,485,26,54,0,126,251,58,30,115,89,14,0,11,479,0,60,1,75,287,0,0,385,24,0,0,0,487,0,0,0,0,0,64,0,0,0,0,2098\n",
      "773,1,562,611,7,2419,251,101,312,6,476,15,69,0,584,267,7,22,4,30,0,0,8,0,13,82,0,0,25,0,74,12,3,0,0,0,359,0,0,3,14,0,688,0,0,0,0,2202\n",
      "1238,0,8,54,0,1887,58,0,433,83,0,1435,3,104,286,0,7,14,154,4,49,1,50,2,441,0,0,102,58,0,0,98,3,0,0,0,11,0,0,24,0,0,0,407,0,0,557,2429\n",
      "870,1,147,97,27,724,130,15,274,27,199,36,2,0,933,128,24,0,202,40,10,0,23,16,0,0,0,23,905,0,81,0,518,0,0,483,66,0,0,9,0,0,0,0,0,0,0,3990\n",
      "660,225,201,1088,32,2427,395,0,92,0,16,244,61,401,5,1,91,145,122,18,0,16,50,1,0,373,0,2,326,0,34,529,167,0,0,9,2,0,0,0,0,0,3,0,0,0,0,2264\n",
      "268,0,101,450,0,595,59,0,9,2,651,177,31,10,37,72,3,153,26,2,7,68,84,0,8,255,0,0,352,0,0,322,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,6256\n",
      "1318,0,13,235,0,551,208,29,31,0,0,471,68,25,325,2,40,425,167,33,42,0,641,0,1,1,2,75,50,1,0,300,63,0,0,0,1,0,901,75,25,0,0,0,26,0,0,3855\n",
      "1801,0,35,42,1,772,1560,0,324,2,184,10,2,0,380,0,0,1,16,2,0,8,67,0,37,3,0,0,408,0,2,2,15,0,0,0,711,0,0,0,0,0,0,0,0,0,1,3614\n",
      "596,11,297,29,0,1242,102,45,11,572,18,153,0,0,87,20,107,39,7,0,0,0,166,0,0,356,0,0,89,0,162,4,163,0,0,0,184,0,0,5,0,0,19,0,0,0,0,5516\n",
      "409,0,171,481,1,2338,9,363,514,406,30,0,6,1,209,14,230,0,31,5,1,176,55,0,32,190,176,0,216,0,0,304,511,0,0,0,56,0,0,0,0,0,0,0,0,0,0,3065\n",
      "356,0,75,353,178,321,409,751,116,177,583,1,0,0,59,32,95,858,103,64,244,0,71,0,0,0,0,0,244,9,18,74,508,0,0,0,116,0,0,1,0,1,7,0,0,0,0,4176\n",
      "866,50,183,151,255,859,300,26,476,0,167,3,1,43,1,32,6,16,91,70,0,30,15,299,0,0,0,1,793,0,22,64,171,0,0,0,29,0,0,0,0,0,0,0,0,0,0,4980\n",
      "2345,285,104,216,259,1750,718,12,402,108,190,185,129,29,433,243,56,35,61,59,128,20,0,1,0,0,4,0,118,0,0,69,0,0,0,0,291,0,0,0,0,0,0,1,0,0,0,1749\n",
      "1044,0,81,413,0,254,1,474,35,50,154,151,39,60,194,0,107,0,6,1416,0,298,338,0,0,8,0,0,78,0,0,17,0,0,1,0,100,0,0,1,105,0,0,0,0,0,0,4575\n",
      "682,0,129,152,0,708,55,735,164,0,230,0,1735,3,379,11,42,12,0,0,0,15,9,1,41,88,127,2,28,0,8,11,1,0,0,1,1,0,0,3,0,0,0,0,17,0,0,4610\n",
      "1411,0,200,382,129,1289,0,1,7,2,544,135,6,0,35,0,1,47,2,360,0,62,139,9,0,0,570,247,3,0,0,10,14,0,0,0,4,0,0,0,0,0,0,0,0,0,17,4374\n",
      "435,0,510,381,0,2236,1,96,944,0,118,5,4,80,105,0,2,0,177,0,0,49,32,6,0,1,0,0,685,0,0,2,0,0,0,0,0,0,0,4,42,0,24,0,0,0,0,4061\n",
      "868,317,1,602,0,2364,20,0,308,50,366,2,0,1,45,6,9,0,320,465,0,0,95,104,0,0,0,1,120,0,9,104,66,0,0,0,0,0,0,64,0,0,0,0,0,0,0,3693\n",
      "1218,0,126,170,0,1544,94,25,224,19,100,31,0,277,397,4,618,4,102,12,0,0,103,29,0,85,0,78,112,0,1,0,41,0,0,0,0,0,0,0,0,14,0,0,0,0,0,4572\n",
      "1467,0,78,82,5,2945,320,12,805,230,160,53,0,28,41,27,41,44,9,51,2,197,12,0,0,5,0,0,127,0,0,16,116,0,0,0,1,0,2,0,0,0,207,0,0,0,0,2917\n",
      "797,0,1507,113,0,1109,180,0,211,0,121,98,414,0,11,4,13,7,11,73,6,5,385,43,0,0,38,0,0,0,7,114,12,0,0,0,16,0,0,3,0,0,229,0,0,0,0,4473\n",
      "230,230,446,687,72,899,18,18,35,580,493,4,518,0,36,0,0,42,400,737,3,0,0,2,0,0,38,1,46,375,0,81,102,0,0,1,81,0,0,0,38,0,0,0,0,0,0,3787\n",
      "177,52,0,555,0,1451,94,0,116,616,826,63,24,7,46,551,109,178,39,70,0,0,14,0,46,277,188,0,183,7,0,587,2,0,0,0,302,0,50,2,0,0,0,0,0,0,0,3368\n",
      "1749,0,112,202,0,2169,366,5,15,56,119,116,182,20,92,0,460,3,3,24,0,0,488,3,0,290,5,0,459,0,0,255,5,0,0,0,15,0,0,0,0,0,0,0,0,0,0,2787\n",
      "206,0,1130,223,4,1004,4,0,112,0,292,648,163,139,12,0,15,29,307,60,0,2,0,0,1,225,0,0,956,0,0,131,0,0,0,0,501,0,0,0,0,0,0,0,0,0,0,3836\n",
      "639,47,320,453,0,792,16,0,436,2,88,37,1,3,20,38,37,0,3,930,0,0,19,0,0,0,30,57,211,0,0,1213,275,0,0,12,1,0,0,19,0,0,0,0,0,0,0,4301\n",
      "189,35,87,1938,0,1890,98,67,65,0,584,73,3,240,1,58,8,3,249,15,156,5,4,6,0,48,0,0,26,0,0,295,12,0,133,5,33,0,0,0,0,0,0,0,0,0,0,3674\n",
      "571,0,1233,321,75,2339,14,87,227,0,185,274,0,15,2,6,297,0,54,3,28,0,107,0,0,20,0,0,259,0,0,19,29,0,0,7,26,0,0,11,0,0,0,0,0,0,0,3791\n",
      "1707,0,533,1448,0,707,1,0,9,0,11,740,3,2,46,23,22,167,653,12,0,0,68,0,0,0,0,1,375,0,0,23,39,0,0,193,0,0,0,1,0,0,0,0,0,0,0,3216\n",
      "91,55,380,480,0,1016,162,327,1271,497,23,22,239,0,31,0,21,1,22,3,1,1,0,321,0,305,0,0,700,0,0,59,77,0,0,166,9,0,0,1,0,0,0,0,0,0,0,3719\n",
      "418,9,46,911,86,541,1,41,490,0,758,0,321,23,20,1,78,84,65,338,0,1,3,41,2,634,1,7,224,0,0,93,1,0,0,0,140,0,0,19,2,0,0,0,0,0,0,4601\n",
      "609,0,453,1582,8,522,3,8,401,5,3,5,45,6,17,141,25,111,0,512,1,734,773,0,58,6,0,0,809,0,0,140,1,0,0,0,82,0,0,0,0,0,3,0,0,0,0,2937\n",
      "2642,38,69,692,1,917,73,86,223,9,15,83,98,59,532,1,0,29,19,109,2,0,46,0,0,37,0,0,446,0,73,19,1097,0,0,0,28,1,0,0,0,0,0,0,0,0,0,2556\n",
      "722,0,558,1078,112,829,448,48,1421,0,0,0,1,30,211,136,6,6,20,1,0,72,0,0,22,30,0,2,69,0,2,17,29,2,0,0,47,0,0,2,59,0,1,0,0,0,0,4019\n",
      "1312,189,7,579,0,2032,48,78,282,67,214,8,263,356,128,3,479,9,55,283,1,65,7,117,0,183,0,4,41,0,283,44,3,0,2,0,7,0,0,0,0,0,0,0,0,0,0,2851\n",
      "1065,136,0,1121,1,526,2,0,44,517,188,43,21,1,239,38,0,220,100,0,25,5,233,0,104,46,0,0,13,0,23,71,335,0,0,0,2,0,0,2,0,0,0,0,0,0,0,4879\n",
      "1512,15,63,643,22,1321,0,16,293,0,15,4,188,0,287,0,192,0,66,41,0,0,65,10,0,0,200,2,19,0,0,929,393,0,0,0,136,0,24,2,0,0,82,0,0,0,0,3460\n",
      "652,142,374,254,28,1077,145,1,286,0,12,7,0,0,31,0,20,191,35,1,342,81,34,0,0,3,3,0,58,0,18,5,2,0,0,0,67,0,29,0,0,0,1,0,0,0,0,6101\n",
      "1374,53,303,1283,352,1923,127,19,36,0,225,3,226,51,17,0,41,28,9,14,13,2,30,0,0,4,8,0,254,26,26,21,0,0,0,0,180,0,0,0,2,0,0,0,0,0,0,3350\n",
      "578,58,138,875,20,730,38,89,1694,0,170,0,0,483,0,0,280,6,235,119,0,0,0,359,0,2,0,0,324,0,0,98,22,0,0,0,3,0,0,0,0,0,41,0,0,0,0,3638\n",
      "418,0,25,1094,0,2274,14,46,300,0,14,71,15,3,251,63,155,71,557,6,0,0,342,0,0,0,0,219,623,0,0,121,119,0,0,1,421,0,39,0,1,0,0,0,109,0,0,2628\n",
      "561,0,8,419,0,858,188,239,112,22,221,188,646,0,3,205,362,6,232,44,21,9,222,0,0,0,0,2,149,0,0,269,20,0,0,0,277,0,0,0,0,0,0,0,0,0,0,4717\n",
      "533,141,2,24,0,766,20,28,135,9,31,0,17,133,466,378,6,0,93,238,0,192,13,0,19,2,207,634,978,0,0,192,43,0,6,0,102,0,0,0,0,0,0,0,0,0,0,4592\n",
      "266,0,46,293,8,2097,281,5,732,30,20,1572,2,39,12,1,567,4,33,208,0,0,209,0,0,0,0,69,40,0,0,22,1,0,0,0,5,0,0,200,0,0,0,4,0,0,0,3234\n",
      "1281,0,6,921,104,886,460,179,340,3,0,193,27,45,484,0,0,9,118,179,4,0,378,0,0,0,0,0,128,0,183,223,807,0,0,0,181,0,0,0,0,0,0,0,0,0,0,2861\n",
      "337,0,694,341,11,416,257,332,115,9,226,12,3,139,138,32,77,0,2,3,1,72,42,0,0,3,4,0,3,0,216,6,0,0,0,90,928,0,0,8,0,0,0,0,0,0,0,5483\n",
      "1131,0,1027,294,1,741,0,86,633,224,119,76,51,42,78,11,1,0,244,10,0,0,104,1,0,839,0,3,468,0,8,56,296,0,0,285,11,0,0,17,0,0,0,1,0,0,0,3142\n",
      "589,0,2,338,0,797,3,757,112,74,1,165,0,1,0,0,241,4,50,33,0,2,295,4,0,152,9,38,43,0,0,695,1,0,0,0,0,0,0,206,0,0,0,0,0,0,0,5388\n",
      "904,437,11,536,1,545,106,0,425,2,30,6,4,5,34,0,124,0,583,1,0,134,383,202,0,0,0,0,247,0,0,0,4,0,0,343,18,0,0,0,0,0,0,0,0,0,0,4915\n",
      "661,0,25,895,0,1051,57,119,602,0,9,132,448,12,83,0,107,14,259,38,0,0,179,89,12,8,0,12,394,0,40,179,806,0,0,0,16,0,1,0,0,0,0,0,0,0,0,3752\n",
      "1322,14,1216,1191,0,824,0,3,477,0,24,17,304,10,150,284,25,2,203,9,13,0,67,0,0,9,125,38,256,0,0,317,4,0,0,0,79,0,0,138,0,0,0,0,0,0,0,2879\n",
      "1034,0,106,45,0,842,23,4,347,0,37,4,44,1,77,0,72,101,687,131,547,16,282,0,3,3,956,1,71,0,0,181,0,0,0,12,96,0,0,0,0,0,35,0,0,0,0,4242\n",
      "521,0,247,68,1,618,14,1190,2232,437,57,34,6,0,0,0,126,81,1,316,20,3,118,0,61,104,0,0,30,0,0,18,334,0,0,108,6,0,0,1,0,0,0,0,0,0,0,3248\n",
      "798,0,21,242,0,1383,25,0,364,0,44,0,43,172,358,25,475,1,7,106,0,1081,347,1,0,0,2,2,5,42,0,127,1,0,0,18,31,0,0,0,11,0,0,0,0,0,0,4268\n",
      "931,2,151,750,0,843,5,4,782,21,3,143,2,17,17,0,0,0,28,41,43,75,1,0,0,10,49,0,443,0,0,856,2,39,0,11,2,0,0,6,0,0,0,0,0,0,0,4723\n",
      "373,0,128,1089,9,1176,23,139,305,12,13,2,9,0,80,0,14,55,211,8,17,256,44,3,0,0,1,0,1848,0,870,2,0,0,0,5,1,0,0,0,0,0,13,0,0,0,0,3294\n",
      "1378,24,11,877,89,1897,18,468,816,1,75,75,0,3,73,0,270,58,50,155,0,13,542,2,0,17,0,19,228,0,17,196,1,0,0,0,395,0,12,12,0,1,0,89,0,0,0,2118\n",
      "233,0,106,593,0,784,49,22,287,0,547,284,11,16,37,91,64,0,68,114,253,71,961,8,5,3,65,15,263,0,0,1,46,0,0,43,84,0,0,585,14,0,99,0,0,1,0,4177\n",
      "949,0,69,81,8,2254,127,14,14,179,93,0,514,10,441,1,37,379,0,0,0,0,0,0,0,1,0,0,16,605,354,150,281,598,322,0,4,421,0,0,0,0,3,0,0,0,0,2075\n",
      "1778,19,221,431,95,1309,282,0,663,0,0,344,51,35,727,0,24,137,192,0,0,0,85,0,0,3,0,0,139,0,0,6,0,13,0,65,125,0,0,4,0,0,0,0,0,0,0,3252\n",
      "700,88,234,1016,33,2054,40,106,72,0,27,1,40,65,106,0,133,2,0,60,485,389,468,2,0,16,80,0,73,0,0,807,26,0,0,0,61,0,0,0,0,0,0,0,0,0,0,2816\n",
      "505,1,80,3103,0,1203,17,184,160,0,285,0,371,36,8,0,113,1,225,0,0,0,91,0,67,52,11,0,323,0,226,8,0,0,0,0,0,0,0,52,0,0,0,0,0,0,0,2878\n",
      "1585,4,766,116,0,1511,8,0,0,0,9,29,107,353,27,0,428,0,121,2,224,0,202,0,2,6,1,0,263,0,0,159,11,0,0,0,3,0,0,102,0,0,0,0,0,0,0,3961\n",
      "620,0,393,239,0,1113,914,0,64,198,429,10,442,373,15,0,0,0,36,912,204,0,10,128,1,0,2,0,240,0,0,69,7,0,0,0,8,0,0,0,0,0,0,0,0,0,0,3573\n",
      "442,491,1591,408,0,1027,77,0,0,0,372,28,2,206,0,0,883,15,99,126,0,0,573,139,3,0,0,0,272,0,103,23,98,0,0,0,3,0,0,7,0,0,0,0,0,0,0,3012\n",
      "1449,8,157,82,0,496,2,6,1153,3,0,450,7,2,184,0,0,49,40,10,5,2,115,8,0,33,0,0,480,0,12,26,250,0,0,0,231,0,0,115,0,0,2,0,0,0,0,4623\n",
      "739,356,688,211,0,766,155,268,870,0,123,12,182,0,3,1,309,0,7,2,0,0,486,14,0,119,11,0,34,0,0,97,68,0,0,0,71,0,0,0,0,0,0,0,0,0,0,4408\n",
      "1214,0,125,344,195,422,44,0,1212,173,36,111,74,63,153,0,145,9,143,8,13,15,139,0,0,245,27,0,726,0,0,0,4,0,0,0,33,0,0,0,3,0,0,1,0,0,0,4323\n",
      "1497,733,24,810,0,1038,61,107,200,30,21,0,105,980,9,214,84,0,0,25,0,19,205,0,0,111,161,0,111,0,0,257,6,0,0,4,3,0,0,0,0,0,11,0,0,0,0,3174\n",
      "2074,1,843,544,0,1354,7,0,27,0,270,248,37,0,0,0,0,0,90,529,0,0,1,0,0,80,0,3,167,0,0,7,187,0,0,0,58,0,0,4,0,0,0,0,0,0,0,3469\n",
      "1332,0,495,615,0,1983,94,5,40,10,275,2,0,211,2,0,84,82,1034,42,0,4,36,0,1,0,2,0,1,270,3,953,125,0,0,0,26,0,63,0,0,0,1,0,0,0,0,2209\n",
      "751,0,727,1252,28,1183,292,713,302,0,456,28,27,0,172,23,8,31,5,23,1,1,596,37,37,2,0,0,229,0,4,142,36,0,0,0,6,0,0,13,0,0,0,0,8,0,362,2505\n",
      "1084,0,1902,687,0,504,186,621,253,2,210,0,127,211,0,1344,102,29,0,126,0,3,3,141,0,2,3,6,43,0,2,0,4,0,0,0,2,0,0,174,0,0,0,0,0,0,0,2229\n",
      "864,0,692,318,0,1083,7,41,329,362,0,73,140,2,160,0,0,0,2,4,2,0,7,0,0,0,4,128,236,0,0,0,4,0,86,495,3,0,0,10,6,0,16,0,0,4,0,4922\n",
      "918,19,0,239,0,1056,7,375,357,0,28,369,780,0,0,288,1,38,7,0,18,0,193,312,20,4,0,0,250,0,3,3,0,0,0,0,6,0,0,0,0,0,0,0,0,0,0,4709\n",
      "748,0,296,345,0,1768,36,0,16,0,616,0,759,45,114,146,60,2,466,523,1,22,228,66,3,18,750,0,50,0,0,94,31,0,0,7,24,0,0,2,1,0,9,0,0,0,0,2754\n",
      "1018,1,312,626,0,1478,229,10,205,64,49,71,0,268,195,0,810,39,53,41,131,0,208,128,0,0,0,10,191,0,0,132,66,0,0,0,0,0,0,1,0,0,0,0,1,0,0,3663\n",
      "1166,0,115,549,0,1842,0,148,221,169,433,30,314,4,22,0,42,32,74,194,16,0,21,0,0,0,0,0,256,0,0,203,183,236,0,0,0,0,0,1,0,0,0,0,0,0,0,3729\n",
      "408,0,14,855,0,377,703,286,126,234,0,7,343,151,85,0,19,126,321,697,535,3,0,0,45,1246,1,23,133,0,0,13,13,0,0,188,0,0,66,6,2,0,0,0,0,0,0,2974\n",
      "1507,0,715,90,0,1082,13,73,243,95,590,101,20,1112,237,194,0,27,316,381,0,71,27,0,0,0,0,0,203,0,0,38,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2865\n",
      "513,0,399,657,0,1207,241,424,355,0,596,123,7,95,0,60,45,0,158,0,784,0,568,31,0,22,97,1,162,0,2,96,97,0,16,6,0,0,16,0,0,0,0,15,0,0,0,3207\n",
      "2163,88,284,338,66,807,212,2,752,36,648,43,95,5,0,4,2,119,0,3,0,28,0,0,0,16,0,195,8,0,0,84,315,0,0,0,331,0,0,0,0,243,0,0,0,0,0,3113\n",
      "615,40,166,67,51,2187,17,6,242,14,0,61,0,2,14,10,3,2,320,51,61,0,145,0,0,0,62,0,51,0,30,16,6,0,0,0,126,0,0,0,0,84,5,261,0,0,0,5285\n",
      "619,875,82,526,19,698,209,0,314,0,113,0,127,1319,0,17,32,0,57,484,1,1,119,0,0,372,57,0,29,0,4,633,6,0,0,0,0,323,0,11,6,0,0,0,2,0,0,2945\n",
      "480,105,730,809,105,504,47,0,245,132,110,1189,3,1,119,176,262,263,2,66,0,81,48,1,0,0,177,1,343,0,3,264,8,0,0,13,129,0,0,0,1,0,0,0,0,0,0,3583\n",
      "1126,289,172,307,0,1998,134,0,12,1,66,20,119,14,60,0,0,17,48,3,0,508,34,1,0,32,0,26,106,0,25,12,30,0,0,0,114,0,0,6,0,0,0,0,5,0,0,4715\n",
      "1298,0,51,566,323,651,154,0,13,0,25,911,0,0,380,0,0,17,41,1694,0,161,28,2,5,0,205,0,1,0,14,296,160,0,0,0,10,0,0,13,0,0,0,0,0,0,0,2981\n",
      "607,2,7,233,0,2334,174,5,218,0,658,11,17,0,0,103,0,1,129,168,15,0,2,0,0,15,24,0,231,412,0,140,46,0,0,0,66,0,0,0,0,0,0,0,0,0,0,4382\n",
      "928,451,492,15,57,324,7,328,1159,0,3,7,2,0,71,7,4,158,0,70,0,0,133,0,4,1,323,0,85,0,43,73,283,0,0,0,1404,0,0,2,1,0,0,0,0,44,0,3521\n",
      "1360,0,1,118,154,894,36,564,201,24,113,105,2,5,242,32,90,51,121,284,205,74,286,216,218,0,46,54,388,7,17,1030,118,1,0,0,319,0,0,0,0,0,0,0,0,0,0,2624\n",
      "2852,0,36,578,12,376,130,176,55,0,38,1,2,65,28,0,37,0,44,61,0,5,6,4,0,0,13,14,670,0,1,101,36,0,0,525,26,0,0,1,0,0,0,0,0,0,0,4107\n",
      "1422,0,10,473,12,1085,103,21,5,1,435,0,73,9,63,179,1,4,74,149,68,94,109,13,0,20,25,0,91,0,0,118,58,0,56,17,897,0,0,0,0,0,0,0,0,26,0,4289\n",
      "2535,38,585,204,0,537,27,0,178,0,7,13,1,15,408,6,87,296,0,0,0,67,47,224,86,502,0,85,950,0,0,0,97,0,0,0,4,0,0,66,0,155,0,0,0,0,0,2780\n",
      "574,0,93,904,121,627,25,12,414,12,700,48,50,0,1045,1,66,9,14,3,9,6,40,0,186,0,0,66,0,0,0,390,0,0,0,30,112,0,19,0,2,0,0,0,0,0,0,4422\n",
      "200,46,172,135,0,2691,33,84,552,6,217,128,0,7,24,0,0,480,360,40,0,0,575,7,0,0,0,457,454,0,0,557,3,0,0,4,84,0,0,0,1,7,0,0,0,0,0,2676\n",
      "1003,0,440,265,6,225,204,51,320,35,121,3,110,15,1526,9,120,9,527,218,12,13,51,8,0,0,0,0,242,0,62,19,30,0,0,0,147,0,0,126,0,0,0,0,0,0,0,4083\n",
      "920,0,16,749,2,1117,637,53,183,0,20,0,405,1,363,0,1,0,61,61,0,3,2,1328,28,7,0,101,551,0,0,8,48,0,0,1,427,11,0,0,0,3,0,0,0,0,0,2893\n",
      "713,12,90,221,0,401,3,7,254,12,14,2,120,317,12,37,19,27,31,524,0,0,20,0,3,1,54,0,471,0,2,175,15,0,0,1,3,0,0,0,3,0,718,0,1,0,0,5717\n",
      "656,125,21,438,0,1332,72,0,89,1,306,23,26,9,30,2,1,0,2,322,773,6,716,43,0,18,0,2,69,269,71,468,1284,0,0,134,1,115,0,0,0,0,0,0,0,0,0,2576\n",
      "1637,0,91,687,0,2201,3,5,18,0,3,336,0,12,88,1,50,0,93,222,0,0,4,0,24,1,11,3,202,0,7,253,0,9,0,0,1,0,0,0,0,179,0,0,0,0,8,3851\n",
      "1271,5,178,893,0,719,61,72,342,0,61,99,58,0,7,37,3,155,12,46,0,8,86,150,0,3,125,0,13,0,55,101,201,0,0,0,1,0,0,0,531,0,0,0,0,0,0,4707\n",
      "1719,0,803,60,23,842,722,487,69,320,28,4,0,0,3,48,221,6,116,131,0,0,153,344,109,2,231,3,91,0,0,65,107,0,0,1,0,0,1,22,0,41,0,0,0,0,0,3228\n",
      "626,22,1,127,5,808,3,28,62,0,275,238,0,0,5,165,55,0,600,116,221,0,256,0,0,429,35,0,181,0,4,164,30,0,0,15,58,0,0,0,5,0,0,0,0,0,0,5466\n",
      "1289,0,0,818,0,1478,24,0,1219,0,249,0,415,1,9,0,7,0,26,168,0,67,681,143,0,5,3,5,267,0,23,349,4,0,0,0,1,0,0,2,2,0,0,0,0,0,0,2745\n",
      "296,0,473,404,139,2737,145,533,714,1,4,0,1,7,474,1,0,3,109,22,126,2,67,4,0,0,684,0,2,0,0,2,24,8,0,0,1,0,0,0,0,0,1,0,0,0,0,3016\n",
      "1014,68,12,773,129,1979,129,0,338,64,4,0,101,0,102,0,358,24,222,4,0,0,5,0,0,28,2,0,161,4,0,483,390,0,0,211,0,0,0,0,0,0,0,0,0,0,0,3395\n",
      "670,0,485,466,0,470,668,14,166,0,522,917,101,0,112,1,90,72,14,831,0,0,9,0,0,5,25,143,43,0,1,229,7,0,0,1,135,0,0,0,46,0,0,0,5,0,0,3752\n",
      "1558,1,89,253,0,865,220,0,547,1,14,0,28,0,81,5,4,280,2,232,0,32,801,0,0,65,0,0,1633,0,0,141,6,0,0,0,9,0,0,9,0,46,1,0,0,0,0,3077\n",
      "390,0,169,948,0,972,237,405,519,0,74,149,15,0,0,553,1,280,715,137,3,0,69,110,103,1,7,0,39,0,50,0,536,0,0,0,5,0,6,0,21,0,0,0,0,0,0,3486\n",
      "1011,5,1017,145,0,1484,3,19,415,42,87,27,59,195,0,59,45,1,14,0,0,0,112,0,57,94,0,146,122,0,408,77,425,0,0,0,3,0,0,0,0,0,0,0,0,0,0,3928\n",
      "683,0,775,1010,0,920,618,82,413,0,23,200,57,0,155,0,3,66,10,9,0,0,1,0,0,0,0,0,345,0,0,42,191,0,0,0,5,0,0,0,24,0,0,114,0,0,0,4254\n",
      "829,0,142,74,0,1363,21,55,267,76,30,12,374,908,970,0,103,65,3,559,852,2,144,0,0,35,0,0,124,0,272,143,4,0,0,0,13,0,0,0,0,0,105,0,1,0,0,2454\n",
      "1422,63,72,628,7,1572,38,43,42,118,85,478,0,0,5,627,77,6,100,250,0,0,0,29,0,8,0,0,844,0,0,136,155,0,0,0,41,3,0,6,0,0,0,0,0,0,0,3145\n",
      "1755,571,120,16,0,980,20,676,207,0,138,290,45,10,95,1,3,16,571,29,0,9,82,0,0,70,0,9,645,1,7,25,397,0,35,6,4,0,0,21,1,0,0,0,0,0,0,3145\n",
      "1391,0,226,493,14,1631,29,550,56,0,49,409,8,2,10,35,0,12,24,93,0,13,94,0,0,0,0,647,418,0,49,47,18,0,0,0,3,0,0,1,0,0,0,0,0,29,0,3649\n",
      "1321,5,41,1402,0,2246,6,112,370,12,24,73,3,3,525,1,93,89,24,598,0,286,584,11,18,11,1,19,199,0,11,0,298,0,0,10,31,0,0,0,58,0,3,0,0,0,0,1512\n",
      "1179,0,42,480,0,1437,210,0,27,178,5,353,25,0,1,103,7,68,126,32,0,1,313,0,0,43,14,5,246,1,0,181,41,0,0,0,477,0,72,0,0,0,0,0,0,0,0,4333\n",
      "1604,2,19,70,0,473,159,0,649,0,155,5,0,0,15,2,4,3,14,86,0,5,36,0,0,1,0,0,42,0,21,249,30,0,0,0,519,0,0,300,0,0,0,0,0,0,0,5537\n",
      "554,0,205,79,41,1768,107,4,51,0,72,453,0,0,116,0,0,852,1152,35,0,0,10,0,2,1,0,1,11,6,0,141,113,0,0,0,179,0,0,132,16,0,0,0,0,0,0,3899\n",
      "479,0,165,295,0,829,11,58,310,163,60,111,0,66,5,174,101,8,56,10,0,0,591,0,13,6,0,4,223,0,0,76,111,0,48,0,5,0,18,0,0,0,0,0,0,0,0,6004\n",
      "1061,0,165,342,0,1452,17,2,8,0,12,36,0,57,346,0,35,0,83,31,31,0,103,1007,0,0,1,1,45,0,61,608,3,12,0,0,23,0,0,0,0,0,0,0,0,92,0,4366\n",
      "203,0,620,223,0,3086,448,60,268,0,72,48,115,0,3,2,52,75,635,7,30,11,117,62,0,0,0,253,325,0,0,8,691,0,0,88,1,0,0,0,0,0,0,170,0,0,0,2327\n",
      "930,479,78,193,2,896,49,39,46,8,172,86,14,71,61,10,3,7,101,3,4,0,846,21,0,0,0,14,751,0,0,377,0,0,0,0,32,0,0,0,0,0,0,0,0,0,0,4707\n",
      "855,286,861,291,127,1157,194,0,61,81,184,0,13,0,62,109,0,0,119,338,0,181,321,0,0,0,77,0,163,0,21,37,21,0,0,0,97,0,1,0,0,67,0,0,0,0,0,4276\n",
      "306,3,315,327,20,1200,1156,0,586,258,325,9,256,56,82,0,2,54,27,101,0,7,123,0,51,2,1,4,746,0,18,3,11,0,0,0,1,0,0,0,3,0,9,0,0,4,0,3934\n",
      "1068,34,22,469,0,1756,135,0,407,32,385,9,8,59,0,2,1,5,10,11,5,0,83,66,0,0,0,0,29,0,831,2,0,0,0,0,17,0,0,0,0,0,0,0,0,0,0,4554\n",
      "336,3,875,836,43,847,384,0,379,0,1,1020,20,0,505,21,111,8,0,149,0,0,37,0,35,75,0,0,286,0,19,571,15,0,0,58,0,0,0,10,0,0,0,0,0,0,0,3356\n",
      "2123,0,1960,30,7,630,89,0,194,0,16,1,2,93,0,4,0,0,442,78,1,0,65,0,0,1,51,0,144,0,7,508,114,2,0,42,0,0,1,0,0,0,0,48,0,0,0,3347\n",
      "1347,88,1320,460,9,561,38,0,314,4,186,8,14,0,70,29,98,855,42,2,0,62,125,5,0,83,0,103,57,0,0,6,0,0,0,0,524,0,26,0,33,0,0,0,0,0,0,3531\n",
      "906,33,7,463,1,1108,19,21,0,332,30,0,10,4,295,0,56,63,103,1,10,0,0,43,0,27,186,157,168,0,132,116,11,0,0,0,228,0,0,0,0,0,1480,0,0,0,0,3990\n",
      "622,0,5,133,0,246,1,50,1678,0,1,33,107,256,22,0,25,1,15,48,4,32,1370,1,0,0,650,0,291,0,5,4,0,0,0,1,3,0,15,0,28,1458,0,112,0,0,0,2783\n",
      "1252,57,239,21,1,1367,27,349,392,88,71,8,10,2,0,10,414,475,43,53,0,0,923,9,53,0,0,25,0,0,0,204,10,0,0,0,3,0,0,2,0,0,0,0,0,0,0,3892\n",
      "1602,0,116,65,1,1050,230,0,1440,0,4,3,159,13,0,0,1,0,0,1149,0,25,373,1,0,169,0,330,74,0,4,8,1,0,0,0,2,0,0,0,0,0,0,20,0,0,0,3160\n",
      "1154,0,11,283,0,2135,275,340,8,50,0,28,265,43,73,0,159,0,130,63,5,62,939,6,345,140,0,0,18,0,1,227,59,0,0,2,52,0,0,0,0,0,0,0,0,294,0,2833\n",
      "2506,0,362,1352,0,950,1,137,239,0,0,29,133,0,9,0,269,1,84,11,2,4,540,0,0,0,0,127,44,0,31,102,33,0,0,0,46,0,0,0,0,0,0,0,0,0,0,2988\n",
      "1006,4,294,110,0,1161,34,87,1060,15,0,0,583,151,50,388,5,1,218,55,0,2,123,0,57,8,110,10,22,0,1,607,0,0,10,49,289,0,79,4,0,0,217,0,0,47,0,3143\n",
      "826,82,4,419,2,763,0,0,37,9,73,10,28,17,0,0,21,16,2,255,0,38,1027,0,12,21,0,0,487,0,3,191,311,0,0,6,651,0,0,3,0,0,2363,21,0,0,0,2302\n",
      "1125,0,278,201,0,1810,167,0,830,139,103,71,1,47,274,69,252,1,170,5,0,211,116,126,0,0,1,1,30,7,19,98,59,0,0,0,1110,0,0,0,0,0,0,0,0,0,0,2679\n",
      "807,0,307,274,1,1080,4,192,123,19,10,0,309,0,115,29,184,6,874,20,0,0,219,0,0,316,19,0,163,0,0,691,0,0,0,0,14,0,74,92,0,0,0,0,0,0,0,4058\n",
      "842,0,604,685,23,673,960,54,18,0,76,134,36,5,0,0,9,35,115,229,8,0,9,0,0,662,0,10,186,0,0,3,0,0,0,103,652,0,129,0,37,0,45,0,0,0,0,3658\n",
      "1216,0,3,1205,0,1130,276,6,569,1,3,6,0,158,260,2,11,67,46,0,0,539,120,373,0,0,0,0,4,0,0,29,152,0,0,2,1036,0,3,0,0,0,33,0,0,0,0,2750\n",
      "374,2,915,570,3,899,144,12,161,272,122,2,0,0,225,0,267,0,424,170,18,25,185,83,0,0,0,0,1639,0,21,46,0,0,0,0,49,0,0,5,0,0,0,0,0,0,0,3367\n",
      "1270,0,155,164,0,994,19,0,253,0,134,0,172,143,0,5,0,329,71,451,0,0,158,12,0,0,0,4,305,0,0,0,0,0,100,0,130,0,0,1,139,0,0,0,0,0,0,4991\n",
      "1744,7,111,494,123,378,40,0,541,6,799,419,0,0,124,42,0,2,58,89,40,7,36,0,0,0,0,2,64,0,56,15,9,0,0,0,487,0,5,0,0,0,0,0,0,0,0,4302\n",
      "563,69,137,420,493,693,14,0,58,396,364,0,35,76,17,0,31,5,154,752,111,19,268,0,0,0,0,5,6,0,454,111,76,0,0,0,1,0,0,0,0,0,17,0,0,0,0,4655\n",
      "1609,78,384,71,0,919,5,22,829,17,639,77,0,123,21,0,38,2,76,22,17,253,159,0,0,0,25,196,307,0,41,175,374,0,0,0,3,0,4,0,0,0,0,0,0,0,0,3514\n",
      "3410,17,85,151,127,633,186,31,109,0,3,13,0,21,3,63,80,1,5,0,815,524,200,0,104,29,0,0,416,167,112,1,0,0,0,44,9,0,0,0,1,0,0,0,33,0,0,2607\n",
      "2025,0,144,728,0,598,203,25,28,0,25,235,158,1,130,1,123,28,628,6,892,2,69,4,0,0,0,0,391,0,8,167,54,0,0,0,9,0,0,13,4,0,1,0,0,0,0,3300\n",
      "665,0,44,232,111,426,1128,40,51,76,51,801,255,63,1056,113,24,103,0,4,26,0,1505,354,0,9,0,55,129,0,2,24,24,0,0,0,121,0,81,0,0,0,0,12,0,0,0,2415\n",
      "614,0,64,228,0,916,144,67,306,18,0,88,19,0,93,6,58,4,137,27,0,0,50,5,54,15,0,331,1344,601,154,223,169,0,0,0,367,0,0,1,2,0,0,163,0,0,0,3732\n",
      "1440,0,158,248,0,1675,16,2,52,0,642,3,5,141,42,8,180,6,673,364,0,40,41,0,0,4,0,395,73,0,34,60,0,106,0,130,110,0,0,0,0,0,0,0,0,0,0,3352\n",
      "1203,22,126,433,94,1666,550,85,141,0,419,46,90,7,103,0,8,120,188,0,271,33,0,1,201,89,1,0,329,0,26,359,0,0,6,1,2,1,0,79,0,0,0,0,0,0,0,3300\n",
      "112,0,295,1016,0,1814,1,114,816,0,93,363,99,113,27,0,158,1,304,20,0,0,0,75,0,0,0,0,29,0,53,256,1064,1,0,3,0,0,0,81,0,0,0,0,1,0,0,3091\n",
      "496,0,7,10,0,1633,57,14,1017,0,529,1300,9,0,369,217,302,17,209,243,0,9,55,11,0,0,0,158,24,0,2,0,1030,0,0,0,8,0,0,0,0,0,0,0,0,0,0,2274\n",
      "1265,0,131,379,16,1632,27,0,307,0,227,11,106,0,254,0,419,4,13,21,0,2,105,17,0,0,0,0,1015,0,0,10,58,0,0,0,1,0,0,2,7,0,0,0,0,0,0,3971\n",
      "790,16,847,419,3,1454,82,26,1017,1,13,33,0,0,13,0,79,186,50,50,11,67,163,0,9,0,2,23,110,0,7,4,310,0,0,0,13,0,0,0,0,0,1,0,0,0,0,4201\n",
      "718,0,3,431,0,971,235,133,1,1,8,286,238,1147,125,12,57,734,150,194,0,0,344,12,0,1,0,0,447,0,0,20,418,30,0,0,141,0,0,0,0,0,0,0,0,0,0,3143\n",
      "1468,0,48,772,90,923,146,297,213,0,32,1,22,60,13,0,161,324,252,0,0,0,46,20,0,14,0,390,0,0,0,18,0,0,0,0,325,0,0,54,0,0,193,0,0,0,0,4118\n",
      "645,0,501,852,59,688,318,163,315,15,263,14,33,0,236,56,86,0,9,1,0,2,158,107,0,655,0,45,38,0,0,19,6,0,0,0,122,0,0,5,0,1,1,0,0,0,0,4587\n",
      "907,1,192,730,0,1783,144,14,900,7,46,323,27,0,171,0,19,49,426,3,1,1,31,0,0,446,221,6,266,0,0,180,4,0,0,378,27,0,0,0,0,0,0,50,0,0,0,2647\n",
      "434,63,90,1013,345,974,546,0,332,32,98,0,2,100,415,20,16,0,35,63,1,1,1231,1,0,0,0,0,13,0,15,97,313,0,0,0,0,0,0,0,0,0,54,0,0,0,0,3696\n",
      "1509,0,224,261,0,1710,639,0,153,201,872,15,0,10,20,0,48,0,1,26,4,0,0,0,0,0,51,0,323,0,0,9,3,0,0,5,1,0,0,0,0,0,0,0,0,0,0,3915\n",
      "790,484,57,60,474,1181,6,0,190,529,971,136,29,0,298,2,6,2,372,496,0,1,267,48,4,6,0,0,95,0,0,155,208,0,0,24,85,0,0,0,0,0,0,0,0,0,0,3024\n",
      "1099,15,14,66,0,2675,200,9,398,3,31,160,43,20,35,72,236,8,240,345,27,221,92,31,0,1,2,39,70,0,55,1,265,0,114,0,76,0,0,0,0,59,0,0,0,0,0,3278\n",
      "612,269,103,621,0,2229,46,239,17,0,0,114,6,136,231,98,10,2,370,17,13,459,80,8,0,127,0,0,2,0,63,6,61,0,0,231,2,12,2,0,0,0,0,0,0,0,0,3814\n",
      "440,0,612,929,6,1449,27,15,59,0,752,535,122,1,2,45,0,194,444,449,222,0,232,460,2,5,0,2,639,1,0,360,60,36,0,1,217,0,0,94,0,27,0,0,0,0,0,1561\n",
      "2452,22,178,909,0,400,88,12,100,11,51,103,0,156,57,672,335,24,33,39,12,147,374,1,26,0,0,414,27,0,0,61,289,0,0,0,46,0,0,0,0,0,0,0,0,0,0,2961\n",
      "1591,0,5,126,3,1245,175,4,472,2,2,4,0,0,491,22,4,26,0,55,0,0,5,0,46,8,0,43,60,0,0,56,227,0,0,60,670,0,0,53,181,0,0,0,0,0,0,4364\n",
      "1074,7,1387,215,5,715,15,37,313,0,131,0,318,0,25,1,7,0,117,5,31,0,551,2,0,112,0,39,369,0,2,410,3,0,0,0,3,0,0,0,25,1,0,0,0,0,0,4080\n",
      "915,2,237,280,0,1429,604,0,988,47,530,2,0,106,10,0,685,0,34,0,0,529,571,0,0,0,3,4,455,2,0,82,0,0,0,0,13,0,0,0,45,0,0,0,0,0,0,2427\n",
      "488,0,113,241,29,1266,47,14,182,0,7,102,61,101,20,6,70,169,364,116,0,1,19,97,0,0,0,5,52,0,38,1108,3,0,0,1,3,0,2,2,68,0,0,0,0,0,0,5205\n",
      "539,45,1105,1636,0,287,8,1,645,0,358,7,34,6,285,22,1,0,22,224,0,1,99,2,0,5,0,0,194,0,0,1,474,0,0,5,44,0,0,0,0,0,0,0,0,1,0,3949\n",
      "662,32,173,1973,223,684,11,13,525,0,231,4,33,1,349,101,1,20,215,27,0,56,668,26,0,0,99,0,25,0,194,201,0,1,7,0,0,0,0,0,0,0,0,0,0,0,0,3445\n",
      "753,0,29,842,44,595,39,0,345,0,24,31,0,15,174,9,499,1,1,3,218,2,35,11,272,0,0,0,176,0,0,9,87,0,0,89,13,0,0,0,0,0,0,0,0,0,0,5684\n",
      "1620,0,71,142,0,1413,828,27,433,0,781,15,0,443,23,0,0,6,128,325,28,0,157,0,2,11,0,0,148,0,5,58,136,0,0,1,12,0,211,0,0,0,0,0,0,0,0,2976\n",
      "199,1,114,971,0,1558,337,0,788,0,53,44,7,237,42,6,23,125,343,417,3,44,27,0,2,15,0,0,31,0,0,1,338,0,0,56,28,0,0,0,0,0,0,73,0,0,9,4108\n",
      "1094,0,118,477,0,1522,106,1,878,83,133,39,4,0,54,0,97,19,3,0,2,406,156,0,0,105,0,0,672,0,0,50,9,0,0,227,22,0,0,0,1,0,0,20,0,4,0,3698\n",
      "1073,0,1083,137,6,1352,81,9,78,0,52,0,0,282,13,0,404,45,58,0,0,0,310,10,0,1,0,37,436,0,0,211,200,0,0,0,0,0,0,9,0,1,0,0,0,0,0,4112\n",
      "831,0,198,97,151,1117,337,0,117,0,90,56,307,1,75,0,1,28,170,0,0,0,554,0,11,2,0,0,201,0,0,101,27,0,165,0,21,0,0,0,0,0,0,0,0,0,0,5342\n",
      "900,110,418,455,5,2035,150,49,73,0,0,178,203,0,0,0,80,4,405,110,0,221,12,22,0,0,0,779,59,0,2,777,15,0,0,0,12,0,1,0,25,0,0,0,0,0,0,2900\n",
      "636,0,608,49,7,1144,160,116,1700,48,585,37,15,0,282,7,141,77,130,2,1,0,388,17,0,9,0,69,37,0,76,133,108,0,0,8,0,0,4,0,0,0,0,0,4,0,0,3402\n",
      "1365,0,33,496,0,349,157,0,1269,2,7,4,165,202,133,0,191,9,221,0,0,0,1259,0,0,2,2,28,71,0,0,4,1460,0,0,0,23,0,0,0,60,0,1,0,0,0,0,2487\n",
      "1149,108,285,926,1,789,16,110,49,8,8,1,5,42,739,0,4,103,0,568,6,0,4,0,0,273,0,4,192,0,15,55,40,0,0,1,13,0,0,0,0,0,0,0,0,0,0,4486\n",
      "1148,15,463,463,3,1166,382,0,425,2,0,0,69,2,47,69,0,10,37,1,0,0,134,0,0,6,2,12,59,0,0,1061,7,0,0,0,33,0,0,0,0,0,0,0,0,0,0,4384\n",
      "1670,46,4,348,0,1462,42,15,110,13,1,22,230,15,16,0,1,116,31,34,521,0,210,258,0,4,144,499,340,0,83,26,0,0,0,0,0,0,4,12,0,0,0,0,0,0,0,3723\n",
      "604,0,112,266,0,917,218,7,315,1,23,922,463,0,36,1,224,0,1,40,0,83,279,42,0,0,29,0,267,0,301,645,102,0,0,1,0,0,8,16,0,0,9,0,0,0,0,4068\n",
      "1923,32,32,343,8,858,96,0,472,0,16,12,64,1,47,1,0,1,3,1398,570,175,0,0,0,0,0,1,17,0,0,44,51,0,0,30,671,0,3,3,0,108,0,1,0,0,0,3019\n",
      "465,0,852,688,0,1633,102,240,375,0,4,72,90,355,34,51,19,0,1,19,0,53,450,0,0,0,17,0,6,4,0,120,34,0,0,0,1059,0,0,0,0,0,0,0,0,14,0,3243\n",
      "886,0,186,79,1,360,0,116,256,0,316,417,87,477,11,156,108,67,93,248,0,0,13,14,0,16,3,70,18,0,0,455,91,0,0,0,183,0,0,0,0,0,0,0,0,0,0,5273\n",
      "963,0,150,776,51,977,266,0,474,0,66,9,13,5,391,0,4,13,0,5,3,244,52,0,1,0,0,0,92,144,0,12,53,0,0,0,8,0,5,0,0,0,0,0,0,0,0,5223\n",
      "2492,106,29,363,0,793,144,18,229,0,156,35,1,0,5,12,2,148,60,369,1,151,41,0,2,137,0,247,801,0,0,200,15,0,0,0,58,0,1,0,2,3,0,0,0,0,0,3379\n",
      "327,11,4,48,3,1783,949,366,38,0,20,490,95,0,25,15,40,52,9,159,0,0,3,160,0,0,32,0,208,1,11,40,16,0,0,2,44,0,13,0,0,0,73,0,0,0,0,4963\n",
      "337,0,1,455,15,1740,12,0,264,1,46,76,0,72,142,0,1,0,369,790,805,132,27,0,0,81,0,12,0,0,1,448,76,0,0,0,323,0,0,0,0,0,0,0,0,0,0,3774\n",
      "785,64,1035,215,7,1787,237,0,145,118,8,100,0,9,35,18,0,34,24,143,5,0,329,109,0,0,0,0,421,0,31,351,1,0,0,366,8,0,15,0,0,0,0,0,0,0,0,3600\n",
      "312,0,83,810,0,300,37,14,152,0,154,46,0,144,24,10,1,40,375,6,0,0,42,29,0,84,0,1,37,0,0,26,343,98,0,0,93,0,0,0,0,0,120,0,0,0,0,6619\n",
      "1111,14,439,97,1,1043,2,53,1658,35,67,56,0,18,0,0,10,29,6,60,0,0,18,1,0,0,0,0,25,0,41,46,297,0,0,0,59,0,376,2,0,0,1,0,0,0,0,4435\n",
      "1682,0,104,7,0,1791,927,0,33,0,11,0,0,57,6,24,2,73,0,242,64,72,396,0,5,0,16,2,767,0,0,1101,63,0,0,0,324,0,1,74,0,0,0,0,0,0,0,2156\n",
      "1599,348,367,93,0,1423,380,251,169,2,9,0,31,0,30,0,1,0,41,27,1,27,398,2,0,46,0,0,310,0,0,296,1,0,0,0,55,0,0,1,0,0,2,51,0,0,0,4039\n",
      "627,0,216,303,0,469,320,180,349,3,0,0,180,1,0,1,156,0,3,533,0,118,26,0,0,0,0,0,1017,0,178,54,64,0,0,0,0,0,0,0,213,0,0,0,0,0,0,4989\n",
      "630,0,440,447,117,300,86,24,237,269,127,49,106,4,372,87,0,23,1672,142,24,0,1043,68,0,4,9,0,15,0,0,319,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3380\n",
      "1299,144,273,341,0,824,560,1,185,2,423,39,1,86,5,0,12,33,10,9,2,48,68,0,0,277,7,0,73,0,413,66,35,0,0,3,1,0,0,0,0,0,0,0,0,0,0,4760\n",
      "410,0,207,198,0,2825,32,8,495,0,75,201,1,195,22,0,315,0,182,32,0,1,561,1,126,2,0,0,180,0,0,1,0,0,0,0,5,23,1,7,0,0,0,0,0,0,0,3894\n",
      "1807,0,42,30,4,1057,420,35,269,5,54,0,201,10,52,0,49,63,360,0,625,9,10,0,0,3,0,5,84,0,0,14,73,30,0,0,0,0,0,0,0,1,0,0,0,0,0,4688\n",
      "935,7,282,174,250,712,442,5,168,2,92,0,6,152,13,327,12,0,292,0,1,406,0,12,0,0,0,0,945,0,0,730,0,0,0,0,38,0,0,0,0,0,6,93,0,0,0,3898\n",
      "869,13,0,815,0,2231,0,18,91,41,0,4,143,0,7,0,4,0,98,353,0,1,0,0,24,0,0,249,335,0,217,13,4,0,0,21,212,0,0,0,0,0,0,0,0,0,0,4237\n",
      "770,110,63,30,2,655,28,0,112,0,8,190,16,96,76,0,16,0,7,191,0,201,308,0,4,18,26,0,369,0,0,262,26,0,0,0,127,0,136,0,0,0,0,0,0,0,0,6153\n",
      "833,0,628,267,41,951,189,0,348,0,43,1,2,0,128,12,0,7,24,0,4,159,17,0,0,0,0,0,41,0,322,10,0,0,0,0,55,0,0,0,0,0,0,0,0,0,0,5918\n",
      "293,436,29,1513,30,1895,38,0,41,19,11,264,46,48,96,0,140,0,45,88,0,34,24,159,106,5,0,0,421,0,4,697,61,0,0,0,0,0,0,28,35,0,0,0,0,0,0,3394\n",
      "967,0,5,387,662,2190,347,3,330,0,36,4,0,215,30,0,34,131,85,4,202,0,0,0,0,384,0,0,191,22,0,112,0,0,0,0,100,0,1,1,0,0,0,0,0,0,0,3557\n",
      "161,0,12,645,2,571,163,0,227,0,28,37,16,1,417,39,1,39,685,138,0,6,142,0,1,0,2,57,753,0,20,318,0,0,0,7,17,6,0,17,0,1,0,0,0,0,2,5469\n",
      "406,0,6,796,24,1886,22,8,153,30,885,12,19,2,345,627,274,0,415,72,0,21,172,46,33,0,21,107,17,0,0,345,185,173,0,0,0,0,1,46,0,0,15,0,0,0,0,2836\n",
      "522,1,98,78,29,1206,142,873,569,0,13,1,10,36,45,94,126,1,0,119,0,0,347,0,0,0,0,399,113,0,0,255,0,0,0,0,50,0,0,0,0,0,0,0,0,0,0,4873\n",
      "1126,0,516,125,35,2657,88,0,43,0,95,9,3,2,258,279,0,0,70,2,0,0,0,22,0,172,0,17,335,0,0,283,2,0,0,0,2,0,0,1,0,0,2,0,0,0,0,3856\n",
      "1210,0,116,724,17,827,44,248,818,0,176,375,29,0,8,37,40,2,57,151,62,0,564,16,0,2,0,1036,50,0,140,0,0,0,0,35,222,0,36,0,0,0,0,0,0,0,0,2958\n",
      "1885,7,628,651,50,1447,66,28,174,0,62,30,81,0,14,88,348,75,69,324,0,76,47,60,0,1,0,26,28,0,348,1,184,0,0,0,25,0,0,0,0,0,0,0,0,0,58,3119\n",
      "656,7,72,51,9,2707,481,12,175,6,20,17,21,283,33,563,0,1,33,20,13,0,5,261,0,0,1,4,268,0,0,240,28,4,0,0,103,0,0,4,0,0,0,0,0,0,0,3902\n",
      "1192,8,318,254,193,716,1288,2,32,0,12,3,0,63,61,0,55,7,1,138,0,0,45,7,0,19,528,0,17,0,314,61,3,56,0,0,98,0,0,0,0,0,0,0,0,0,0,4509\n",
      "394,0,95,400,2,2224,92,4,401,4,602,0,2,0,130,0,96,0,29,44,119,3,592,48,3,0,4,0,126,0,38,1,76,0,90,0,3,0,0,0,0,0,0,0,0,0,0,4378\n",
      "818,0,23,498,0,967,294,218,40,15,10,15,0,21,21,1,39,0,0,24,0,0,120,7,0,2,568,0,652,0,249,501,0,0,0,487,23,0,0,0,0,0,0,0,0,0,0,4387\n",
      "756,0,107,548,0,911,10,0,364,249,0,140,145,11,467,0,281,403,993,80,0,51,1,0,0,3,1,0,9,0,0,120,0,0,0,4,119,0,0,36,1,0,0,0,0,0,0,4190\n",
      "1690,0,5,112,0,1214,57,33,42,0,212,7,89,103,27,0,11,0,37,100,1,0,5,149,5,0,0,0,278,0,42,11,5,4,0,0,153,0,0,2,0,282,0,3,0,0,0,5321\n",
      "887,0,115,246,20,1021,12,111,103,72,52,123,0,1,263,0,135,1,556,275,250,150,661,0,0,39,487,4,71,0,1,396,519,0,0,0,681,0,99,1,0,2,0,0,0,0,0,2646\n",
      "704,16,682,532,33,929,0,2,274,12,17,0,113,23,7,9,1,195,46,1,0,0,13,0,85,2,0,0,446,0,1,191,296,0,0,0,1024,0,0,69,81,0,0,0,0,0,0,4196\n",
      "2505,0,86,46,3,1043,119,27,180,251,2,23,340,0,333,0,4,135,42,0,0,335,10,0,443,7,0,0,110,0,4,30,465,0,0,0,32,0,0,0,180,0,0,0,0,0,0,3245\n",
      "327,8,336,587,0,899,0,5,56,0,30,0,44,1,6,0,70,78,224,85,0,0,79,49,0,1,0,0,469,154,72,116,24,2,0,1,581,0,0,0,0,0,0,0,0,0,0,5696\n",
      "517,0,103,869,39,1600,40,46,100,6,119,3,7,253,145,6,36,0,4,212,0,0,17,0,0,69,0,5,7,363,0,21,413,0,0,0,262,0,0,0,0,0,0,0,0,0,0,4738\n",
      "753,0,401,300,0,954,4,237,482,0,824,480,210,0,2,324,2,0,0,6,37,21,93,186,5,0,0,0,117,0,4,503,6,0,0,0,252,0,0,0,12,0,0,0,0,0,0,3785\n",
      "729,26,497,44,10,968,16,281,270,1,315,12,3,460,19,52,59,690,46,297,8,2,154,0,944,384,0,5,147,0,0,71,1,0,0,0,343,0,0,19,0,0,36,0,0,0,1,3090\n",
      "1223,0,204,171,0,1401,437,38,281,91,284,0,55,0,4,71,138,88,2,78,0,10,10,166,0,0,0,0,66,0,0,13,21,0,0,0,132,0,0,0,0,0,5,0,0,0,0,5011\n",
      "1148,30,266,678,0,1222,526,569,1289,0,20,53,0,0,62,39,44,0,642,0,0,0,435,0,0,0,0,1,64,0,0,385,183,0,0,0,74,0,0,2,0,0,0,0,0,0,0,2268\n",
      "1087,0,97,443,0,2190,142,273,148,30,57,0,21,89,502,0,214,3,18,240,134,153,12,0,0,0,0,0,103,0,0,23,0,0,0,0,8,0,7,0,0,0,0,0,0,0,0,4006\n",
      "1384,75,1734,589,0,1332,152,1,211,697,4,3,111,58,3,40,0,0,27,67,16,39,3,0,4,79,0,8,82,0,0,84,17,0,0,0,122,0,0,0,8,0,0,0,0,0,0,3050\n",
      "661,0,981,139,0,871,27,201,66,0,389,87,198,0,286,2,190,3,13,0,0,47,471,7,0,100,676,0,113,0,0,11,5,0,0,0,152,0,0,2,0,0,0,0,0,0,0,4302\n",
      "1501,14,1029,137,29,1546,245,1,92,8,22,134,65,0,70,0,152,24,2,11,0,0,8,0,79,0,0,97,196,0,0,9,571,0,0,0,18,0,0,0,0,0,0,0,0,0,0,3940\n",
      "1423,167,74,206,0,350,244,1,140,41,10,0,19,0,9,1,482,62,82,9,91,773,73,0,0,807,0,49,11,0,14,6,0,0,0,135,353,0,0,0,0,0,733,0,0,0,0,3635\n",
      "183,0,124,272,52,1898,361,2,135,0,1,0,104,0,57,17,2,1180,322,424,0,0,84,2,0,0,0,0,258,3,7,261,112,0,0,0,188,0,0,0,0,0,0,0,0,0,0,3951\n",
      "545,8,64,1109,182,691,763,0,242,102,437,1,0,0,2,1,1222,77,606,12,1,62,123,0,0,0,0,1,496,29,0,71,34,0,0,0,134,0,278,0,0,0,0,0,0,0,0,2707\n",
      "647,0,26,435,16,3785,979,355,80,0,7,363,7,0,4,0,58,2,254,186,0,0,39,0,0,54,0,0,120,0,3,5,2,0,0,0,84,0,0,25,0,0,0,0,0,0,8,2456\n",
      "3960,45,122,388,6,921,153,41,4,11,124,0,3,64,675,1,0,0,57,35,3,0,12,0,0,0,0,14,246,0,4,5,6,0,0,0,172,0,0,0,0,0,0,0,0,0,0,2928\n",
      "1047,0,9,206,37,1013,435,134,360,228,4,0,162,0,95,159,833,35,2,301,0,0,212,0,0,478,0,0,15,0,0,30,2,7,0,0,465,0,0,214,14,0,0,0,6,0,7,3490\n",
      "1907,54,45,29,2,2607,546,1,75,0,152,198,1,92,0,0,13,4,14,2,0,0,40,253,25,5,30,0,99,0,2,6,281,0,0,192,135,0,0,0,0,0,56,0,0,0,0,3134\n",
      "1123,0,158,68,58,1290,95,66,311,734,1,298,0,2,13,1,2,0,514,0,43,0,485,0,0,53,1,0,9,0,32,8,88,0,0,0,11,0,0,0,0,0,0,0,0,0,0,4536\n",
      "1087,0,63,807,1,1786,19,3,282,1,28,268,0,0,39,1,1,190,144,20,0,37,55,0,0,1,0,0,27,0,0,20,956,0,0,0,12,0,0,0,0,0,0,0,0,0,0,4152\n",
      "477,2,196,601,12,411,249,2,540,201,140,537,21,0,906,0,0,6,449,0,0,2,136,209,0,117,0,96,1,0,0,327,1,0,0,3,49,0,0,0,0,0,0,0,0,0,0,4309\n",
      "176,0,7,775,0,2240,1,1,454,0,0,1,0,80,0,115,378,0,397,98,0,34,126,0,0,3,0,0,766,0,0,142,10,0,0,0,3,0,0,0,0,0,0,0,0,0,0,4193\n",
      "713,97,160,215,0,1594,120,3,484,19,12,129,148,2,0,0,351,1,1,0,12,77,52,20,0,0,101,0,246,0,0,71,0,0,0,33,86,0,0,0,0,0,126,14,0,0,0,5113\n",
      "820,2,382,1016,0,1176,167,42,116,76,89,31,760,0,0,106,529,0,62,273,0,0,24,0,0,0,0,20,36,0,0,296,68,0,0,0,81,0,0,1,3,0,4,0,0,0,0,3820\n",
      "1655,0,294,246,285,1009,38,59,75,0,203,29,0,1,18,16,424,294,186,259,12,0,11,44,0,44,0,0,104,0,0,9,0,0,0,25,65,0,0,0,0,0,0,0,0,0,0,4595\n",
      "1236,0,454,681,0,1265,87,0,17,3,3,27,3,112,210,0,13,131,81,1011,0,134,7,13,0,5,0,6,205,0,294,336,56,0,0,16,145,0,0,179,75,0,354,0,0,0,0,2841\n",
      "689,30,204,35,301,915,45,0,161,0,461,1,359,79,122,18,39,0,93,7,803,1,367,4,0,596,0,0,52,0,1,51,183,0,0,0,4,0,0,2,0,0,5,0,0,0,0,4372\n",
      "1305,0,1082,33,15,1163,15,0,923,861,35,23,75,38,79,0,276,40,2,221,187,46,38,0,0,0,2,24,211,0,0,224,30,0,0,75,0,0,0,0,0,21,0,0,0,0,0,2956\n",
      "710,0,764,260,0,2348,0,27,189,9,68,12,58,0,163,18,309,8,130,20,0,0,75,0,209,42,0,0,48,19,0,378,12,0,0,0,8,0,0,0,0,92,0,0,0,0,0,4024\n",
      "2452,0,92,267,321,1345,630,0,558,0,111,81,114,0,17,0,92,252,156,74,0,0,50,0,0,5,0,0,425,0,1,18,322,0,0,8,21,0,0,0,0,0,6,0,0,0,0,2582\n",
      "1650,0,185,219,0,1482,44,106,330,0,1,57,0,1,1,95,243,1,0,52,1,43,21,0,0,0,7,39,931,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,246,4241\n",
      "902,154,2,181,0,1013,1,1,1486,4,47,75,10,6,14,0,12,312,66,599,0,825,6,0,0,134,0,172,7,0,18,4,15,0,0,29,99,0,0,6,0,0,0,0,0,0,0,3800\n",
      "582,70,416,126,146,633,170,0,10,0,165,0,7,24,1162,3,32,15,29,444,147,1,195,0,0,18,0,131,89,0,0,11,12,0,0,140,1,0,3,0,0,0,6,0,0,0,0,5212\n",
      "650,76,352,73,20,1164,56,320,21,0,5,52,0,0,387,0,79,0,4,1677,5,93,185,0,0,215,0,128,194,0,0,111,227,0,0,0,41,0,0,0,0,0,0,0,0,0,0,3865\n",
      "302,0,304,470,6,1895,301,1,226,0,39,3,199,5,38,568,194,63,227,51,628,0,135,0,0,0,123,0,64,0,214,90,414,0,0,0,318,0,0,146,538,0,0,27,0,0,0,2411\n",
      "391,0,18,960,1,537,39,0,44,28,615,0,29,0,0,147,71,0,5,55,186,0,408,31,32,0,0,0,89,0,529,378,6,0,1,23,0,0,0,0,65,0,2,0,0,0,0,5310\n",
      "1105,61,315,230,0,767,102,0,201,132,371,115,65,50,35,0,0,109,177,0,0,297,412,183,0,0,1,0,34,0,1,354,80,0,0,0,539,0,0,5,0,0,0,0,0,0,0,4259\n",
      "365,47,40,394,88,3011,10,257,179,194,0,34,1,688,268,508,0,17,245,17,33,0,47,1,19,0,39,0,35,412,0,14,0,0,0,15,14,1,0,0,0,0,0,0,0,1,0,3006\n",
      "558,25,254,961,36,681,3,144,124,5,141,234,63,8,526,0,152,2,250,15,247,0,116,0,0,1,0,0,187,1134,0,2,25,0,95,0,69,0,0,0,0,60,0,0,0,0,0,3882\n",
      "343,0,573,605,8,1269,0,144,206,50,18,106,728,5,304,32,20,44,31,147,0,0,185,0,0,0,1,0,334,159,140,63,333,0,0,0,39,1,0,0,0,594,0,0,0,0,0,3518\n",
      "2667,0,17,516,221,567,115,0,789,54,206,886,88,0,143,34,130,207,30,19,1,6,27,2,0,15,0,0,496,0,0,65,0,0,0,2,112,0,0,0,0,0,0,0,0,0,0,2585\n",
      "946,0,143,636,0,1350,5,0,167,224,16,41,67,58,0,13,129,8,14,362,15,0,124,1,0,3,1,21,237,0,21,173,0,0,0,0,405,0,0,0,0,1,1,0,0,23,0,4795\n",
      "174,0,519,362,0,1349,130,16,190,13,15,0,25,33,4,12,28,20,130,66,0,169,57,152,0,0,0,292,790,0,0,108,162,0,0,0,130,0,0,0,3,0,0,0,0,0,0,5051\n",
      "1936,0,245,924,0,322,607,11,2,61,68,212,11,2,749,35,203,4,7,59,3,0,15,0,0,0,0,0,212,0,167,15,64,0,0,13,746,0,7,73,0,0,0,0,0,0,0,3227\n",
      "322,1,4,1274,0,746,221,10,805,0,20,26,0,25,113,0,11,24,274,89,89,242,110,5,0,92,0,32,27,0,0,2,17,0,0,0,67,0,16,492,0,0,0,0,0,0,0,4844\n",
      "1967,0,227,381,8,667,234,27,0,0,57,126,60,9,121,0,0,168,69,5,80,1,78,0,0,0,0,106,172,0,0,476,25,0,0,0,10,0,0,0,2,0,0,0,0,0,0,4924\n",
      "418,0,301,729,456,912,55,30,132,274,211,495,0,10,349,28,314,35,12,0,0,99,368,0,0,0,11,0,24,0,30,14,54,0,0,0,6,0,0,456,0,0,1,0,0,0,0,4176\n",
      "859,120,42,57,12,406,119,167,96,0,68,127,12,4,66,0,294,662,41,0,2,2,0,76,15,811,506,15,213,0,16,261,3,0,0,4,46,0,0,0,0,0,0,0,0,0,0,4878\n",
      "547,0,10,29,6,1856,104,0,38,11,143,82,502,122,660,43,55,4,92,43,11,189,228,0,0,0,103,2,519,0,0,38,69,0,0,0,50,0,0,0,0,0,0,0,0,0,0,4444\n",
      "2291,5,190,792,4,663,95,374,430,0,371,20,7,3,169,0,522,0,42,98,0,4,0,34,0,6,0,2,319,7,0,190,774,0,0,0,0,0,0,4,1,209,0,0,954,0,0,1420\n",
      "794,0,647,1016,0,1829,121,11,36,0,142,0,15,5,46,5,4,4,42,150,4,0,93,4,0,1,1,5,32,0,7,237,189,0,0,9,52,0,0,0,0,0,0,0,0,0,0,4499\n",
      "1770,159,10,407,0,1199,9,91,1083,0,82,1,127,1,58,28,1246,122,3,4,9,162,136,0,6,0,40,62,497,0,0,15,2,1,0,0,259,0,0,0,0,0,12,0,0,0,0,2399\n",
      "1523,0,12,664,0,965,24,171,15,593,15,56,0,90,261,13,0,3,96,91,15,3,262,0,0,51,0,15,14,0,0,15,32,0,0,0,101,0,0,383,0,0,0,0,0,0,0,4517\n",
      "652,4,459,887,2,798,77,75,258,4,268,0,0,0,206,1,8,319,18,314,45,0,13,0,0,145,54,0,502,0,14,706,13,3,0,0,99,0,0,0,0,0,0,0,0,0,0,4056\n",
      "1829,0,88,137,0,1534,27,1,145,0,81,40,7,1,843,6,272,0,224,13,12,0,490,5,0,1,0,1,722,0,0,432,47,31,0,0,50,0,0,14,0,78,0,0,0,0,0,2869\n",
      "2285,127,255,755,0,1430,2,91,30,1,122,0,222,127,6,4,133,3,192,50,0,0,83,0,0,91,31,1,6,0,0,4,0,0,0,0,23,0,0,0,0,0,57,0,0,0,0,3869\n",
      "862,0,1019,136,17,1287,36,240,98,88,149,429,148,306,0,0,1,36,443,1,60,0,89,21,4,236,148,3,367,0,0,185,12,0,0,4,91,0,0,3,93,2,0,0,0,0,0,3386\n",
      "990,51,84,230,0,2364,165,11,74,0,64,568,178,3,16,0,7,7,334,141,0,192,277,1,0,60,0,0,554,0,0,134,13,0,0,0,8,0,0,0,0,0,39,0,0,0,0,3435\n",
      "2315,5,480,752,0,1054,1,15,549,0,0,0,379,4,2,121,62,1,138,2,4,0,190,0,0,1,0,0,126,0,0,3,6,0,0,0,47,0,0,164,0,0,0,0,0,0,0,3579\n",
      "158,0,611,788,16,337,1062,18,39,89,4,174,1,0,4,396,30,0,275,225,59,0,260,105,0,0,972,183,109,0,0,140,147,0,0,2,9,0,0,0,0,0,0,0,5,0,0,3782\n",
      "998,14,145,318,82,1385,888,0,56,0,0,302,13,45,188,17,0,2,19,1,0,0,1,0,8,179,37,1,315,0,0,923,81,0,0,0,18,0,0,0,0,0,47,0,0,0,0,3917\n",
      "100,0,28,0,0,1177,42,248,93,23,147,261,14,77,29,0,10,22,120,16,0,3,295,901,5,0,121,0,247,0,32,49,57,0,0,0,475,0,719,0,0,0,0,3,0,0,0,4686\n",
      "2587,2,82,117,16,1381,88,0,166,67,364,64,0,3,3,363,118,1,32,0,0,0,75,0,0,0,0,105,628,0,7,0,2,0,0,0,39,0,0,0,40,1109,134,0,0,0,0,2407\n",
      "1136,0,703,493,0,776,119,3,4,91,0,38,0,246,114,96,115,0,298,39,0,0,157,19,0,1,4,1,864,0,0,3,211,0,0,0,83,0,0,0,2,0,0,0,0,0,0,4384\n",
      "247,28,177,76,19,1907,350,1,714,69,90,78,0,497,225,36,116,2,72,93,11,0,7,0,10,0,0,0,74,0,0,346,22,0,0,0,12,0,0,0,0,0,0,0,0,5,0,4716\n",
      "271,0,171,673,2,918,22,243,17,0,1,318,101,10,172,171,830,1,126,6,0,23,332,0,0,24,0,0,35,0,1217,61,267,0,0,0,35,0,0,15,0,0,0,0,0,0,0,3938\n",
      "1030,28,375,64,0,2046,221,2,433,0,299,120,178,157,555,14,128,326,29,111,0,0,3,0,0,1,4,0,610,0,22,19,13,0,0,0,65,0,0,0,0,0,0,0,0,0,0,3147\n",
      "421,194,32,90,0,1604,252,0,1063,113,13,0,40,295,478,41,26,0,249,4,0,0,148,0,0,2,0,0,392,0,74,4,15,0,0,0,15,0,0,0,0,0,0,0,0,0,0,4435\n",
      "658,1,16,636,0,1035,524,30,229,190,741,53,76,390,7,0,194,1,54,89,0,0,4,180,2,5,13,0,214,315,0,58,4,0,0,6,27,0,54,0,0,0,0,0,0,0,0,4194\n",
      "603,0,362,129,0,2403,91,23,22,0,261,1,108,0,93,0,72,51,3,395,0,7,14,18,0,13,0,0,40,0,0,1172,2,0,0,117,37,0,0,0,0,92,0,0,0,0,0,3871\n",
      "427,81,92,260,61,2507,285,0,38,0,269,33,931,44,0,1,58,37,22,172,1,113,82,6,0,0,0,0,584,0,0,13,216,0,0,0,8,0,0,6,0,0,0,0,0,0,0,3653\n",
      "1222,0,861,678,0,677,140,13,753,0,131,16,3,70,18,7,30,0,106,101,4,42,1,0,0,0,21,49,671,0,0,100,1,0,0,35,624,0,0,184,0,0,0,0,0,0,0,3442\n",
      "1977,10,325,1564,0,493,1292,0,570,0,46,5,141,42,256,0,191,47,196,105,0,0,7,0,0,32,40,0,7,0,315,2,2,0,0,0,28,0,0,0,1,0,0,0,0,0,0,2306\n",
      "351,10,123,9,0,2318,0,0,292,2,1196,0,0,1,1,0,298,0,1,58,0,0,99,1,0,0,1,0,457,0,0,135,164,0,0,0,81,0,0,1,0,0,0,0,0,0,0,4401\n",
      "922,0,382,1982,2,299,358,0,125,246,20,0,9,18,178,5,0,45,593,15,0,0,28,0,0,11,314,0,315,0,830,17,140,0,0,11,0,0,0,0,0,0,0,0,0,0,0,3135\n",
      "1951,0,147,381,111,1403,379,4,190,65,311,18,1,0,5,86,116,91,134,37,10,0,68,0,0,45,0,2,271,0,8,380,1,0,0,0,0,0,0,16,4,0,0,0,0,1,0,3764\n",
      "1059,0,379,348,0,420,7,90,1789,1,119,5,65,227,355,0,50,6,749,89,137,0,55,0,0,1,0,1,28,8,0,3,231,0,0,0,159,0,2,0,0,0,0,0,0,0,0,3617\n",
      "1257,237,27,224,5,406,394,0,203,0,0,0,244,9,1172,36,144,0,31,8,68,244,13,0,0,0,1,0,476,0,1,464,226,1,0,2,53,0,0,0,0,0,0,0,0,0,0,4054\n",
      "481,264,251,55,0,1560,143,356,5,5,6,0,16,12,840,0,315,4,15,193,0,0,131,1,0,45,28,0,124,0,15,401,0,0,0,0,417,0,0,0,19,0,38,0,0,173,0,4087\n",
      "788,2,190,100,0,1660,461,15,187,0,320,2,71,0,11,0,100,5,2,118,0,0,3,1,0,1,0,0,470,0,0,54,0,0,0,0,120,0,110,0,0,0,5,0,0,0,0,5204\n",
      "1266,0,367,63,1,1334,12,72,10,15,323,183,11,61,223,15,208,899,89,1,28,0,52,0,3,57,0,0,717,0,0,8,565,0,0,94,7,0,0,1,0,0,0,0,0,0,0,3315\n",
      "564,799,30,801,0,1179,603,423,26,0,3,27,0,100,8,0,77,0,158,231,3,23,920,53,0,34,0,0,176,0,0,977,9,4,0,306,0,0,0,0,0,0,0,0,0,0,0,2466\n",
      "1252,1,319,34,26,761,1,24,53,190,557,20,200,0,214,7,54,329,891,225,0,0,509,462,56,2,8,0,55,0,0,0,295,0,0,71,28,0,0,0,0,0,0,0,0,0,0,3356\n",
      "1010,0,153,366,197,1196,56,0,294,0,539,186,10,2,403,26,257,132,41,216,43,360,209,165,0,26,10,28,91,0,0,265,235,0,0,0,138,0,0,1,0,0,0,0,0,0,0,3345\n",
      "820,13,1225,34,0,1058,243,200,861,0,257,48,322,2,207,0,1,16,177,4,33,0,43,0,16,0,0,0,281,0,0,193,70,0,0,5,87,0,0,55,0,0,0,0,0,0,0,3729\n",
      "1372,3,355,1289,0,615,761,62,115,0,73,14,263,453,3,56,35,35,279,121,0,0,7,183,8,25,0,5,291,0,157,71,137,0,0,2,254,0,0,5,0,0,0,0,0,0,0,2951\n",
      "1029,0,246,175,190,508,295,34,248,1,131,15,185,1769,0,174,375,40,31,1,2,0,557,0,0,0,1,8,224,0,105,130,314,0,0,0,14,0,0,0,0,0,0,0,0,0,0,3198\n",
      "686,0,120,957,409,1118,35,0,277,1,296,29,0,10,1174,106,65,96,66,17,0,0,11,0,0,7,41,229,459,0,0,424,81,25,0,0,528,0,0,0,4,0,0,0,0,0,0,2729\n",
      "1519,6,519,747,1,469,36,0,431,3,59,163,0,0,31,265,260,7,51,524,0,4,362,0,0,0,0,4,289,0,0,14,10,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4226\n",
      "1343,0,573,205,46,1060,789,22,564,0,119,154,121,92,53,4,364,12,46,36,4,147,0,1,0,1,0,0,84,0,4,49,639,0,0,70,222,0,0,1,35,0,7,0,0,0,0,3133\n",
      "1458,0,234,128,0,494,182,1,1556,36,16,5,197,6,443,131,37,84,14,43,6,2,130,0,0,0,0,0,2,501,0,0,44,0,0,0,508,0,0,1,0,1,0,0,22,0,0,3718\n",
      "937,8,367,123,7,1930,298,1,652,173,0,10,157,14,95,0,98,157,261,38,4,228,4,49,175,18,1,83,59,0,0,3,168,0,4,0,106,0,0,76,6,0,0,1,0,0,0,3689\n",
      "664,0,155,480,179,1162,740,8,632,0,271,21,8,0,0,3,12,5,150,92,210,0,259,2,0,88,1,0,44,0,445,131,22,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4216\n",
      "289,78,763,184,264,1229,62,13,259,3,14,14,248,49,1,0,117,48,245,8,0,360,127,203,2,0,1,2,123,40,24,81,25,0,0,0,10,0,0,0,1333,0,0,0,0,0,0,3781\n",
      "861,0,66,78,0,2330,46,160,126,176,132,13,39,31,12,7,0,99,7,0,60,45,110,3,0,13,0,2,3,0,152,515,1391,0,0,267,60,0,0,8,0,0,0,0,0,0,0,3188\n",
      "410,6,477,38,0,2080,101,0,1401,27,359,23,0,26,87,4,25,9,0,0,1,0,257,0,0,0,0,0,306,0,0,251,0,0,0,0,402,0,0,0,0,0,0,0,0,0,0,3710\n",
      "1862,0,323,1605,24,1327,276,69,174,0,25,134,0,12,98,0,6,0,31,114,0,105,2,0,0,0,0,0,43,0,111,532,236,0,0,0,48,0,0,0,0,0,12,0,0,0,1,2830\n",
      "1017,0,228,724,1,1419,639,112,295,0,698,8,130,0,0,0,233,5,875,134,0,12,122,0,0,0,3,0,8,0,0,77,14,0,0,1,0,0,0,32,0,0,0,0,0,0,0,3213\n",
      "483,0,125,590,0,2091,248,279,703,1,95,6,0,16,462,1,74,5,279,112,0,243,231,7,0,3,1,74,165,0,370,16,254,0,0,0,2,14,0,0,0,1,0,1,0,0,0,3048\n",
      "743,12,88,1038,94,2390,73,5,265,73,24,274,1,0,55,303,121,11,80,9,6,0,511,145,0,0,0,14,3,0,34,6,732,0,0,0,10,0,9,0,0,0,0,0,0,0,0,2871\n",
      "865,16,130,241,0,1572,1489,156,174,14,169,12,68,88,8,0,2,0,491,307,44,55,497,7,0,5,0,0,5,0,37,2,1,0,0,34,513,0,0,0,0,0,0,0,0,0,0,2998\n",
      "1014,0,1,335,519,367,11,408,197,22,22,113,0,11,5,13,235,10,122,52,0,0,339,2,0,4,0,3,96,0,115,293,0,0,0,27,1,0,0,0,0,0,4,2,0,0,0,5657\n",
      "194,0,40,220,0,2262,43,226,2,0,10,26,54,213,7,262,0,0,393,44,0,49,142,0,0,0,0,45,43,0,0,362,367,0,0,11,118,0,0,0,0,0,0,0,0,0,0,4867\n",
      "1890,8,131,371,12,802,134,34,750,120,703,13,0,28,82,5,0,45,5,204,0,89,120,0,0,0,11,0,294,0,714,3,0,0,0,0,4,0,1,0,3,0,0,0,0,0,2,3422\n",
      "552,0,89,442,0,1793,653,12,185,18,60,88,1,23,167,0,45,42,11,264,0,11,150,32,0,0,1,13,43,0,0,1,231,0,0,3,16,0,0,3,0,0,0,0,0,0,0,5051\n",
      "447,0,728,119,0,2153,38,48,763,1,6,1,0,155,5,19,33,390,349,44,1,0,30,0,21,0,0,50,313,746,1,113,2,0,0,0,0,0,10,0,1,0,0,0,0,0,0,3413\n",
      "550,0,9,314,0,1472,2,0,463,79,0,67,4,0,10,781,56,2,369,21,0,17,182,0,0,13,0,0,154,0,13,67,612,0,0,1,110,0,0,0,0,0,0,0,10,0,0,4622\n",
      "1007,6,654,606,11,1194,3,499,49,8,14,1,53,88,15,136,16,0,39,0,1,0,10,0,0,24,253,1,254,0,0,831,25,0,0,0,3,0,10,0,0,0,545,0,0,0,0,3644\n",
      "2113,26,113,472,98,1645,204,41,30,15,78,0,843,0,0,29,0,0,140,5,6,6,929,1927,0,0,8,0,8,0,0,11,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1253\n",
      "920,0,200,1708,75,864,41,13,91,0,223,265,39,6,1,6,193,578,340,2,13,7,46,11,0,0,0,3,168,0,0,5,0,0,0,51,227,0,1,5,0,0,0,0,0,0,0,3898\n",
      "1443,0,55,142,0,1063,0,15,745,0,45,16,520,73,220,47,2,0,0,9,37,13,86,0,73,0,8,15,25,0,0,95,0,0,0,0,535,0,0,31,0,0,0,0,0,0,0,4687\n",
      "1886,71,415,110,18,1597,52,5,149,0,17,15,187,90,38,249,22,11,142,813,0,185,203,0,0,431,0,0,875,0,8,36,0,0,0,0,183,0,0,197,0,0,0,0,0,0,0,1995\n",
      "589,176,302,120,0,2187,25,212,295,380,75,0,358,68,64,1,0,3,110,605,0,0,0,0,0,0,0,112,33,0,0,3,75,0,0,83,24,0,0,0,0,0,58,0,0,0,0,4042\n",
      "1467,0,130,140,31,1986,0,141,237,0,245,12,0,86,272,0,104,2,401,17,289,121,3,124,0,19,0,0,637,0,0,22,0,0,0,0,0,0,1,4,0,0,13,0,0,0,0,3496\n",
      "684,3,179,493,0,1245,0,3,710,5,427,2,85,4,0,109,12,0,374,313,5,17,96,0,6,12,230,0,1099,0,65,296,27,0,0,0,441,0,0,0,0,0,0,0,0,0,0,3058\n",
      "224,27,0,940,0,858,55,0,644,161,298,0,1,0,227,1763,32,0,27,3,2,4,165,0,0,0,0,151,760,0,0,13,165,0,0,0,154,0,0,3,6,0,0,0,0,0,0,3317\n",
      "1586,0,0,70,85,1868,191,20,126,192,152,0,1,0,22,11,11,166,423,23,0,0,206,5,0,2,7,0,72,0,123,12,103,0,0,0,16,0,0,2,0,0,0,0,0,0,0,4505\n",
      "1639,0,7,381,5,1999,213,0,259,4,0,15,5,64,15,43,24,22,175,8,0,0,45,3,0,25,4,6,244,0,0,258,270,0,0,1,154,0,0,0,3,0,0,0,0,0,0,4109\n",
      "1121,0,53,111,0,1676,105,0,356,4,55,0,526,53,1322,14,25,139,551,7,250,0,78,3,0,0,61,8,6,0,0,22,147,0,0,0,36,0,0,0,0,0,0,0,0,0,0,3271\n",
      "1002,18,27,261,0,1353,51,253,112,138,368,29,6,11,83,414,5,4,4,138,97,0,190,16,0,31,0,15,17,0,1,192,17,199,0,0,1,0,1,0,0,0,0,0,0,0,0,4946\n",
      "781,0,25,513,0,500,200,29,152,237,525,889,15,0,2,0,93,54,3,639,0,191,199,0,0,47,0,0,10,0,0,2,109,0,0,6,0,0,0,0,0,0,0,0,0,0,0,4779\n",
      "569,0,1222,186,0,1389,43,53,31,0,40,0,1,0,118,0,0,0,14,287,1,0,9,0,0,6,0,0,102,298,0,367,21,0,0,0,245,0,0,12,0,3,0,0,0,0,0,4983\n",
      "1563,1,198,719,0,1408,14,9,5,9,112,686,0,0,602,238,26,0,554,15,3,0,14,7,0,15,0,1,453,124,0,345,0,0,0,34,24,0,0,3,0,0,0,0,0,0,0,2818\n",
      "961,0,162,575,0,1630,2,1175,131,13,34,0,170,184,770,0,14,30,1,59,0,0,120,0,3,142,166,6,28,0,4,0,428,0,0,0,125,0,0,0,0,0,0,0,0,0,0,3067\n",
      "143,0,42,1745,3,1036,63,0,118,108,13,39,49,54,53,21,99,0,32,0,46,0,537,351,22,0,4,0,14,0,4,900,1,0,0,0,3,1,0,0,0,0,0,0,0,0,0,4499\n",
      "2159,3,38,400,0,966,223,22,355,0,2,0,16,211,0,1,88,30,102,330,8,109,645,100,59,0,0,0,115,5,3,12,65,0,0,751,1,0,0,0,1,0,0,0,0,0,0,3180\n",
      "1260,0,7,61,0,1219,436,46,205,23,78,29,265,153,0,195,56,118,204,0,1,3,151,0,1,0,0,0,696,0,1,206,5,0,0,3,331,0,0,0,18,0,0,0,0,0,0,4229\n",
      "1028,0,0,421,5,393,213,0,794,87,106,3,23,22,0,1,427,112,13,361,10,0,204,1,0,1097,0,42,47,0,0,112,0,0,0,0,11,0,0,22,248,93,0,0,0,0,0,4104\n",
      "3366,7,333,266,0,525,259,89,1,0,66,209,319,124,254,0,91,0,134,8,158,6,439,235,0,0,0,0,95,0,0,15,183,0,0,277,84,0,0,6,0,0,0,0,0,0,0,2451\n",
      "2147,0,182,321,296,1817,15,3,188,0,1,355,115,0,287,2,0,187,10,21,35,0,106,0,147,232,0,3,125,0,25,31,0,0,0,0,128,0,132,4,57,0,0,0,0,0,0,3028\n",
      "1064,1,33,232,0,1082,825,63,762,287,145,195,281,10,5,1,54,0,170,192,77,1,6,0,3,0,280,0,57,0,26,251,250,0,0,0,0,0,0,347,0,0,0,0,0,0,0,3300\n",
      "657,536,257,809,0,662,170,376,475,0,7,1,496,474,0,4,8,3,94,121,1,0,96,224,0,19,15,126,15,0,0,227,0,0,0,0,199,0,0,0,1,0,0,0,0,0,0,3927\n",
      "1012,0,34,204,0,1049,370,42,596,7,2,0,257,630,1,8,0,216,112,2,0,9,995,0,0,127,31,83,0,0,0,124,3,0,0,0,488,0,0,0,0,0,0,0,0,0,0,3598\n",
      "1251,135,683,81,0,1248,67,0,83,56,78,0,192,13,65,451,0,86,122,203,0,515,69,0,2,54,3,50,564,0,265,13,1,0,0,0,498,0,0,0,0,0,0,0,0,0,0,3152\n",
      "1464,0,620,187,0,809,88,3,79,18,692,2,2,50,47,117,259,4,39,95,1,3,0,1,0,0,7,274,4,0,168,16,45,0,0,0,917,0,0,188,0,0,0,46,0,0,0,3755\n",
      "586,0,148,498,3,929,113,35,183,98,740,77,0,0,8,0,0,1711,276,2,0,91,9,0,16,0,0,0,895,0,0,2,389,0,0,0,35,0,0,0,99,0,214,0,0,0,0,2843\n",
      "698,15,841,1055,16,1355,2,386,6,1,0,107,55,26,32,80,4,113,60,17,166,10,277,9,0,2,105,0,4,0,0,41,29,0,0,0,138,0,0,2,3,0,0,0,0,0,0,4345\n",
      "1482,19,194,343,34,1140,98,0,566,0,126,0,67,132,2,1,182,31,95,299,0,24,606,0,1,0,0,112,491,0,0,24,1,0,0,0,1521,0,0,0,0,0,0,0,0,0,0,2409\n",
      "786,0,261,206,0,3129,43,30,43,0,22,276,83,6,38,0,32,0,43,95,0,116,15,0,0,0,0,15,51,0,0,142,53,0,0,0,25,0,0,65,146,0,1,0,0,0,0,4278\n",
      "572,0,837,197,0,611,236,42,269,571,147,197,67,258,14,0,4,2,251,931,0,0,3,10,0,0,145,1,141,0,47,0,157,0,0,0,69,0,0,0,0,0,0,161,0,0,0,4060\n",
      "955,0,321,654,1,1307,205,33,166,0,79,0,400,69,174,84,17,105,442,217,10,0,24,0,0,167,0,0,58,0,0,93,32,0,0,0,3,0,22,36,0,0,0,0,0,0,0,4326\n",
      "1452,0,309,489,0,486,113,254,211,207,22,7,691,169,161,920,431,3,173,0,0,42,285,10,0,3,0,0,162,0,61,301,377,0,0,96,0,0,0,13,0,0,0,0,0,0,3,2549\n",
      "491,0,255,555,1,1029,0,0,651,680,330,16,0,91,39,0,357,233,185,922,3,40,23,0,2,0,0,0,1181,0,54,74,37,0,0,0,4,0,0,7,0,0,151,0,0,0,0,2589\n",
      "1153,5,293,780,2,1155,18,0,83,11,209,253,3,2,380,234,32,2,479,732,1,6,787,10,0,176,0,69,140,211,5,4,0,0,0,0,33,0,0,0,0,0,1,0,0,0,0,2731\n",
      "1517,0,869,48,0,558,14,0,127,0,1,2,1,145,710,1,6,305,12,624,0,0,38,2,7,0,13,90,55,0,0,46,2,0,0,0,0,0,0,2,0,0,0,0,0,30,0,4775\n",
      "902,17,204,159,0,2635,464,1,329,0,103,19,89,0,0,0,86,362,33,88,131,0,209,72,0,5,3,483,30,0,0,2,0,0,0,9,25,0,0,0,0,0,0,0,0,0,0,3540\n",
      "2241,0,11,950,0,958,1,3,23,0,259,466,377,0,6,0,82,677,5,260,0,0,361,128,15,73,209,1,164,0,2,64,0,1,0,0,277,0,0,0,0,0,0,0,0,0,67,2319\n",
      "459,0,536,601,16,1470,420,291,518,3,340,0,85,0,0,459,63,98,60,7,0,0,98,8,0,0,460,46,157,0,1,1206,1,0,0,351,0,0,0,0,0,0,0,0,0,0,0,2246\n",
      "527,0,41,635,0,1624,529,26,161,40,51,50,147,76,191,6,17,6,24,665,0,11,828,8,0,0,0,0,182,0,0,103,4,0,0,254,757,3,0,758,0,0,0,7,0,0,0,2269\n",
      "812,96,36,589,0,1609,169,0,190,0,102,0,4,0,16,468,187,1,10,164,0,33,462,25,1,22,86,0,62,0,163,985,1,0,0,0,38,0,0,0,164,0,0,0,0,0,0,3505\n",
      "1436,0,78,230,0,2178,203,105,46,45,53,12,133,63,45,44,15,31,28,89,0,275,60,3,0,49,50,0,121,0,0,221,32,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4355\n",
      "1586,678,165,142,161,1427,5,0,39,0,205,38,0,183,108,1,36,105,85,75,89,158,110,0,0,164,0,0,21,0,91,333,2,0,0,0,2,4,0,0,0,0,0,0,0,0,0,3987\n",
      "874,0,176,381,83,1015,1520,10,62,0,678,27,37,45,28,72,81,74,311,93,0,0,293,6,0,0,0,0,14,0,111,1234,19,0,0,40,1,0,0,0,0,0,0,0,0,332,0,2383\n",
      "1495,0,83,1219,20,1102,1,0,787,113,35,5,4,20,289,0,22,26,315,639,0,92,121,73,4,0,7,22,302,0,29,6,99,0,0,0,29,0,0,0,0,0,8,0,0,0,0,3033\n",
      "1066,5,324,71,129,600,546,1,356,321,204,68,3,656,37,313,129,273,127,21,1,4,458,0,0,0,1,0,248,0,0,165,23,0,0,18,83,0,9,0,0,0,102,0,0,0,0,3638\n",
      "466,2,378,343,0,1591,346,825,250,0,2,196,478,51,0,27,0,157,140,4,0,0,5,8,20,10,0,0,113,0,0,0,69,0,0,824,1,0,0,51,408,8,0,0,0,0,0,3227\n",
      "537,0,122,370,2,484,340,0,488,6,30,0,39,387,46,187,116,0,207,299,25,97,594,0,0,0,14,2,234,118,5,27,227,0,0,0,94,0,0,279,0,0,0,0,0,0,0,4624\n",
      "2098,12,332,1625,59,2034,831,17,140,0,2,13,15,10,87,0,4,0,0,58,226,6,5,11,0,34,39,0,279,0,125,72,1,0,0,0,1,0,0,0,62,0,0,0,0,0,0,1802\n",
      "1297,0,302,252,0,897,95,0,66,0,119,393,143,328,23,236,172,356,321,83,0,0,1,0,0,10,0,0,792,0,0,91,22,15,0,0,0,0,0,96,0,0,0,0,0,0,0,3890\n",
      "1511,250,498,371,0,1544,184,232,338,0,135,0,60,149,453,0,21,40,87,275,0,0,92,0,0,0,0,0,11,0,0,91,0,9,0,44,2,0,0,13,109,0,27,0,0,0,0,3454\n",
      "667,0,937,126,15,1508,79,6,462,222,26,112,137,0,35,0,0,200,582,2,0,4,545,136,0,0,0,24,24,0,2,209,32,0,0,0,463,0,0,0,0,0,0,8,0,0,0,3437\n",
      "185,4,479,507,0,681,114,134,1176,0,0,0,245,0,134,6,247,0,188,186,0,0,1061,54,0,180,3,0,32,0,75,8,37,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4264\n",
      "1386,0,313,199,2,1334,84,63,84,47,188,222,152,6,113,0,3,4,137,8,8,0,282,0,0,115,0,142,411,0,4,1082,47,0,0,0,25,91,0,0,0,0,0,0,0,0,0,3448\n",
      "2408,30,81,781,0,967,399,7,372,0,34,0,0,8,179,23,57,371,155,223,0,3,28,0,58,0,0,0,947,22,0,15,0,0,0,13,13,0,0,247,0,0,2,0,0,8,0,2549\n",
      "2304,5,70,46,0,1625,19,1,218,21,403,19,81,106,32,0,0,0,0,0,0,124,475,0,26,162,1,356,508,0,0,168,8,0,0,0,6,0,0,0,0,0,0,0,0,0,0,3216\n",
      "1452,0,172,169,10,2880,306,0,612,0,276,7,132,0,579,78,136,77,157,22,0,11,71,14,9,318,1,27,201,0,0,82,0,0,0,0,286,0,0,2,1,0,0,0,0,0,0,1912\n",
      "523,0,295,1654,0,1613,75,339,399,0,48,0,113,19,202,0,168,364,31,210,0,2,332,0,0,1,49,0,518,0,1,30,22,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2992\n",
      "2657,0,246,68,0,2889,46,68,3,0,21,5,11,1,0,1,87,0,3,30,0,18,260,2,3,8,0,0,168,0,0,578,30,0,0,0,141,0,0,0,0,0,0,0,0,0,0,2656\n",
      "1948,0,45,175,0,1215,2,30,124,211,33,199,75,28,105,1,36,171,72,259,5,12,163,0,0,13,27,0,29,156,0,1271,160,0,0,0,159,0,0,0,8,0,0,0,0,0,0,3268\n",
      "1567,0,351,589,0,1250,57,3,273,54,10,21,0,33,37,0,409,1,91,90,5,516,124,1,0,11,30,0,24,0,88,78,0,75,0,0,0,0,0,0,0,0,0,0,0,8,0,4204\n",
      "1065,131,346,181,0,829,4,0,32,0,106,194,319,15,69,50,274,291,229,7,1,0,313,3,0,2,0,0,121,0,0,1,1,0,0,862,36,0,0,1,0,0,0,0,0,0,0,4517\n",
      "2441,0,98,672,0,816,8,77,66,8,375,344,18,276,40,7,64,10,4,11,23,0,165,5,85,11,0,53,375,0,326,53,62,0,0,0,337,0,0,0,0,0,21,0,0,0,0,3149\n",
      "1931,0,45,207,15,1140,53,0,35,552,188,175,41,89,37,18,12,46,113,35,137,0,136,9,0,42,3,0,313,0,29,83,552,0,0,0,0,0,0,0,4,0,0,0,0,0,0,3960\n",
      "139,0,323,115,0,1220,755,1,1148,0,27,39,90,1,90,18,0,1,183,150,7,3,854,27,0,22,0,538,13,0,0,26,2,0,0,0,216,0,0,0,0,0,0,0,0,0,0,3992\n",
      "1242,0,537,620,0,1299,52,334,64,124,1,15,58,97,758,4,286,0,29,153,0,277,1,0,4,10,72,226,189,0,0,391,45,0,4,0,26,0,0,0,0,0,244,0,0,0,0,2838\n",
      "1548,0,87,95,0,1142,787,85,288,36,14,56,370,0,41,2,0,123,68,21,0,0,198,0,0,477,0,14,41,0,0,1576,9,0,0,0,48,0,0,1,0,0,0,0,0,0,0,2873\n",
      "38,0,1421,41,321,1357,31,20,1480,0,77,4,559,1,201,0,348,0,89,0,0,1,3,80,14,131,10,50,76,0,0,102,229,0,0,262,35,0,15,1,0,1,0,0,0,0,0,3002\n",
      "1245,58,147,96,0,1985,56,74,35,0,158,33,0,86,850,0,6,1,1244,6,57,0,21,0,118,0,4,12,205,0,90,48,0,0,0,0,21,0,0,0,0,0,6,0,0,0,0,3338\n",
      "924,37,175,80,4,742,103,220,1450,0,122,19,175,20,279,0,0,63,119,0,1,1,2,121,0,0,0,0,233,0,0,6,3,0,0,541,29,0,11,0,9,441,0,0,0,0,0,4070\n",
      "1851,0,46,169,0,1270,10,296,476,1,41,2,25,3,613,271,0,12,3,9,9,0,162,0,0,6,0,59,340,0,4,64,249,0,0,14,4,0,0,6,0,35,0,17,0,0,0,3933\n",
      "460,23,116,494,0,1172,43,186,618,466,3,0,126,0,169,12,147,9,120,499,0,0,155,12,0,6,75,503,26,0,5,539,9,0,0,0,273,0,0,0,0,0,0,0,0,0,0,3734\n",
      "704,0,89,255,4,1494,0,6,10,19,2,234,87,28,2,117,5,86,170,222,675,0,549,0,35,161,9,378,7,0,0,34,41,0,0,49,6,0,0,0,0,0,0,2,0,0,0,4520\n",
      "1258,26,21,262,186,958,20,12,231,0,0,0,38,49,726,538,7,28,15,170,195,0,309,0,0,0,0,0,539,0,0,226,0,0,0,0,0,0,0,0,8,0,0,0,0,0,22,4156\n",
      "1060,114,267,609,7,443,281,7,51,5,31,0,128,207,25,30,35,2,22,19,1,6,297,0,196,1,1,10,635,1,0,131,64,0,0,34,472,0,0,0,0,0,0,0,0,0,0,4808\n",
      "487,4,5,105,18,2828,0,0,754,0,46,15,13,6,10,84,1090,163,550,146,0,65,353,1,0,3,0,0,14,0,0,120,6,0,0,0,7,0,0,1,0,0,0,0,0,0,0,3106\n",
      "647,0,12,241,0,1464,0,5,3,179,0,4,0,0,219,80,0,0,173,2,0,28,250,0,0,0,98,67,353,0,0,167,203,0,0,26,75,0,0,183,0,0,28,0,0,0,0,5493\n",
      "978,1,507,174,0,1414,34,5,193,1004,23,1,99,0,334,31,1,43,1,0,0,230,450,0,73,1,0,0,139,0,0,48,1,0,0,0,65,0,0,0,0,0,0,0,0,0,0,4150\n",
      "762,80,185,661,0,1172,422,98,81,1,3,32,0,1,7,28,26,298,1,1,0,1,212,1731,0,94,0,1,306,0,0,222,8,0,0,0,4,0,12,0,0,426,0,0,0,0,0,3124\n",
      "578,0,70,154,0,1604,123,13,598,0,644,201,204,0,105,0,0,0,0,228,155,0,1675,118,56,0,80,159,300,0,1,150,0,0,0,0,0,0,0,0,39,0,0,0,0,0,0,2745\n",
      "509,0,124,519,0,558,119,84,318,0,151,106,120,124,1,0,3,510,1153,869,14,0,140,1,1,0,0,0,29,3,37,217,11,0,0,0,149,0,0,588,0,0,0,0,0,0,0,3542\n",
      "795,329,122,469,12,1015,282,0,577,0,35,513,52,0,862,85,44,68,3,306,0,46,30,0,194,1,307,7,116,12,0,396,392,0,0,0,46,0,0,1,0,0,0,0,0,0,0,2883\n",
      "1674,0,409,1255,3,788,558,320,533,0,0,5,33,242,39,0,334,0,0,4,0,114,622,0,0,0,0,78,8,0,17,36,317,0,0,27,39,0,1,0,0,0,7,0,0,0,0,2537\n",
      "344,0,29,543,0,574,0,0,544,0,10,1,13,71,22,49,0,7,2,274,0,0,357,27,0,199,0,1,804,0,0,391,4,17,0,0,2425,0,0,0,473,0,144,0,0,0,0,2675\n",
      "2628,0,162,292,22,1374,65,336,158,1,61,1,7,0,80,12,119,93,52,9,31,75,52,48,0,5,3,0,590,0,0,60,28,0,0,0,196,0,0,0,0,0,0,0,0,0,0,3440\n",
      "1236,0,637,524,0,300,3,131,193,0,126,3,41,2,90,14,492,373,386,871,0,678,101,0,0,286,0,303,174,0,12,214,164,0,0,3,10,0,0,27,0,1,0,0,0,0,0,2605\n",
      "712,0,282,194,6,1394,118,205,128,0,94,4,11,330,53,22,88,0,98,12,0,26,865,446,1,103,4,0,351,0,9,0,7,0,0,0,0,0,0,1,0,0,0,0,0,0,0,4436\n",
      "696,0,176,389,209,893,334,352,103,0,55,0,39,12,306,7,52,124,114,27,74,1,2,0,0,0,0,0,32,0,0,377,14,0,0,4,43,0,14,0,0,0,2,0,0,0,0,5549\n",
      "1715,4,598,636,355,1199,0,122,676,0,124,97,1,252,8,30,0,0,531,7,0,0,305,36,0,0,130,0,0,0,0,162,88,0,0,0,80,0,0,0,0,107,19,0,0,0,0,2718\n",
      "412,176,211,200,0,1735,90,37,884,8,34,156,20,0,1434,0,0,121,93,11,6,0,7,1,15,1,73,0,400,0,5,114,0,0,0,6,4,0,0,0,0,0,0,0,0,0,0,3746\n",
      "697,25,32,335,0,2095,14,20,312,192,72,300,128,1,5,111,0,487,46,11,1,0,277,0,0,0,13,14,3,0,1,132,709,0,0,41,55,0,32,0,0,0,0,0,0,0,0,3839\n",
      "584,14,709,456,293,2864,81,0,219,0,25,133,5,0,121,0,59,0,68,153,1,46,226,1,0,17,0,9,9,0,3,0,8,0,0,0,1,0,0,0,0,0,0,0,0,0,0,3895\n",
      "585,0,703,236,16,1151,1,0,277,0,2,546,631,0,119,34,1448,21,115,0,0,0,44,3,0,0,0,30,15,0,0,37,86,0,0,7,136,0,0,0,0,0,0,0,0,0,0,3757\n",
      "751,0,122,276,0,1347,7,1,304,0,1,16,13,0,0,24,232,0,528,167,0,3,32,484,0,2,243,0,136,60,226,0,0,0,0,0,10,0,0,0,0,0,0,0,0,0,0,5015\n",
      "650,0,139,204,0,1737,43,0,168,0,460,0,5,1,244,136,505,2,152,8,0,54,78,0,0,4,32,0,28,0,17,27,82,0,0,0,72,0,0,30,0,0,0,0,0,0,0,5122\n",
      "1388,0,282,1640,0,627,233,110,39,0,27,0,0,9,15,12,0,13,20,43,2,231,304,212,0,0,0,1,0,0,3,0,0,0,0,0,187,0,0,2,0,0,0,0,0,0,0,4600\n",
      "474,31,72,283,775,1763,72,17,111,135,300,1,336,1,258,6,0,0,157,84,0,0,23,0,63,0,0,15,10,0,260,71,204,0,0,19,193,0,0,15,2,0,0,0,0,0,0,4249\n",
      "1416,0,457,521,6,881,0,514,180,0,104,21,81,391,52,125,95,0,2,0,20,0,601,0,0,6,0,0,1084,0,1,15,0,0,0,27,15,0,0,84,0,0,0,0,8,14,2,3277\n",
      "846,1,210,756,0,1352,0,0,653,3,67,7,83,0,9,65,5,0,68,5,0,0,151,0,0,384,1,0,519,0,0,660,157,0,0,75,4,0,0,60,0,0,0,0,0,0,0,3859\n",
      "1939,0,242,255,133,1423,18,1,64,22,509,7,26,82,4,41,282,3,2,196,52,23,22,136,3,62,58,0,607,0,50,22,7,0,0,0,7,0,0,1,6,0,0,0,0,0,0,3695\n",
      "1139,0,135,460,0,661,2,3,1140,0,23,11,74,15,2,65,110,0,800,0,29,2,17,156,0,0,0,0,1,0,0,108,65,0,0,0,4,0,0,0,0,6,25,0,0,0,0,4947\n",
      "1040,143,491,582,0,1348,92,28,8,0,1491,5,10,70,274,0,6,0,317,17,245,108,215,0,0,0,317,47,433,0,4,3,1,0,0,0,0,0,0,56,0,0,0,0,0,0,0,2649\n",
      "1328,0,67,357,24,895,333,0,25,8,28,92,1,0,566,0,1370,0,544,999,7,28,40,0,57,60,299,0,81,0,5,1,3,20,0,0,314,0,2,19,6,0,0,0,0,0,0,2421\n",
      "596,0,121,284,4,1226,48,42,754,0,9,825,0,32,100,124,130,0,8,18,5,84,181,3,0,547,3,0,517,0,169,8,0,0,0,0,7,1,0,0,0,7,0,0,0,0,0,4147\n",
      "1385,14,137,130,0,916,80,35,75,2,420,4,0,17,261,0,49,288,0,161,313,0,7,8,0,188,0,0,454,0,1517,5,153,0,0,0,283,0,0,8,0,0,34,0,0,0,0,3056\n",
      "1981,0,525,546,2,1176,79,26,71,48,43,0,0,45,49,3,0,21,16,117,0,0,315,0,5,0,0,3,55,0,4,372,41,1,0,0,1,0,0,0,0,0,0,0,0,0,0,4455\n",
      "2193,9,490,119,0,1573,95,2,135,0,113,26,331,0,136,10,67,22,88,212,0,0,128,0,0,2,89,34,75,0,0,498,0,0,0,0,93,0,0,0,6,0,0,0,0,0,0,3454\n",
      "830,0,178,278,0,1276,128,3,163,2,390,1120,82,15,1,68,487,133,11,21,4,5,119,3,1,1,1,0,29,0,0,168,284,0,0,0,1,0,0,97,0,0,0,0,0,5,0,4096\n",
      "780,87,26,877,238,1790,29,39,172,0,15,0,53,70,62,2,62,89,92,43,428,21,4,80,0,11,0,0,271,0,0,2386,211,0,0,0,15,0,0,2,0,0,0,0,0,0,0,2045\n",
      "417,2,1241,84,149,1151,345,61,158,0,90,129,63,145,45,6,28,295,9,69,0,16,1,0,0,0,5,0,597,0,5,356,35,0,0,0,13,21,9,0,69,0,109,1,0,0,273,4003\n",
      "264,0,241,284,322,1291,167,1,130,535,36,40,44,51,144,0,0,2,111,86,1,165,49,78,0,0,197,0,885,0,50,3,36,0,0,0,4,0,0,0,0,0,0,0,0,0,0,4783\n",
      "1109,2,138,1077,760,2069,640,168,5,0,16,0,0,0,1,123,451,0,3,1,1,2,144,0,79,1,0,0,28,0,0,14,50,0,0,0,5,0,0,14,0,0,0,0,0,0,0,3099\n",
      "1904,1,168,117,0,974,74,611,87,221,118,136,432,994,171,0,8,19,21,0,0,1,33,15,0,0,0,6,55,0,0,305,258,81,0,0,8,0,3,0,0,0,0,0,0,0,0,3179\n",
      "1068,4,213,243,1,1129,0,4,910,193,0,14,155,11,964,0,1,0,25,14,170,2,394,41,0,231,0,218,23,0,0,443,270,0,0,3,859,0,0,0,0,0,35,0,0,3,0,2359\n",
      "1927,0,8,538,0,1094,165,1218,58,107,7,414,1,108,3,414,7,5,24,45,2,115,0,152,0,1,0,3,12,31,4,303,358,0,0,0,58,0,0,35,0,0,0,0,23,0,0,2760\n",
      "1312,5,243,1086,0,922,5,162,38,30,2,375,17,20,173,7,89,183,274,6,0,31,9,146,0,457,0,0,76,0,0,21,153,68,0,0,1463,0,0,2,1,0,0,0,0,0,0,2624\n",
      "2308,0,141,120,9,1281,37,113,210,0,214,39,0,269,441,206,4,35,12,77,0,143,233,0,38,0,1,8,923,0,0,106,10,0,0,5,3,0,0,0,0,0,0,0,0,0,0,3014\n",
      "870,144,314,1615,0,751,89,0,132,113,9,0,0,0,222,108,0,29,70,4,0,0,24,0,0,75,120,5,620,0,7,374,14,0,0,0,104,0,0,73,0,0,0,0,0,0,0,4114\n",
      "144,117,0,91,0,1494,367,3,84,98,688,547,2,5,0,19,0,330,9,1,0,186,0,0,0,0,139,0,375,0,0,15,61,0,0,21,56,0,5,0,0,0,0,0,0,67,0,5076\n",
      "274,65,97,444,24,1993,45,152,146,947,1119,68,1,118,0,4,86,1,2,245,14,0,101,52,0,0,1,0,77,0,41,407,29,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3447\n",
      "672,2,134,1181,3,1844,67,0,770,2,527,2,117,92,50,32,81,0,5,153,10,441,122,44,0,0,0,0,49,24,0,0,110,0,0,0,6,0,2,0,0,1,0,0,0,0,0,3457\n",
      "503,109,791,415,0,836,99,463,14,0,11,15,148,43,2,6,144,0,163,0,99,1,58,104,0,0,110,0,539,0,0,15,362,0,28,1,56,0,501,0,0,0,0,0,0,0,0,4364\n",
      "619,0,612,459,72,2116,13,10,285,0,212,0,2,1,0,10,27,44,870,231,2,0,42,0,0,0,0,0,155,0,156,371,15,0,0,0,20,0,0,6,0,0,0,1,569,0,0,3080\n",
      "214,165,3,43,0,559,1180,770,316,0,46,5,1,438,788,0,76,32,189,133,0,52,1,260,0,287,363,0,75,0,6,197,553,0,0,1,1,0,0,1,0,0,0,0,0,0,0,3245\n",
      "516,0,157,359,6,1105,0,4,267,65,0,5,2,0,1135,7,8,124,118,154,0,41,16,0,0,21,0,0,100,0,0,12,32,0,0,0,710,0,0,2,0,0,2,0,0,0,0,5032\n",
      "1742,8,178,149,5,663,154,294,138,0,733,641,572,0,8,0,5,56,1,0,143,60,289,165,1,0,0,0,27,1,875,190,1,0,0,0,188,0,0,0,0,0,0,0,0,0,0,2713\n",
      "403,11,528,66,9,1232,2,18,40,0,486,272,19,725,33,163,212,51,237,819,197,0,23,0,0,1,34,0,1,0,0,7,784,0,0,85,14,0,0,361,0,244,0,0,0,0,0,2923\n",
      "1032,0,79,923,2,1913,41,1,27,0,76,7,0,0,96,0,2,2,270,180,1,0,108,102,0,28,5,0,4,0,21,192,3,0,0,0,134,364,0,5,0,15,0,0,0,0,0,4367\n",
      "686,0,4,514,142,809,71,317,1,822,3,40,6,3,121,0,106,993,7,24,0,126,141,23,144,2,0,38,194,0,10,227,234,0,0,60,71,0,0,295,0,0,0,0,0,0,0,3766\n",
      "790,0,311,722,5,1335,501,0,355,98,0,165,9,0,0,12,680,417,252,36,0,1,99,145,360,0,0,0,127,0,3,339,21,0,0,0,6,0,0,2,112,0,0,0,0,0,0,3097\n",
      "485,0,260,340,0,1539,278,59,1134,20,69,0,8,1,24,0,69,0,413,313,0,40,2,0,0,192,0,44,5,30,15,107,0,0,10,89,0,0,0,2,0,0,0,0,0,0,0,4452\n",
      "958,0,400,47,0,757,142,26,940,27,8,21,7,7,115,0,13,7,18,586,866,188,298,23,1,1,0,0,254,0,2,241,7,0,0,0,0,0,0,25,0,34,0,108,0,0,0,3873\n",
      "918,0,0,201,0,1001,96,438,398,0,53,0,44,1082,46,4,67,4,0,521,69,0,130,69,16,7,12,56,413,4,0,147,51,0,0,1,9,0,0,0,105,9,0,0,0,0,0,4029\n",
      "411,0,167,924,52,1156,262,6,36,0,0,341,0,0,5,0,3,0,22,2,13,1,9,0,0,0,0,3,54,0,97,7,0,0,0,15,0,124,0,1,0,0,0,0,0,0,0,6289\n",
      "339,0,15,411,0,2186,29,0,157,442,1430,0,1,1,79,362,20,5,232,22,0,102,269,761,0,7,0,7,635,0,1,0,43,0,0,0,146,0,0,23,5,0,0,0,0,0,0,2270\n",
      "833,0,5,668,102,1104,242,158,4,0,0,0,94,61,511,45,105,14,15,509,0,0,518,377,0,0,0,189,142,0,0,663,19,0,0,0,168,0,0,0,0,0,0,0,0,0,0,3454\n",
      "495,175,45,225,346,686,4,0,296,216,871,208,6,8,6,1,152,57,2,32,472,75,392,2,5,0,0,0,47,0,0,1,56,0,0,47,36,0,103,0,0,0,0,0,0,0,0,4933\n",
      "1712,401,927,98,602,1047,122,300,75,8,134,2,80,60,107,17,12,243,10,3,1,1,629,21,142,0,0,0,84,0,1,122,0,0,0,0,17,0,0,0,0,0,0,0,0,0,0,3022\n",
      "868,2,301,1600,0,1426,90,0,326,0,33,10,0,0,51,0,2,32,0,278,2,0,251,195,0,5,0,281,464,29,0,0,0,0,0,13,1,0,0,0,0,0,0,102,0,0,0,3638\n",
      "801,46,396,36,25,614,317,2,336,0,135,761,22,153,601,0,1,490,1,6,0,270,1,228,6,0,0,0,47,0,1,13,328,0,0,0,223,0,0,0,0,0,27,0,0,0,0,4113\n",
      "1346,0,2,1381,0,263,8,55,1540,245,8,0,0,0,397,77,95,28,1,72,0,0,194,1,0,11,3,0,160,0,4,3,31,0,17,0,1,1434,0,0,0,0,0,0,0,0,0,2623\n",
      "1284,23,18,841,0,400,3,114,156,0,8,28,128,13,982,14,1165,22,65,2,0,3,0,0,40,0,415,1,242,0,0,79,0,0,0,1098,222,0,71,0,0,0,0,0,0,11,0,2552\n",
      "607,0,576,56,84,2713,226,1,139,414,253,30,73,37,278,297,0,43,96,1,0,0,30,0,0,0,5,4,16,0,12,277,25,0,0,54,6,0,0,0,0,0,0,0,0,0,0,3647\n",
      "1352,294,105,144,5,1227,135,83,564,70,44,0,85,20,0,43,19,172,121,531,18,47,9,3,214,109,0,0,19,0,0,551,3,0,0,0,3,0,49,0,0,0,0,0,412,0,0,3549\n",
      "821,1,16,373,627,1000,0,107,17,0,32,223,20,256,234,3,0,0,201,483,0,0,98,4,0,0,0,0,454,0,79,204,71,0,0,0,71,0,6,0,0,0,0,0,0,0,0,4599\n",
      "1738,0,212,220,0,2270,46,1,115,7,0,132,11,0,2,0,0,6,58,624,65,0,85,1,0,0,41,14,236,0,0,14,0,0,0,4,3,0,0,0,47,779,7,0,0,0,0,3262\n",
      "1392,0,549,401,3,1178,0,29,543,21,14,19,407,43,0,960,519,605,82,9,0,0,173,0,0,0,0,0,9,11,0,63,1,0,0,0,7,0,0,211,10,0,0,0,0,0,0,2741\n",
      "1431,3,135,770,1,1706,83,78,49,0,15,2,132,358,755,38,0,162,241,113,0,10,11,117,0,38,8,149,117,0,0,852,4,60,0,0,0,0,6,18,68,5,0,0,0,0,0,2465\n",
      "737,0,630,579,59,882,176,67,257,8,108,170,53,101,91,0,10,0,3,153,49,568,609,42,0,0,0,0,17,45,1,1,2,7,0,0,157,0,0,1,0,0,0,0,0,0,0,4417\n",
      "856,0,61,112,0,926,31,13,244,719,41,318,13,226,30,1,0,76,2,45,1,0,119,21,0,43,3,55,33,0,2,193,88,0,0,0,284,0,226,104,5,0,0,0,0,0,0,5109\n",
      "1278,0,202,11,0,466,585,34,42,15,3,766,0,0,51,74,8,6,88,213,0,77,412,1,0,0,0,35,1026,254,0,340,0,0,0,22,43,0,0,0,0,2,0,0,0,0,0,3946\n",
      "1416,0,262,206,2,1344,79,0,267,0,0,9,265,0,381,4,1,0,92,66,18,454,37,4,0,36,0,31,64,0,45,47,0,0,0,1,47,278,1,0,0,0,0,0,0,0,0,4543\n",
      "474,257,445,354,1,782,362,883,517,192,0,179,16,11,284,0,502,0,14,112,0,3,171,0,0,262,0,0,152,0,0,27,10,0,0,0,0,0,0,18,0,0,0,0,0,0,0,3972\n",
      "1770,1,457,176,0,1569,22,43,6,76,91,37,41,0,405,3,1,4,121,1,0,0,837,1,1,0,206,2,147,0,0,10,1,0,0,1,232,0,0,0,1,0,0,0,0,6,0,3731\n",
      "985,0,117,12,0,2989,45,101,259,0,143,0,12,91,170,35,316,11,194,73,0,6,220,1,116,145,19,1,26,0,0,284,289,0,0,0,99,0,0,0,0,0,0,0,0,0,0,3241\n",
      "515,0,14,793,0,934,34,148,52,40,257,5,4,1,172,0,25,18,316,232,749,0,20,2,0,151,0,0,238,4,3,443,100,0,0,0,12,0,0,0,0,0,0,23,0,0,0,4695\n",
      "781,2,203,80,303,2262,13,289,211,2,11,0,11,1,12,257,0,0,119,23,7,4,1,1,0,0,0,0,174,0,6,129,8,0,0,63,727,0,0,0,1,0,0,0,0,0,0,4299\n",
      "1298,0,201,1111,0,1798,494,37,246,26,516,117,22,106,103,8,64,109,200,59,0,23,381,8,0,0,0,0,6,0,0,47,9,0,0,0,6,0,0,0,0,0,0,158,0,0,0,2847\n",
      "567,9,3,691,8,2506,363,0,690,0,0,55,47,53,7,58,758,86,35,10,64,6,22,270,7,16,0,0,108,0,1,4,232,40,0,0,142,0,1,0,0,2,0,0,0,0,0,3139\n",
      "739,7,2126,10,1,1043,1,1,299,0,15,5,15,21,13,0,40,315,11,0,73,68,1,0,0,0,1,0,218,0,44,0,1,0,0,0,67,0,0,28,0,0,0,0,0,0,0,4837\n",
      "872,66,22,547,0,1893,6,0,145,0,596,0,73,0,6,106,8,88,16,0,2,6,1,0,5,85,5,0,318,0,0,647,0,0,0,9,7,0,0,17,0,1,0,37,0,0,0,4416\n",
      "713,0,49,735,0,1435,32,116,2,27,10,1,1,0,27,4,201,2,9,278,163,254,0,1,7,211,0,0,776,0,0,366,2,0,0,0,34,0,0,3,0,0,401,0,0,0,0,4140\n",
      "771,0,12,39,28,1187,196,0,134,55,335,625,2,3,410,90,229,0,707,8,1,1,157,24,87,0,5,0,113,0,0,785,21,0,0,4,120,0,0,2,194,174,125,0,0,0,0,3356\n",
      "1481,14,179,39,2,2218,67,5,161,566,172,135,51,0,3,111,12,63,5,2,4,0,262,4,26,2,1,0,0,0,0,59,0,0,0,51,0,0,25,1,0,0,0,0,0,0,0,4279\n",
      "1519,0,16,895,3,1688,8,0,17,0,18,1,18,0,162,33,428,0,755,0,0,0,9,0,0,242,0,0,178,0,109,82,0,0,7,0,24,0,0,150,0,0,0,0,0,0,0,3638\n",
      "433,15,551,1771,0,1665,82,24,39,1,38,0,12,203,1,2,4,15,346,2,0,299,296,127,57,0,0,0,27,0,0,7,26,0,0,0,147,0,0,13,0,0,0,114,0,0,0,3683\n",
      "453,6,855,102,6,401,14,39,165,111,14,0,30,196,873,741,12,2,91,466,11,268,383,38,0,0,0,350,4,0,296,0,3,0,0,56,2,150,155,0,1,0,1,0,0,0,0,3705\n",
      "333,0,74,306,0,1686,314,0,18,0,188,5,93,260,757,7,52,120,331,368,48,105,0,12,311,2,19,0,81,0,27,771,3,0,1,0,856,0,0,0,0,0,6,0,0,4,0,2842\n",
      "556,0,522,390,116,2173,423,5,8,0,10,1,435,0,59,0,0,19,160,250,32,5,125,25,0,0,0,0,4,0,286,84,95,0,0,39,67,0,0,0,0,1,0,0,0,0,0,4110\n",
      "414,49,474,719,0,511,227,2,582,348,0,3,63,36,27,542,109,1,26,374,30,71,0,37,0,42,0,88,79,0,0,1186,90,0,0,0,136,0,0,0,0,0,100,0,0,0,0,3634\n",
      "1373,4,568,563,271,1931,108,0,91,10,87,8,45,52,20,34,4,160,13,49,39,0,6,1,0,13,23,0,4,55,35,393,4,0,0,135,130,0,0,0,0,0,123,0,0,0,0,3648\n",
      "301,899,12,416,0,2128,742,514,4,0,122,300,45,10,24,1,514,322,563,72,0,191,62,0,0,0,0,0,553,0,39,0,0,0,0,6,356,0,0,15,0,0,0,0,0,0,0,1789\n",
      "1567,0,90,89,0,1502,234,3,84,0,97,1,54,31,66,0,262,0,47,260,0,0,2,0,0,0,8,0,261,0,0,372,295,0,0,2,3,0,0,0,0,35,0,0,0,0,0,4635\n",
      "600,0,2,134,66,1859,237,1349,369,15,117,2,16,167,40,88,122,1,170,8,302,0,224,145,2,1,204,0,0,0,13,57,0,0,0,42,69,0,0,65,0,0,0,0,0,0,0,3514\n",
      "1194,5,60,625,2,767,28,6,993,0,2,21,20,78,57,95,36,1,5,0,0,12,649,0,1,0,1,9,214,563,0,76,35,60,0,0,3,0,0,0,1,45,0,90,0,0,0,4246\n",
      "1132,0,55,319,13,1916,347,0,1433,5,0,0,4,0,8,28,4,39,92,0,4,0,304,0,0,4,0,0,67,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4226\n",
      "1612,0,567,31,15,1688,122,5,914,44,17,10,1,0,0,5,1,40,308,39,4,15,191,3,0,130,0,206,0,0,0,1037,4,0,0,0,16,0,0,0,0,0,0,0,0,0,0,2975\n",
      "590,0,1063,572,0,1369,49,0,464,19,0,10,0,3,7,135,15,8,631,6,2,71,113,0,0,41,0,214,98,36,0,0,254,0,0,0,110,0,0,9,40,0,0,0,0,0,0,4071\n",
      "893,0,808,66,0,1693,68,0,49,3,5,1,0,0,1098,3,0,0,1,1245,0,102,121,0,319,0,0,0,27,0,0,95,0,0,0,0,3,0,0,0,0,13,44,0,0,3,0,3340\n",
      "1506,107,458,203,0,927,291,0,7,27,325,66,182,260,333,12,22,239,41,27,0,4,14,0,0,0,1,1,876,0,0,19,111,2,0,3,4,0,3,8,0,0,0,0,0,0,0,3921\n",
      "1477,337,68,64,0,883,349,1,433,0,97,8,2,0,148,0,6,44,737,89,14,21,343,0,0,34,16,197,188,0,225,10,6,0,0,165,273,0,0,0,0,0,486,0,0,0,0,3279\n",
      "1104,0,400,477,0,2141,778,11,175,1,0,33,13,8,8,334,0,8,375,79,0,19,1,26,0,0,2,34,1,0,0,264,0,0,0,0,242,0,0,0,0,0,7,0,0,0,0,3459\n",
      "558,0,71,135,115,1696,64,5,13,51,0,58,0,409,4,0,362,326,34,18,14,0,362,24,0,0,9,0,39,1,5,130,2838,0,0,0,0,0,0,0,0,0,8,0,0,0,0,2651\n",
      "1975,0,260,521,0,1104,9,33,865,0,152,117,19,102,191,75,259,8,675,0,0,15,116,5,0,0,137,37,242,1,0,559,8,0,26,0,58,0,0,1,0,0,249,0,0,0,0,2181\n",
      "1645,0,69,57,0,1124,171,2,10,9,85,14,48,214,23,0,260,36,41,236,0,101,21,2,0,49,47,25,765,0,6,469,2020,1,0,1,223,0,0,0,0,0,0,0,0,0,0,2226\n",
      "613,1,118,702,61,1103,84,23,903,0,267,36,0,19,128,15,4,35,168,343,0,0,576,38,0,28,0,261,21,1,246,52,21,0,0,6,0,0,0,49,0,1433,0,0,0,0,0,2645\n",
      "1087,0,13,196,4,779,0,22,111,73,182,0,111,0,90,11,28,0,5,0,0,86,37,215,88,4,411,0,158,0,1,986,34,0,0,0,1,0,0,0,51,0,0,0,0,0,0,5216\n",
      "362,0,765,207,0,947,1,31,1,23,483,0,31,1,133,93,371,2,326,142,123,0,821,59,0,14,0,17,1029,0,323,128,0,0,0,373,46,0,32,0,2,0,0,0,0,0,0,3114\n",
      "452,0,884,58,31,2294,4,7,352,0,2,2,113,0,122,40,401,0,0,63,0,6,123,14,0,20,16,31,839,0,5,715,0,0,0,0,17,0,0,1,0,0,3,0,0,0,0,3385\n",
      "492,0,7,314,0,525,108,14,158,0,36,0,32,0,77,90,746,427,275,329,0,0,1349,7,0,791,52,66,124,0,0,2,2,0,0,2,83,0,0,0,0,0,0,0,0,0,0,3892\n",
      "1667,32,88,1600,1,919,3,0,2,190,427,0,0,2,18,0,2,94,64,131,0,0,2,2,0,0,0,0,207,14,417,550,1,0,0,0,95,0,0,6,0,0,0,0,0,0,0,3466\n",
      "416,2,131,438,12,453,33,2,291,7,21,48,346,18,219,4,20,786,189,37,4,3,0,1,0,0,0,1010,827,0,89,633,198,0,0,3,2,0,0,2,0,0,0,73,0,0,0,3682\n",
      "909,0,24,963,62,507,2,274,32,0,0,10,1,9,18,125,172,356,7,0,0,0,96,0,0,397,0,0,6,1,15,33,241,0,0,20,90,0,0,11,0,0,0,0,0,0,0,5619\n",
      "907,0,217,226,0,2147,64,1,88,2,3,0,743,19,3,3,0,18,417,1,0,0,65,0,0,465,35,0,109,0,9,182,8,0,0,0,169,0,0,0,0,0,2,0,0,0,0,4097\n",
      "1136,0,733,122,0,984,3,17,166,31,34,64,3,577,21,0,447,44,401,146,0,0,16,0,0,0,493,35,474,0,0,59,152,0,0,2,54,0,0,144,0,0,0,0,0,0,0,3642\n",
      "406,546,178,890,0,1139,9,7,536,2,82,66,0,13,21,16,3,0,6,11,742,42,4,356,0,42,69,21,14,0,206,176,119,0,0,0,151,0,0,214,0,0,0,0,0,0,0,3913\n",
      "796,2,130,562,2,2172,328,33,249,48,0,49,0,18,299,7,3,48,12,399,0,0,339,0,4,0,0,120,184,0,0,0,0,0,0,0,0,0,0,4,0,0,0,0,0,0,0,4192\n",
      "1220,0,6,251,3,1346,40,1,82,0,29,5,1,0,274,0,1,271,10,539,94,30,42,4,331,0,0,25,234,74,2,318,0,0,0,3,293,0,0,0,0,0,84,0,0,0,0,4387\n",
      "566,0,311,516,54,2134,9,0,464,0,410,86,0,3,0,1,1,334,358,3,75,112,1029,0,15,0,12,0,61,0,100,55,55,0,0,0,0,75,0,0,0,0,0,0,0,0,0,3161\n",
      "1459,655,158,840,88,755,5,3,85,167,16,11,10,349,0,30,1059,0,59,1,108,248,154,0,0,0,0,24,985,0,3,537,12,0,0,0,9,0,0,37,0,0,0,0,0,2,0,2131\n",
      "3792,189,164,134,18,579,32,30,169,7,289,2,1272,0,1,6,0,0,2,0,0,0,49,2,0,49,234,39,216,0,0,52,16,4,0,0,4,0,0,4,0,1,0,0,0,0,0,2644\n",
      "927,0,69,321,0,2055,67,0,488,64,12,0,124,1,639,1,26,27,1,255,0,1,124,0,0,46,0,0,87,0,1709,424,139,0,0,0,0,0,0,92,186,1,0,0,0,0,0,2114\n",
      "620,50,7,194,0,1378,238,2,729,734,86,0,0,11,9,39,410,184,81,355,222,0,391,0,0,0,0,17,30,0,0,77,149,0,0,104,433,0,146,4,0,0,0,0,150,0,0,3150\n",
      "588,14,542,753,0,496,48,67,392,95,137,330,205,454,161,0,0,10,158,25,12,18,26,22,0,0,1,0,595,0,0,599,7,0,0,0,28,0,0,139,0,0,0,0,0,0,0,4078\n",
      "699,0,493,733,0,1161,0,0,231,32,234,38,12,2,513,3,5,32,1353,1,6,13,0,0,0,0,116,24,44,26,0,633,3,0,0,0,124,0,0,138,0,0,0,0,0,0,0,3331\n",
      "658,228,329,144,130,543,289,41,127,0,552,71,6,6,31,0,123,0,109,10,0,2,780,0,1,36,0,74,572,0,0,131,41,0,0,0,0,0,0,0,0,0,0,0,0,81,0,4885\n",
      "1887,0,84,241,0,1164,450,0,102,0,0,13,2,0,183,45,0,0,266,4,0,0,0,0,89,0,0,0,196,0,1,68,53,0,0,0,207,0,0,0,0,0,0,0,0,0,0,4945\n",
      "986,35,125,227,56,2076,285,5,35,340,94,6,1,2,322,0,255,399,72,80,0,154,36,0,0,0,32,3,390,0,0,0,287,0,0,19,155,6,347,121,7,0,3,0,0,0,0,3039\n",
      "1893,0,93,27,0,1110,556,3,86,37,415,13,137,0,280,0,12,19,625,111,81,77,274,0,47,0,0,5,489,0,0,127,33,0,0,0,75,0,0,22,0,0,0,0,0,0,0,3353\n",
      "3284,0,190,93,0,1281,240,20,788,2,128,94,28,3,93,0,19,26,3,1,0,142,935,2,17,8,0,24,353,0,0,314,1,0,0,0,22,0,0,2,0,0,0,0,0,0,0,1887\n",
      "1466,56,283,271,0,1237,45,23,1231,0,24,1,186,495,47,456,459,4,0,68,0,0,82,53,0,61,1,6,142,0,207,331,1,0,0,0,49,0,0,0,4,31,0,91,1,0,0,2588\n",
      "575,0,527,951,540,1564,512,140,225,30,64,5,0,292,20,31,8,8,41,27,1,70,713,60,0,1,162,0,160,0,0,4,18,0,2,0,209,0,0,0,0,0,56,0,0,0,0,2984\n",
      "348,5,20,383,0,2484,145,36,1552,7,20,34,358,4,63,4,0,1,38,9,1,33,17,16,0,2,16,4,900,0,506,87,0,0,634,0,82,0,0,0,39,0,0,12,0,0,0,2140\n",
      "688,23,34,472,0,1130,199,0,95,55,4,336,519,107,83,910,100,199,443,0,0,1,98,0,7,16,37,9,367,0,0,107,299,0,0,0,39,0,0,114,0,0,0,0,0,0,0,3509\n",
      "1064,34,497,480,0,917,189,0,135,0,72,70,94,177,36,3,288,0,561,65,1,119,38,17,0,0,0,44,14,0,0,983,0,0,0,0,193,0,0,0,550,12,0,159,0,0,0,3188\n",
      "281,7,248,68,0,1303,7,21,1307,0,55,34,407,188,32,0,272,2,188,242,0,3,467,0,0,109,0,0,739,0,0,47,0,0,10,0,5,0,0,0,2,0,0,0,0,0,0,3956\n",
      "698,34,61,443,15,1225,526,21,209,1,53,0,972,9,2,70,78,0,27,13,0,1,21,9,0,42,55,46,36,366,3,1536,6,0,0,308,830,0,0,0,1,0,0,0,0,0,0,2283\n",
      "941,3,92,478,85,1584,118,66,366,0,163,0,4,0,6,8,4,745,3,40,0,0,0,0,0,0,17,0,278,0,0,227,92,0,0,0,0,0,191,808,0,0,0,0,0,0,0,3681\n",
      "1036,0,336,1014,390,1206,8,175,448,11,552,760,110,4,27,26,10,0,1159,61,14,277,7,0,0,0,0,63,20,0,16,89,16,0,0,3,94,0,0,0,5,0,0,0,0,0,0,2063\n",
      "1644,143,417,92,0,1004,0,0,212,0,4,6,52,0,1,16,1508,2,0,71,1,321,144,209,1,33,0,0,7,0,0,31,0,0,0,0,761,0,0,0,0,31,0,0,0,0,0,3289\n",
      "848,0,495,336,0,897,93,14,402,10,41,0,537,32,122,0,55,2,678,95,0,3,124,4,0,0,0,0,58,0,15,1,5,0,0,1,145,0,0,0,0,0,0,0,0,0,0,4987\n",
      "574,0,317,214,274,1495,4,42,136,44,270,248,0,17,54,2,565,226,423,561,0,97,94,0,0,0,0,1,4,0,0,47,128,0,0,0,0,0,0,0,363,0,0,0,0,0,0,3800\n",
      "425,0,70,509,0,630,1010,4,42,0,564,28,464,0,883,47,30,161,16,1,15,43,150,0,0,11,0,0,299,0,38,9,16,0,0,138,0,0,0,0,0,0,0,0,0,0,11,4386\n",
      "224,14,81,992,26,1298,11,3,447,23,3,0,0,10,61,290,5,38,226,612,12,160,2,0,0,103,0,0,6,0,0,60,0,0,0,0,220,0,0,0,9,0,0,0,0,0,0,5064\n",
      "365,41,430,90,13,2312,737,105,81,0,118,287,0,44,80,7,20,0,105,228,0,0,348,507,1,2,573,0,5,0,0,80,88,0,0,1,115,0,0,0,0,0,0,0,0,0,0,3217\n",
      "502,0,575,76,0,2389,9,0,6,18,179,0,3,282,17,0,288,74,212,400,89,2,33,0,0,0,112,229,244,0,36,91,1,0,0,0,214,0,0,574,0,0,0,0,0,19,0,3326\n",
      "1114,0,29,287,0,997,247,0,119,0,10,58,178,1,155,62,21,259,106,16,0,0,10,0,0,89,0,17,2707,0,16,9,486,0,0,0,12,0,0,0,0,0,1,0,0,0,0,2994\n",
      "1347,94,98,386,30,1258,42,3,583,4,114,5,2,407,50,0,5,2,62,0,0,0,721,58,0,0,2,3,54,0,0,29,23,0,0,0,413,0,0,0,5,0,0,0,0,0,0,4200\n",
      "1773,0,1120,908,0,1288,537,0,398,0,59,0,0,0,12,0,374,3,41,553,2,18,82,0,0,0,14,0,43,0,0,49,108,0,0,0,2,0,0,1,0,0,0,0,0,0,0,2615\n",
      "218,7,1069,72,0,2185,50,10,761,28,1,0,76,14,55,35,5,0,199,163,500,8,0,0,0,6,0,0,414,0,0,508,0,0,0,0,43,0,0,4,1,55,17,0,0,0,0,3496\n",
      "374,0,739,61,102,1229,18,157,313,0,33,0,4,3,480,183,25,3,135,7,0,379,185,0,0,156,43,0,143,0,0,33,0,0,0,0,116,0,0,1,0,0,416,0,0,0,0,4662\n",
      "1292,13,139,600,0,942,49,14,160,19,27,802,0,1,0,0,96,0,0,251,0,0,88,0,12,0,1,4,721,0,132,245,278,0,0,5,0,0,0,91,0,1,0,0,0,0,0,4017\n",
      "1139,0,162,109,248,1633,35,174,69,0,43,15,3,252,5,3,82,3,3,0,158,12,268,20,160,0,1,0,761,7,20,1048,38,0,0,47,215,0,0,0,0,0,36,0,0,0,0,3231\n",
      "305,2,831,444,0,829,271,0,100,0,2,16,32,102,32,23,177,0,40,1,37,0,773,0,1,214,7,0,119,40,0,0,0,0,0,0,8,0,0,11,0,1,0,0,0,8,0,5574\n",
      "1332,0,75,457,0,1422,5,6,400,336,92,11,117,24,9,3,32,21,22,72,2,0,19,0,24,61,0,6,421,0,0,38,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,4991\n",
      "1153,62,423,313,130,335,569,51,527,16,59,1,339,0,98,3,590,0,52,129,0,78,308,8,0,131,0,1,74,0,107,124,97,0,0,0,0,0,0,2,0,0,0,0,0,0,0,4220\n",
      "1211,0,696,262,223,667,40,0,20,0,192,11,0,0,41,134,3,82,349,98,0,0,226,0,0,10,0,99,167,0,0,194,44,0,0,0,39,0,626,18,0,7,0,0,0,0,0,4541\n",
      "516,29,4,758,0,257,9,6,229,0,142,129,12,0,331,1,101,3,12,0,29,27,12,2,10,0,0,2299,20,0,0,71,0,0,0,10,1,0,0,0,0,0,0,0,0,0,0,4980\n",
      "1419,1,80,366,0,1020,277,37,61,47,88,9,0,0,215,123,3,102,54,1,2,0,136,8,202,9,0,11,226,36,0,32,1200,543,0,161,0,0,0,0,0,0,0,0,0,0,0,3531\n",
      "1633,0,469,187,458,863,107,549,23,6,85,23,2,123,1,52,662,19,62,231,0,44,689,0,0,4,0,26,865,0,0,18,30,0,0,0,2,10,60,0,82,0,0,0,0,0,0,2615\n",
      "1190,0,1,795,0,2127,0,7,264,0,1,3,22,0,6,4,763,83,253,90,0,16,136,0,0,309,0,0,363,0,0,334,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3233\n",
      "218,1,288,945,1,1264,91,0,7,16,557,584,1,94,11,0,1,0,314,15,84,30,54,112,0,11,0,44,248,0,0,675,27,0,0,0,55,0,0,13,0,0,0,0,0,0,0,4239\n",
      "1554,148,19,489,0,807,155,0,741,1,384,22,0,320,175,29,0,0,75,9,0,372,4,2,0,0,8,0,127,0,136,66,50,0,0,0,104,0,0,0,37,0,51,0,0,0,0,4115\n",
      "1282,217,48,193,5,1234,59,0,575,0,154,711,204,56,0,2,323,2,11,7,0,1,1306,4,0,365,1,1,397,0,64,11,173,0,0,0,60,0,0,0,0,0,0,0,0,0,0,2534\n",
      "422,0,1009,401,0,853,124,4,214,2,830,41,23,1,369,301,5,0,417,9,86,0,460,0,5,336,0,0,52,0,0,746,0,0,0,0,49,0,0,0,0,0,0,0,99,0,0,3142\n",
      "620,0,36,376,0,1487,173,0,30,135,522,7,88,1,13,3,0,147,395,41,137,0,235,15,0,22,0,5,380,0,0,2,42,0,0,54,226,0,0,694,0,0,0,0,0,0,0,4114\n",
      "1481,0,2,129,13,1399,301,6,334,40,133,701,6,19,4,8,615,99,204,52,25,3,109,0,1267,2,0,0,193,4,18,222,1,0,0,0,85,0,0,11,0,0,0,0,0,0,0,2514\n",
      "667,0,104,1446,1,1492,299,20,435,0,39,11,22,1,57,51,329,4,11,32,0,0,6,2,0,0,46,23,27,0,0,35,2,0,0,0,1151,0,0,0,0,2,0,0,0,0,0,3685\n",
      "3059,0,239,96,0,766,524,0,449,0,10,0,28,148,14,6,218,153,766,1,1,0,671,0,0,0,0,0,7,0,10,92,12,0,0,0,180,0,0,0,0,0,0,0,0,0,0,2550\n",
      "308,6,310,704,41,1370,32,0,135,0,29,71,4,0,388,3,91,0,239,291,3,0,34,1,0,21,0,0,630,0,0,0,2,0,1430,0,78,0,0,0,0,0,0,0,0,0,0,3779\n",
      "477,0,1432,677,0,2066,2,0,46,0,17,9,0,24,102,0,314,5,269,944,666,156,23,95,6,4,21,0,17,0,0,5,39,0,0,0,294,0,0,0,0,0,0,0,0,0,222,2068\n",
      "2427,0,392,921,0,657,159,8,182,3,65,0,16,80,9,105,162,0,41,366,0,0,5,1,0,0,0,11,388,46,0,37,5,0,0,0,118,0,0,0,0,0,0,0,0,0,0,3796\n",
      "1718,1,54,496,105,996,0,772,658,0,8,13,21,1,583,660,24,10,114,17,52,0,88,0,0,1,2,3,31,0,0,307,42,0,0,0,133,0,0,1,0,0,5,0,0,0,0,3084\n",
      "767,1,19,9,198,1551,85,0,663,18,10,69,38,18,205,46,73,88,327,12,0,0,242,1,1,7,0,0,47,0,0,1075,2,0,0,0,2,0,0,0,0,37,0,0,0,0,0,4389\n",
      "89,0,886,744,11,1267,426,35,26,0,492,227,120,340,63,1,346,540,14,0,0,31,0,0,0,1,0,0,52,0,0,624,3,0,0,87,115,0,0,0,0,0,0,0,0,0,0,3460\n",
      "1278,4,633,31,0,721,112,3,93,0,188,3,192,0,0,282,1076,1,180,446,0,0,511,10,1,0,0,92,578,0,69,6,104,0,0,0,11,0,225,0,0,0,0,0,0,0,0,3150\n",
      "919,0,57,44,114,1197,0,0,79,4,19,370,30,60,15,0,1,0,90,393,352,1,1831,442,7,0,0,175,25,0,0,25,82,0,0,1,10,0,0,76,0,0,0,7,0,0,0,3574\n",
      "834,3,44,213,12,1528,25,0,301,0,1621,270,17,2,10,230,28,6,74,596,63,119,0,0,0,0,23,0,92,0,447,29,291,0,0,0,17,0,1,8,0,0,0,0,0,0,0,3096\n",
      "771,429,1032,46,0,1951,61,467,168,0,30,0,1,0,625,0,18,49,14,197,190,0,50,13,0,59,0,0,555,0,3,0,141,0,0,0,45,0,0,17,0,0,0,0,0,0,0,3068\n",
      "1653,0,270,134,0,931,60,81,294,2,0,85,0,427,2,4,37,6,221,94,0,11,125,86,37,0,578,0,213,0,6,15,66,0,0,0,256,0,0,2,0,0,10,0,0,0,0,4294\n",
      "1223,0,101,327,361,1377,1270,12,832,0,57,0,498,510,78,17,4,0,36,61,1,1,119,740,3,0,0,0,44,0,0,79,47,0,0,0,184,0,0,0,0,0,0,0,0,23,0,1995\n",
      "1353,274,322,83,0,1332,21,7,94,2,828,60,718,0,142,143,22,17,151,194,0,0,123,0,0,158,4,4,43,0,0,58,56,0,0,1,118,0,0,0,0,0,0,0,0,0,0,3672\n",
      "664,0,300,970,0,2394,949,192,232,0,3,0,0,36,12,5,0,0,4,92,0,1,50,44,0,0,79,8,118,0,9,57,75,0,0,3,455,0,0,0,13,0,0,0,0,340,0,2895\n",
      "350,40,59,876,436,352,47,6,212,0,4,103,1,139,0,2,345,164,67,3,3,0,38,0,0,0,0,3,22,0,0,376,2044,33,0,107,4,0,2,0,0,0,1,0,0,0,0,4161\n",
      "606,1,80,473,0,2120,35,0,77,0,17,0,124,0,621,263,102,125,224,59,0,1,60,0,2,11,0,0,225,0,0,71,49,0,0,45,65,0,0,2,0,0,8,0,0,0,0,4534\n",
      "1649,0,698,211,0,1057,586,696,198,0,0,165,0,6,59,217,34,0,0,236,2,0,803,0,0,1,13,65,358,0,0,19,36,0,0,0,73,0,0,34,0,0,706,0,0,0,0,2078\n",
      "785,15,265,1620,0,1371,9,109,0,11,254,2,0,0,10,0,174,1,591,46,12,1,595,0,0,47,0,0,294,0,0,115,25,0,0,0,39,0,0,0,1,0,0,0,0,0,0,3608\n",
      "2242,163,49,411,268,907,78,1,73,2,51,8,0,161,0,107,3,5,11,381,14,1,30,0,0,0,0,2,22,0,0,384,24,0,0,0,374,0,0,6,0,0,35,0,0,0,0,4187\n",
      "539,0,185,283,0,1077,662,0,89,0,262,177,5,1,294,5,91,644,125,3,2,0,100,5,31,5,2,0,3,0,1,47,134,0,0,0,1,0,0,345,0,0,0,0,0,0,0,4882\n",
      "498,0,15,430,7,2210,14,7,447,0,51,89,23,41,8,72,6,0,1098,64,3,16,28,0,80,130,0,0,106,0,5,229,122,0,0,12,0,0,0,412,0,0,0,0,0,0,0,3777\n",
      "2077,64,122,145,4,234,238,0,62,0,180,0,11,162,69,41,68,4,54,627,0,21,270,23,0,8,5,18,210,0,65,464,31,0,0,0,0,0,0,70,0,0,0,0,0,0,0,4653\n",
      "336,137,300,131,0,1477,0,336,175,43,23,73,97,0,49,12,62,3,1023,248,0,564,42,0,0,43,0,95,13,0,0,281,4,0,0,5,132,0,610,4,0,0,0,0,0,0,0,3682\n",
      "675,17,257,1364,4,1092,192,1,28,100,122,853,1,1,159,110,0,3,12,2,244,25,6,0,0,0,11,0,337,0,0,171,540,0,0,0,75,0,0,6,55,0,0,0,0,0,0,3537\n",
      "370,168,100,509,0,1800,68,88,94,21,0,206,37,1,554,0,68,210,9,123,47,0,300,0,0,10,18,1,269,0,0,0,1,0,0,0,7,3,32,0,0,0,0,0,0,0,0,4886\n",
      "1317,0,70,140,0,1820,91,11,239,0,2,0,4,2,100,0,15,0,435,677,58,0,0,0,0,0,0,92,568,0,0,308,672,0,0,1,21,0,0,0,2,0,5,0,0,0,0,3350\n",
      "579,386,234,538,0,504,276,4,510,23,388,19,3,1091,523,13,128,153,24,109,266,0,45,0,0,1500,39,6,41,14,0,0,3,4,0,0,99,0,2,2,0,49,0,0,0,83,0,2342\n",
      "1682,456,1,282,0,2567,178,1,24,0,149,0,17,2,94,6,1,1,1,34,0,0,119,21,0,219,0,0,161,0,13,43,17,0,0,0,16,0,191,0,0,0,0,0,0,0,0,3704\n",
      "141,0,216,386,3,2963,63,139,531,0,391,25,108,57,57,0,8,35,296,344,0,0,516,550,0,645,4,0,152,0,0,78,18,0,0,356,18,0,0,16,0,1,0,0,0,0,0,1883\n",
      "889,0,114,41,0,1047,86,0,237,33,295,0,1084,0,151,10,1,1,18,197,804,0,12,0,1,0,0,0,306,0,0,488,25,0,0,6,153,0,0,267,37,0,0,0,0,0,0,3697\n",
      "651,19,3,239,21,1139,124,0,237,1,153,13,107,180,89,2,1,0,808,311,0,11,754,107,0,1,0,124,94,0,1,294,519,0,0,0,1,0,0,10,0,0,21,0,0,0,0,3965\n",
      "1772,1,73,124,0,1448,29,5,216,0,75,223,23,2,1,45,2,46,169,172,3,0,324,468,572,36,0,3,120,0,0,17,0,0,0,0,32,0,0,0,0,0,840,0,0,0,0,3159\n",
      "404,69,667,359,18,2233,315,48,346,3,7,0,111,0,22,0,5,85,627,91,8,0,258,0,6,0,386,0,20,0,0,190,0,0,0,3,19,0,0,22,1,0,0,0,0,0,0,3677\n",
      "1213,0,0,691,5,802,146,342,122,3,8,82,27,6,1,0,0,30,86,22,0,3,57,134,426,67,0,36,31,0,194,11,118,0,0,0,189,0,0,0,9,0,0,0,0,0,0,5139\n",
      "512,0,163,736,33,644,504,5,1133,0,2,248,0,119,2,24,948,193,76,185,0,0,152,56,0,0,1,0,95,0,0,11,211,0,0,0,29,0,0,0,0,0,0,0,0,0,0,3918\n",
      "824,0,784,149,0,1054,315,8,274,0,706,0,234,2,2,2,105,59,107,136,0,118,22,19,0,59,140,359,29,0,152,11,0,0,0,0,9,0,0,2,0,0,0,0,0,0,0,4319\n",
      "1266,6,111,19,0,402,8,8,85,0,125,14,699,0,21,70,154,0,873,119,0,133,108,0,17,16,0,7,136,0,4,1,0,0,0,0,30,0,0,0,0,0,0,0,0,0,7,5561\n",
      "1285,9,291,151,2,585,881,144,255,110,140,5,2,0,113,4,1,304,257,11,118,0,304,0,0,0,6,7,149,11,0,53,351,0,0,16,316,0,370,0,0,0,0,0,0,0,0,3749\n",
      "642,47,275,1277,0,1960,56,31,844,36,14,61,648,1,0,0,5,0,282,149,0,13,59,0,0,3,0,0,535,0,0,439,52,0,0,0,55,0,0,0,0,0,0,0,0,0,0,2516\n",
      "837,1,45,474,2,2891,342,0,257,0,26,227,0,20,88,0,1,40,164,521,0,154,66,0,0,0,7,397,21,0,2,17,0,20,0,2,165,0,0,0,16,0,0,0,0,0,0,3197\n",
      "455,37,141,1268,17,1874,5,14,470,2,37,15,0,1,592,120,4,0,161,120,9,1,9,0,0,2,0,0,6,0,1,817,2,0,0,0,311,0,0,11,0,0,0,0,0,0,0,3498\n",
      "340,196,33,399,0,1463,1,1,291,63,32,2,0,0,193,7,12,48,81,151,3,71,0,0,0,1,0,0,8,0,381,574,362,0,0,0,365,0,0,0,0,0,0,0,0,0,0,4922\n",
      "619,0,195,195,0,162,40,45,1162,0,1769,57,254,0,849,0,406,32,7,327,0,90,116,0,0,93,3,0,103,28,0,24,0,0,0,53,48,0,0,0,0,0,0,0,0,0,0,3323\n",
      "1393,242,421,428,0,421,0,2,126,0,12,0,1560,106,3,417,1,3,79,730,0,187,46,0,0,0,0,1033,227,0,0,7,5,0,0,113,2,0,0,0,0,11,1,0,0,0,0,2424\n",
      "309,1,205,1287,36,407,27,11,354,0,44,24,393,123,116,0,0,37,335,3,0,262,403,0,0,73,0,1,41,0,0,104,192,0,0,0,55,0,0,113,0,0,0,3,0,0,0,5041\n",
      "806,1,364,21,14,2121,301,5,242,0,69,9,73,0,38,0,4,59,0,128,27,6,181,0,0,37,0,0,76,0,2,106,0,0,0,3,219,0,0,0,0,0,0,0,0,0,0,5088\n",
      "417,23,35,162,218,1829,15,72,661,1,535,39,71,14,1,54,75,85,83,29,378,271,114,34,0,2,0,15,576,0,105,149,459,0,0,0,194,0,0,0,0,0,30,0,0,0,0,3254\n",
      "476,0,731,456,2,753,64,6,320,169,21,326,90,0,82,1,9,0,190,63,113,2,0,228,0,52,0,0,5,0,1,20,7,0,0,0,135,0,0,0,2,0,0,0,0,0,0,5676\n",
      "654,0,350,359,9,3490,185,128,52,0,0,1,62,18,25,0,270,8,13,31,4,0,14,0,0,22,0,738,193,0,26,282,2,0,0,0,3,0,5,8,0,0,0,0,0,0,0,3048\n",
      "1968,0,158,87,136,1509,400,9,514,0,215,105,9,0,0,5,3,0,3,6,0,0,224,6,0,4,0,0,71,0,1,2,516,0,0,0,358,0,0,12,1,0,0,0,0,0,0,3678\n",
      "916,0,461,93,4,948,198,2,696,36,37,22,1,114,403,0,121,406,262,145,6,7,270,178,0,0,91,12,164,0,0,289,204,6,0,0,16,0,0,0,0,0,0,0,0,0,0,3892\n",
      "582,8,154,648,0,1592,86,0,328,16,72,0,2,0,149,172,0,38,411,1131,0,0,154,0,0,5,0,0,407,0,0,1163,31,0,0,0,1,2,0,0,0,0,8,0,0,0,0,2840\n",
      "2567,0,18,552,0,636,1,22,92,3,104,12,0,0,34,50,71,222,117,61,0,287,130,0,0,0,0,10,334,0,0,705,6,0,0,0,0,0,0,3,0,0,0,0,0,0,0,3963\n",
      "2108,360,303,392,38,675,67,0,242,2,1,19,92,214,171,0,151,55,13,44,11,0,37,37,0,0,0,2,512,0,0,82,61,0,0,0,0,0,0,3,3,0,0,0,0,0,0,4305\n",
      "330,48,33,473,0,1585,3,22,544,0,14,360,303,0,98,5,0,0,158,3,14,68,310,17,0,27,0,427,81,344,2,61,0,0,0,0,248,0,0,23,0,0,5,0,0,0,0,4394\n",
      "1329,0,83,35,0,549,11,12,581,42,110,0,20,0,3,8,645,414,38,164,2,0,216,356,0,3,4,1185,81,0,0,245,2,0,0,1,239,0,0,4,0,1,0,0,0,0,0,3617\n",
      "1427,13,693,601,0,1213,386,6,625,132,239,0,83,0,1,1,1,14,24,102,0,0,2,0,12,139,0,16,737,420,0,91,0,0,0,2,19,0,0,0,0,0,7,0,0,0,0,2994\n",
      "342,0,443,126,0,1407,279,2,23,136,49,29,69,0,155,466,54,0,1524,380,0,0,15,30,0,0,0,0,539,0,0,31,0,0,0,23,87,0,0,3,0,0,0,0,0,0,0,3788\n",
      "1715,45,316,74,0,1104,621,3,3,585,4,11,19,2,164,0,78,0,5,4,0,0,329,0,35,186,3,0,109,144,0,632,21,0,0,0,345,0,0,0,428,1,0,0,0,0,0,3014\n",
      "1954,7,208,411,67,408,74,484,3,0,964,0,13,37,762,5,0,0,252,2,69,0,794,0,14,394,0,0,81,0,29,586,66,0,0,4,2,0,27,0,0,0,0,0,0,0,0,2283\n",
      "1653,6,734,542,0,1336,14,0,168,117,215,15,83,0,2,22,10,0,0,414,43,0,4,27,0,452,345,0,5,0,39,8,146,342,0,0,2,0,0,0,2,0,0,0,0,0,0,3254\n",
      "193,0,165,159,1,1135,17,0,299,8,267,14,404,47,74,4,168,140,0,333,0,3,37,0,0,0,0,0,24,0,34,118,0,0,0,0,1,0,113,21,0,0,0,0,0,0,0,6221\n",
      "634,256,1116,380,0,1302,276,124,625,0,69,2,34,0,0,10,6,84,349,5,34,134,38,0,0,0,0,0,8,0,95,12,41,0,0,1,27,0,0,1,0,0,1,0,0,0,0,4336\n",
      "3030,198,284,1115,0,882,108,1,994,0,148,60,309,0,2,0,1,204,64,0,1,210,170,7,0,0,0,33,400,0,60,11,7,0,0,0,0,0,0,0,20,5,0,0,0,0,0,1676\n",
      "1446,1,4,31,112,2389,128,8,749,1,365,10,0,0,15,0,4,5,87,347,166,165,405,0,45,157,0,26,98,0,31,5,31,0,0,0,2,0,0,0,0,5,0,0,0,0,0,3162\n",
      "286,184,262,1199,47,1230,254,72,153,0,236,1,4,0,149,59,3,0,19,0,124,0,221,3,1,0,0,430,152,0,71,311,234,0,0,4,251,0,0,0,2,0,0,0,0,0,0,4038\n",
      "1747,0,315,71,0,1212,70,811,219,7,153,4,0,0,50,1,83,74,24,639,1,0,10,0,0,1,4,0,2,43,1,333,0,52,0,1,62,0,0,0,0,0,0,0,0,0,0,4010\n",
      "332,2,90,180,118,2442,339,0,346,0,194,556,0,334,6,11,116,59,14,13,7,2,27,0,0,18,0,0,29,0,3,10,1,0,0,0,1,0,0,6,0,0,0,0,0,0,0,4744\n",
      "1975,0,384,214,0,1099,88,1,61,0,1262,2,0,301,115,4,0,38,44,466,0,0,279,22,42,0,3,33,120,1,11,147,0,401,0,0,578,0,1,0,0,0,26,0,0,0,0,2282\n",
      "1491,1,386,48,187,2005,279,50,344,0,20,14,22,84,565,45,11,8,0,257,0,1,759,1,0,139,0,16,44,0,0,98,0,0,0,0,2,10,0,0,0,0,0,41,0,0,0,3072\n",
      "1190,0,8,519,0,1760,28,0,410,0,46,59,255,27,16,0,3,0,405,292,18,0,216,1,0,52,90,12,395,4,13,109,304,0,0,0,12,0,0,6,0,0,52,0,0,0,0,3698\n",
      "262,0,14,280,19,1518,29,5,202,0,29,118,20,60,0,107,0,9,182,344,0,26,473,6,3,0,0,55,161,27,75,138,16,2,0,3,1620,0,0,5,0,0,0,0,0,0,0,4192\n",
      "211,128,564,598,15,406,596,17,220,0,9,323,446,6,100,1,5,39,50,2,0,1,47,17,405,1,17,0,184,0,0,310,0,0,0,18,152,0,0,278,0,0,0,0,0,0,0,4834\n",
      "619,0,143,1248,0,965,76,150,1133,0,437,697,24,0,400,0,0,28,272,317,6,0,832,0,50,144,0,1,64,0,10,296,84,0,0,0,137,0,0,0,0,0,0,0,0,0,0,1867\n",
      "1270,2,372,1320,0,658,18,0,26,1,699,0,199,199,319,38,111,209,34,11,0,0,223,27,0,638,0,100,175,0,0,40,0,0,0,0,93,0,0,25,0,0,0,0,0,0,0,3193\n",
      "2936,0,69,1476,0,500,369,0,254,0,56,17,88,0,353,0,11,0,32,306,16,0,153,1,0,0,1,104,29,0,27,157,1,0,0,0,24,0,0,0,0,0,0,0,0,3,23,2994\n",
      "518,0,113,218,0,2277,276,0,10,16,637,1,35,1,0,12,102,58,0,1,1,1,414,0,0,0,427,9,231,0,0,415,0,0,0,0,86,0,0,11,0,0,0,0,0,0,18,4112\n",
      "413,14,30,1246,0,927,543,151,11,23,437,114,6,12,32,0,0,16,2,3,1,0,240,32,21,0,0,14,1,0,1,7,0,0,0,0,5,0,22,6,0,0,0,0,41,0,256,5373\n",
      "2618,118,32,86,1,574,15,6,24,37,8,3,0,0,311,185,64,310,325,42,0,4,49,0,0,47,9,26,460,0,166,24,11,0,0,0,206,0,0,41,0,0,0,0,0,0,0,4198\n",
      "1221,0,534,223,0,919,60,148,661,2,266,36,12,0,23,0,18,8,1,26,0,75,895,0,1,725,26,268,111,0,0,376,46,0,0,0,47,0,0,0,1,48,0,0,0,0,0,3223\n",
      "534,1,305,520,0,3251,42,8,310,0,504,15,2,0,103,4,340,1,131,36,0,0,15,0,0,0,6,0,510,0,63,45,477,0,0,0,1,0,0,11,0,0,0,0,0,0,0,2765\n",
      "636,0,194,568,13,1510,201,0,38,2,258,127,41,22,117,4,110,244,229,401,97,735,322,0,21,0,0,0,198,16,0,19,60,0,0,0,39,0,0,0,0,168,0,0,0,0,0,3610\n",
      "880,74,793,282,21,1175,4,5,799,9,350,161,1,46,19,3,5,1,21,53,0,14,1249,15,0,193,0,14,231,0,0,78,2,0,0,0,29,0,0,0,0,2,0,0,0,0,0,3471\n",
      "2231,101,457,647,38,937,90,1,58,11,2,197,16,9,468,28,213,56,9,53,15,0,56,11,0,744,1,1,24,0,0,474,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,3048\n",
      "1744,69,18,1661,8,293,48,0,2,28,806,66,0,92,76,114,10,1,283,54,0,315,92,0,0,716,11,0,448,94,0,53,4,0,0,0,191,0,0,0,0,0,10,0,0,0,0,2693\n",
      "714,46,372,888,0,1356,78,24,49,0,27,6,0,10,624,15,556,261,74,60,0,130,281,0,0,96,28,90,228,0,0,903,13,0,0,44,238,0,0,0,0,0,0,0,0,0,0,2789\n",
      "803,0,440,198,0,692,150,0,25,0,208,267,735,1,420,10,4,28,352,68,80,0,2,0,330,104,0,0,125,0,20,124,2,0,0,4,0,0,0,0,0,0,0,0,0,0,1,4807\n",
      "1021,0,799,188,0,1135,2,0,1311,1,67,1,3,2,134,43,3,0,356,50,0,0,150,30,0,9,13,14,55,0,0,221,35,0,0,0,357,0,0,0,0,0,0,0,0,0,0,4000\n",
      "2491,0,576,160,0,990,263,34,121,0,25,49,46,0,0,344,0,0,19,15,0,26,197,1,0,152,0,1,139,0,67,7,7,0,0,0,289,0,0,0,0,0,0,0,0,0,0,3981\n",
      "1664,0,26,341,32,1197,337,0,239,12,6,0,2,93,91,0,1,0,368,52,0,0,118,0,0,26,152,201,916,0,0,24,128,0,0,0,7,0,124,0,0,0,4,0,0,0,0,3839\n",
      "1780,1,72,397,12,348,67,0,1311,0,4,0,3,0,400,118,236,44,0,0,85,1,3,143,0,719,0,0,381,0,5,5,7,0,0,0,32,0,0,1,5,0,0,0,0,0,0,3820\n",
      "1516,14,167,226,0,1873,10,0,62,18,124,0,112,2,43,542,13,20,178,13,0,0,228,178,51,5,0,0,374,0,54,43,36,0,0,4,99,0,0,9,0,0,0,0,0,0,0,3986\n",
      "1419,19,201,160,0,1740,1150,13,32,13,162,49,81,116,2,1,0,10,0,65,0,0,21,0,64,11,0,253,87,0,0,211,0,0,0,0,42,0,0,0,426,0,0,0,0,0,0,3652\n",
      "1643,0,98,170,4,1326,113,150,523,233,42,3,0,16,3,0,27,11,31,108,0,0,306,171,0,28,0,63,94,0,49,285,0,0,0,3,97,0,0,0,0,0,0,0,0,0,0,4403\n",
      "600,0,197,841,0,969,254,249,497,0,146,103,208,0,103,173,7,97,86,2,0,1,917,0,0,11,206,23,565,0,0,213,299,0,0,0,17,0,0,0,0,0,0,0,0,0,0,3216\n",
      "687,0,618,443,0,1188,443,405,909,6,206,414,148,0,0,0,21,207,67,111,0,16,209,129,0,6,0,0,152,0,0,489,0,0,0,0,0,0,0,3,0,0,0,0,0,0,0,3123\n",
      "856,146,107,2011,2,1399,16,1,63,0,15,0,0,5,27,34,701,0,210,179,0,112,552,1,0,0,0,0,15,0,0,227,204,0,0,0,111,0,258,0,0,0,0,0,0,0,0,2748\n",
      "666,0,178,600,11,689,24,6,10,0,26,15,867,0,93,8,1,215,254,1,0,175,28,365,0,0,2,6,193,0,0,28,5,0,0,6,7,0,0,7,0,0,0,0,0,0,0,5514\n",
      "741,63,499,442,0,3143,496,35,20,0,0,1,180,22,33,37,1,11,124,27,5,2,24,0,0,0,0,0,199,0,0,44,12,0,0,0,467,0,0,222,0,0,0,0,0,0,0,3150\n",
      "1599,6,43,158,9,1533,16,302,139,5,0,137,14,0,543,0,181,0,45,7,0,3,13,12,81,0,0,507,998,0,3,264,14,0,0,0,149,0,262,0,4,0,0,0,0,0,0,2953\n",
      "730,9,0,176,0,3437,234,0,9,1,22,305,0,0,30,0,292,0,0,60,94,0,818,0,2,40,0,0,36,1,221,96,279,0,0,0,149,0,0,0,0,0,0,0,0,7,0,2952\n",
      "971,0,103,120,12,651,3,0,223,86,2,1,120,684,189,12,47,0,116,4,0,0,408,10,960,0,0,0,168,0,39,129,217,0,0,0,147,0,0,0,0,0,0,0,2,0,0,4576\n",
      "596,224,51,134,0,2384,242,292,416,42,17,92,1,6,135,1,76,2,0,151,16,0,5,0,0,312,34,66,425,0,31,8,219,0,0,450,185,0,55,19,0,0,1,0,0,0,0,3312\n",
      "1067,5,238,670,253,796,774,70,456,5,270,757,209,4,2,1,289,125,4,6,15,11,45,2,0,223,0,1,37,0,0,47,3,0,0,1,162,0,106,0,4,0,0,0,0,0,0,3342\n",
      "188,32,1599,1665,0,347,593,23,66,3,590,13,12,24,84,38,10,198,169,289,104,21,138,0,1,14,0,95,524,0,0,41,41,0,0,0,98,0,0,0,0,0,0,0,0,0,0,2980\n",
      "811,66,229,164,0,1616,182,38,159,0,18,63,1,0,63,1,38,176,207,1062,0,0,8,8,0,0,199,79,39,0,219,37,114,0,0,0,6,0,0,0,0,0,0,0,0,0,0,4397\n",
      "657,67,140,52,0,1639,15,148,239,0,891,50,15,83,3,42,24,585,20,17,75,66,422,0,0,109,172,274,13,151,0,10,0,0,0,0,399,0,0,34,0,0,0,0,0,0,0,3588\n",
      "1634,193,459,500,33,1594,90,7,151,0,19,3,642,17,598,0,0,146,29,14,0,38,25,1,0,34,0,2,41,0,54,1,15,0,0,0,131,0,164,1,0,0,0,0,0,0,0,3364\n",
      "1053,0,434,640,0,2345,187,42,270,0,76,43,0,110,52,0,11,5,20,37,68,93,7,0,0,2,0,0,19,0,25,239,416,0,0,1,341,0,7,105,0,0,0,0,0,0,0,3352\n",
      "1566,148,1,381,33,906,86,48,131,0,4,0,21,128,11,0,13,23,12,316,59,255,218,0,0,102,3,0,178,0,262,2,11,0,0,0,50,0,0,0,0,0,0,0,0,0,0,5032\n",
      "238,8,176,74,10,926,338,41,296,0,17,0,143,0,0,8,114,266,121,261,0,56,207,273,3,7,0,11,987,0,0,82,17,0,127,0,154,1,17,0,12,0,283,0,0,0,0,4726\n",
      "271,0,248,1006,10,685,9,218,200,0,3,75,788,0,169,1235,1,88,45,10,5,1,114,0,0,0,5,0,7,0,3,156,349,0,0,0,18,0,0,0,3,0,0,0,94,0,0,4184\n",
      "1090,1,398,267,0,1444,24,0,143,21,455,0,4,0,44,0,1,0,245,1,11,1,19,2,0,64,3,24,42,0,281,51,94,0,0,1,6,0,0,0,0,47,0,0,0,0,0,5216\n",
      "1050,493,1470,105,12,648,6,335,84,0,8,19,0,23,143,181,54,0,35,34,9,0,240,2,0,237,0,27,1,0,0,7,48,0,86,0,262,0,0,0,0,0,0,18,0,0,0,4363\n",
      "641,1,91,186,2,2168,405,93,176,64,16,81,3,5,50,0,23,142,46,39,144,0,5,1,0,149,1,3,6,0,53,0,0,0,0,0,114,0,0,0,23,0,0,0,0,0,0,5269\n",
      "1148,7,30,602,1,1035,368,0,387,18,180,47,85,61,327,105,108,848,1588,139,18,2,329,0,0,273,0,2,14,0,0,37,109,0,0,0,744,0,8,16,0,0,0,0,0,0,0,1364\n",
      "837,0,244,306,2,1325,33,6,654,0,155,81,135,285,24,11,64,2,396,329,152,0,71,382,0,15,0,0,23,0,0,541,39,0,0,0,0,940,0,321,0,0,0,0,0,0,0,2627\n",
      "410,0,1128,136,13,1181,153,44,43,90,313,1,441,283,28,15,255,3,368,244,0,610,72,0,0,9,0,67,129,334,0,48,1,0,0,32,30,0,0,1,11,0,16,0,0,0,0,3491\n",
      "2673,0,145,704,73,1328,246,58,315,44,70,0,0,14,25,4,310,8,11,429,0,4,222,0,0,0,0,0,122,0,0,30,205,0,0,0,851,0,0,0,0,0,0,0,0,0,0,2109\n",
      "471,0,59,25,17,1289,347,4,1155,414,162,0,64,450,341,0,17,2,540,38,347,349,184,0,0,1,85,82,9,0,0,829,3,0,0,3,652,0,0,0,0,0,0,0,0,0,0,2061\n",
      "493,0,300,208,1,1248,8,238,187,0,38,482,135,4,144,0,173,12,130,1296,5,0,480,0,0,0,279,0,38,7,0,160,183,0,0,405,128,18,46,21,0,0,0,0,0,0,0,3133\n",
      "746,25,312,226,0,1392,33,0,469,10,174,212,61,0,47,3,25,0,1,212,0,0,29,110,157,0,103,0,62,0,173,6,0,0,0,0,10,0,0,59,0,0,205,0,0,152,0,4986\n",
      "879,0,47,416,1,1464,335,0,254,0,0,66,29,15,191,0,116,88,35,31,409,40,1,0,0,0,0,0,237,0,165,145,11,0,0,0,121,0,0,1,0,0,0,0,0,0,0,4903\n",
      "1350,0,57,703,29,744,83,3,133,22,55,6,19,24,54,187,8,6,59,38,3,186,23,0,0,25,0,35,55,0,0,0,0,0,0,0,1293,0,0,0,27,0,0,0,0,0,0,4773\n",
      "1826,7,28,372,0,1970,157,0,224,18,14,162,38,0,241,12,5,34,81,70,0,11,1085,5,0,0,58,46,111,0,21,222,10,0,0,0,153,0,0,0,0,0,0,0,0,0,0,3019\n",
      "2550,4,44,45,52,616,289,151,488,179,350,85,305,1,88,30,471,4,241,59,5,0,34,0,155,0,26,1,54,0,11,247,0,0,0,0,0,0,0,0,1,248,2,0,0,0,0,3164\n",
      "297,0,173,326,0,272,165,0,601,77,302,0,2,0,62,435,490,632,9,1132,321,0,123,0,0,83,0,0,12,35,0,16,216,0,0,0,15,0,0,0,0,0,0,0,0,0,0,4204\n",
      "376,13,302,368,140,969,659,2,169,0,0,49,314,58,295,150,70,85,82,1,124,0,199,293,0,0,0,0,1,0,0,1,76,0,0,207,0,0,115,0,0,0,0,0,0,0,0,4882\n",
      "754,5,425,90,1,1455,9,532,84,3,4,41,2,0,990,30,35,579,200,3,0,0,69,0,0,0,0,0,370,0,0,178,23,0,0,1,1,0,0,0,1023,2,0,0,0,0,0,3091\n",
      "1191,18,11,405,0,810,40,461,605,406,217,0,185,10,30,14,250,124,4,513,14,109,10,9,1,0,0,0,441,0,0,102,0,0,0,0,140,0,0,0,0,0,0,0,0,0,0,3880\n",
      "276,150,394,131,0,839,0,0,93,367,0,47,151,221,71,22,289,0,25,1,6,22,217,32,0,14,145,0,394,0,26,8,95,2,0,0,289,0,0,14,0,0,0,0,0,0,0,5659\n",
      "1111,70,412,112,102,656,1063,176,93,0,2,16,0,0,270,0,40,1,130,26,92,0,30,84,0,0,0,0,1065,0,0,0,10,0,0,0,370,0,0,0,216,219,0,10,0,0,0,3624\n",
      "945,0,327,596,14,438,544,0,77,16,12,565,34,4,0,6,0,0,14,18,0,75,80,9,0,0,1,7,1,0,0,35,0,0,0,0,188,0,0,0,0,0,0,0,0,0,0,5994\n",
      "1285,0,870,146,0,1128,65,67,188,5,458,0,17,18,157,217,223,31,94,0,0,3,93,174,0,97,0,2,844,0,0,329,400,0,0,0,2,0,0,0,0,17,12,0,0,0,0,3058\n",
      "1221,0,146,1219,0,1414,407,1,246,6,51,88,1,11,8,29,21,6,3,350,69,0,34,72,0,0,0,0,8,0,0,1,386,0,0,0,1,2,0,0,0,0,0,0,0,0,0,4199\n",
      "1094,0,246,129,0,1689,322,279,218,78,54,5,189,44,64,76,50,8,700,154,3,5,1,0,2,64,172,16,292,0,0,163,11,0,0,0,3,0,11,0,5,0,0,0,0,0,129,3724\n",
      "645,1,308,407,0,3564,138,6,3,246,151,213,13,36,10,212,1,66,5,470,0,0,206,351,0,35,0,42,296,0,30,62,39,0,0,0,23,0,0,1,0,0,0,0,0,0,0,2420\n",
      "344,0,790,465,1,768,15,0,365,202,623,7,145,12,0,0,2,55,9,2,0,7,712,824,0,4,0,44,123,0,0,102,5,0,0,0,487,0,0,0,94,0,68,0,0,0,0,3725\n",
      "822,319,315,90,2,2118,571,0,265,5,952,0,0,275,17,36,163,36,216,245,69,0,736,0,0,26,0,1,344,2,226,44,48,0,0,0,2,0,35,0,8,0,2,0,191,0,0,1819\n",
      "2206,21,363,367,1,1084,466,58,172,105,94,130,5,106,9,0,7,14,8,17,75,0,8,1235,0,0,1,98,252,0,0,155,659,0,0,0,40,0,0,0,0,0,0,0,0,4,0,2240\n",
      "707,0,186,265,0,859,1486,204,216,0,558,0,21,30,17,32,0,21,1,274,12,0,241,13,29,0,10,9,19,0,6,2683,45,0,13,0,13,0,0,0,0,0,0,0,0,0,0,2030\n",
      "527,0,830,327,0,1421,162,25,153,24,85,31,1,12,8,51,17,219,7,16,0,0,944,0,0,1,37,0,12,0,0,199,1,0,0,0,39,0,0,0,0,0,0,0,0,0,0,4851\n",
      "383,60,43,1073,0,1130,1559,0,329,133,1,94,63,0,53,0,0,2,215,463,28,122,204,0,0,0,0,0,178,0,0,25,42,0,1,0,13,0,0,2,0,0,108,0,0,0,0,3676\n",
      "924,0,221,567,0,529,0,185,427,3,584,422,28,971,17,0,0,0,569,8,4,21,138,0,0,4,0,35,56,0,0,29,0,0,0,0,74,0,0,0,0,0,0,0,0,0,0,4184\n",
      "1208,0,35,876,85,1369,307,7,25,1,81,248,49,350,27,313,207,9,6,203,8,209,284,0,9,0,1,715,12,0,32,11,20,0,0,35,211,0,0,0,0,0,0,0,0,0,0,3047\n",
      "1147,36,444,178,0,1143,0,9,264,2,9,159,12,327,13,42,178,1563,2,271,156,224,134,4,0,0,522,0,2,0,0,1,0,0,0,0,139,0,0,0,0,0,0,0,0,16,0,3003\n",
      "920,0,4,677,3,660,261,3,0,5,498,6,7,58,2,194,18,2,47,658,0,0,480,26,0,6,0,101,130,0,0,30,548,0,0,0,12,0,0,0,0,5,0,12,0,0,0,4627\n",
      "1148,3,92,120,129,695,7,48,238,473,16,0,34,0,331,660,107,1,21,0,0,0,95,192,593,0,0,2,414,0,0,0,88,0,0,0,9,0,0,0,112,0,0,12,0,0,0,4360\n",
      "987,0,1022,32,28,398,837,301,298,25,31,0,0,12,27,65,0,12,156,1,467,0,2,0,0,0,0,0,324,0,0,984,259,0,0,0,390,0,0,0,0,0,0,0,0,0,0,3342\n",
      "944,0,77,128,442,1377,103,0,229,0,70,0,113,0,489,56,2,0,175,37,6,0,527,0,0,1,11,87,170,1515,0,0,140,0,0,0,424,0,1,27,24,0,0,35,0,0,0,2790\n",
      "703,0,174,751,316,1933,15,0,0,4,0,46,405,0,35,0,14,53,33,9,10,0,639,0,0,1195,47,0,39,0,0,197,5,0,0,0,529,0,0,0,0,0,0,0,0,0,0,2848\n",
      "1345,11,46,179,280,1583,570,4,7,0,2,0,146,92,15,3,315,58,88,42,48,0,66,0,0,0,600,0,48,0,2,4,203,0,0,2,0,0,3,0,0,0,1,0,0,0,0,4237\n",
      "594,0,557,1297,0,1118,0,380,114,2,22,81,3,1,6,0,282,400,74,52,1,8,29,78,0,142,91,0,299,0,194,4,57,0,0,0,128,0,27,0,0,0,7,0,0,0,0,3952\n",
      "1453,0,103,111,0,1689,541,3,291,2,367,0,362,2,24,11,524,0,11,4,1,53,208,2,0,1,0,0,254,0,42,59,16,0,0,2,175,0,0,49,9,0,0,0,0,0,0,3631\n",
      "2326,91,6,47,13,943,1,0,142,0,450,3,13,0,562,2,983,0,243,0,100,2,8,0,0,1,0,0,370,411,0,42,9,139,0,2,11,0,0,27,0,0,0,0,0,0,0,3053\n",
      "301,1,112,419,0,566,193,0,463,225,27,30,131,63,236,275,512,0,0,108,0,0,373,1,0,22,0,19,426,0,0,350,2,0,0,0,2,0,138,2,0,0,0,0,0,0,0,5003\n",
      "2169,4,63,184,0,760,587,26,709,0,10,29,0,0,82,0,3,0,2007,176,178,0,225,0,4,0,1,0,466,0,0,23,1,0,0,178,178,0,0,21,14,0,0,0,0,0,0,1902\n",
      "1578,0,54,819,2,1143,77,4,158,0,4,335,17,4,38,26,475,34,3,17,4,316,23,0,0,39,11,0,67,0,7,16,110,0,0,29,33,0,0,0,0,0,0,10,0,0,0,4547\n",
      "872,14,60,289,7,1575,28,1070,140,0,465,3,0,2,51,0,141,6,29,19,26,0,31,0,0,5,130,3,315,0,0,464,64,0,0,5,883,0,4,75,6,0,0,0,0,0,0,3218\n",
      "819,0,696,1519,4,1654,59,0,1018,0,139,48,156,0,195,89,397,15,134,285,43,34,0,3,8,0,0,0,274,0,7,10,41,1,0,6,2,0,0,0,0,0,0,0,0,0,0,2344\n",
      "1441,153,526,1565,0,1609,306,253,209,0,1,167,12,0,15,0,0,139,59,13,0,1,75,80,7,43,13,1,341,0,127,6,443,0,0,0,113,0,0,1,0,0,0,0,0,0,0,2281\n",
      "1108,11,119,443,0,827,61,13,161,0,163,26,48,145,108,2,11,7,121,543,706,201,104,14,0,570,0,0,151,0,3,41,54,0,0,1,66,0,0,0,0,0,79,0,0,7,4,4082\n",
      "1188,21,113,182,0,2158,12,63,881,0,0,38,0,14,40,0,21,8,7,94,131,0,209,42,0,4,0,5,527,0,0,27,8,0,0,93,457,0,5,0,0,0,0,0,0,0,0,3652\n",
      "774,108,144,1634,271,329,52,113,206,0,37,14,0,12,23,2,27,36,2,356,19,0,159,9,0,74,0,0,1,0,33,992,412,0,0,0,0,0,0,4,0,0,57,0,0,0,0,4100\n",
      "1366,12,107,1433,15,530,156,14,13,214,9,310,345,1,4,0,563,62,217,0,59,1,114,138,10,0,0,0,1635,12,0,839,37,0,0,0,319,0,0,0,6,0,0,0,0,0,0,1459\n",
      "2022,43,293,123,0,693,342,11,429,3,31,6,16,16,449,0,22,22,314,241,0,0,88,0,0,0,1,0,131,0,0,2,22,0,0,3,0,0,0,5,0,0,208,9,0,0,0,4455\n",
      "1811,0,245,376,0,1257,7,431,450,99,405,0,0,39,843,4,7,0,122,1,326,4,311,0,0,0,4,15,485,0,7,241,107,0,0,7,46,0,10,0,0,0,0,0,0,0,0,2340\n",
      "741,29,311,621,87,1690,0,66,821,4,811,160,85,165,41,207,4,1,3,278,5,188,104,1187,0,0,0,135,0,0,0,23,0,0,0,0,186,0,23,0,2,0,0,0,0,0,0,2022\n",
      "1657,0,574,268,325,1334,27,0,573,0,6,82,11,6,290,7,211,36,41,77,0,2,72,2,409,0,0,257,5,0,0,7,19,10,0,406,64,0,0,92,0,0,0,0,0,0,0,3130\n",
      "762,4,675,508,0,1660,18,21,60,0,0,4,0,592,56,281,2,118,824,4,0,278,317,1,15,0,285,141,518,0,4,311,1,0,0,107,108,0,2,0,0,0,0,0,0,0,0,2323\n",
      "328,0,417,597,4,988,119,0,625,15,38,0,28,0,382,44,358,224,3,0,679,0,0,103,0,0,18,0,82,0,6,39,865,0,0,0,42,0,0,62,0,0,0,0,0,0,0,3934\n",
      "1417,78,70,1352,0,1514,259,201,6,18,10,2,3,294,38,244,269,10,37,16,0,2,243,0,251,512,390,31,31,7,0,1,18,0,0,0,42,0,0,2,53,0,0,0,201,0,0,2378\n",
      "1398,0,335,617,25,1712,8,0,2,9,1,207,1,412,51,44,2,3,45,67,0,5,469,33,0,20,0,0,16,0,6,45,238,0,0,0,2,0,0,31,0,0,0,0,0,0,0,4196\n",
      "1005,136,148,516,0,1840,273,66,445,0,41,45,4,82,15,216,861,103,978,15,0,0,5,11,0,0,0,82,295,0,0,55,0,0,0,12,56,0,0,0,0,0,0,0,0,0,0,2695\n",
      "2703,6,1,732,4,690,186,0,308,194,488,12,81,51,8,0,69,127,462,3,0,384,0,0,0,0,0,0,5,0,0,233,105,0,0,31,258,0,153,198,0,0,0,0,0,0,0,2508\n",
      "463,0,204,373,3,1343,79,77,456,2,96,92,73,468,1,0,5,336,66,180,0,67,115,29,0,0,527,0,369,0,0,21,37,0,0,24,165,0,0,0,0,0,47,0,0,0,1490,2792\n",
      "615,23,102,696,0,910,8,27,156,0,8,0,0,672,10,0,2,4,93,0,0,0,40,277,0,96,0,1,415,0,28,10,23,0,0,0,1245,0,0,330,0,0,0,55,0,0,0,4154\n",
      "1357,112,32,477,11,677,35,16,163,5,33,37,31,0,99,10,126,399,16,0,214,0,96,0,0,4,0,11,201,0,76,7,7,0,0,0,322,0,3,1,0,0,0,0,0,0,0,5422\n",
      "2114,1,580,99,0,863,118,2,151,13,10,3,182,25,1,164,760,19,212,168,0,0,63,0,52,0,0,16,114,0,307,113,31,0,0,0,897,0,0,1,0,0,0,0,0,0,0,2921\n",
      "1018,0,396,485,26,615,118,285,73,13,27,0,394,7,1595,116,157,0,10,4,0,0,71,0,0,512,0,0,42,0,62,284,8,0,0,0,1108,0,0,0,99,0,0,0,0,0,0,2475\n",
      "1516,0,221,81,82,1713,493,636,19,6,3,21,5,0,321,0,61,0,9,24,33,0,12,16,0,0,0,1,145,0,12,370,152,0,0,6,146,0,0,34,0,0,0,0,0,0,0,3862\n",
      "432,2,222,595,369,1777,439,0,209,0,579,87,3,0,44,2,343,0,556,37,0,180,32,286,0,0,14,11,43,0,0,464,143,0,0,0,0,0,0,3,40,0,0,0,0,0,0,3088\n",
      "2786,20,100,37,0,845,453,2,286,16,70,1,0,13,102,26,1,5,0,14,0,0,36,176,0,0,12,0,295,0,252,118,147,0,0,194,27,0,3,0,0,46,0,0,0,0,0,3917\n",
      "586,5,63,63,3,975,146,13,65,178,40,148,546,2,126,0,0,163,124,1,0,9,51,0,0,80,0,1,171,0,0,233,4,0,0,8,4,0,0,0,0,0,7,0,0,0,0,6185\n",
      "816,1,489,140,4,1176,498,117,50,298,140,210,0,12,979,6,0,3,88,2,0,0,45,17,0,7,66,0,709,0,0,10,0,0,0,0,24,0,0,0,0,0,0,0,0,0,0,4093\n",
      "933,0,48,84,0,2237,724,18,336,0,47,0,0,563,148,141,49,7,0,347,23,240,115,0,0,0,0,351,49,0,3,64,671,0,0,0,6,0,0,2,0,0,7,0,0,0,0,2787\n",
      "447,150,98,1565,0,986,301,70,118,0,711,193,43,20,17,1,0,0,68,0,0,0,20,0,0,1,0,463,74,0,0,1,114,0,0,6,11,0,0,0,1,0,0,0,0,0,0,4521\n",
      "1398,0,34,110,99,1279,395,1,255,0,213,181,12,377,3,6,266,21,93,150,0,284,73,8,0,37,3,9,9,0,0,263,0,0,0,15,1,0,6,20,5,0,0,0,0,0,0,4374\n",
      "2109,0,129,946,0,1759,1,83,4,2,111,40,0,64,651,31,412,11,147,0,205,2,227,47,0,4,81,59,81,0,20,20,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,2752\n",
      "1111,0,528,468,1,981,145,3,1137,42,17,8,129,0,23,0,0,2,59,160,280,5,10,0,0,185,0,0,385,0,0,104,81,0,0,0,370,0,0,0,0,0,0,0,0,0,0,3766\n",
      "2097,97,58,647,0,579,52,112,944,0,19,457,147,47,471,21,257,0,9,209,3,142,180,0,0,0,4,1,255,0,0,21,16,0,0,0,470,0,0,0,0,0,0,0,0,0,0,2685\n",
      "1051,0,84,65,11,1438,180,85,615,0,201,24,57,566,69,0,18,272,73,53,168,106,161,0,0,21,0,0,982,0,0,152,8,0,0,0,240,0,0,0,1,0,0,0,0,0,0,3299\n",
      "597,0,317,2091,158,1529,279,33,507,2,1401,0,0,0,21,0,10,104,30,79,234,1,112,0,0,0,1,0,141,0,0,65,63,0,0,0,0,0,11,0,0,0,0,0,0,0,0,2214\n",
      "490,0,29,60,0,638,118,9,762,0,537,40,96,171,1,6,142,1,526,396,0,0,758,6,27,40,0,188,83,0,0,7,0,0,0,0,156,0,0,0,0,0,0,283,0,0,0,4430\n",
      "2445,0,81,680,415,939,123,375,397,3,0,53,96,0,0,2,285,22,41,132,16,0,5,0,5,0,0,7,258,0,0,25,52,0,0,19,103,0,0,0,0,0,0,0,0,0,0,3421\n",
      "890,114,208,367,475,1095,59,0,66,150,4,8,120,10,615,374,2,0,160,55,2,127,184,15,0,0,0,0,350,0,13,7,5,0,0,0,763,0,0,0,0,0,0,0,0,0,0,3762\n",
      "725,120,7,250,226,1035,552,441,4,51,21,1,52,0,280,0,0,4,11,15,0,9,5,0,0,0,131,0,167,384,50,13,53,0,0,0,180,0,0,254,0,0,1,0,0,0,0,4958\n",
      "545,2,55,210,0,420,348,0,300,10,804,23,0,0,4,1,76,150,77,5,768,0,412,2,76,4,570,0,556,0,0,17,62,0,0,2,59,0,0,0,416,0,0,0,0,0,0,4026\n",
      "1941,0,313,741,0,574,25,3,388,4,565,61,22,12,24,0,20,205,0,24,134,1190,154,1,5,14,106,239,10,0,0,27,59,0,0,453,0,0,104,0,0,0,0,0,0,0,0,2582\n",
      "922,2,18,140,24,2011,71,167,19,0,637,71,325,502,253,185,35,226,51,22,0,310,83,1,0,0,0,0,51,2,4,210,509,0,1,0,7,0,0,0,0,0,0,8,0,0,0,3133\n",
      "229,0,49,806,77,1393,337,41,391,5,3,45,4,0,14,0,49,15,58,94,21,0,92,0,0,0,75,0,251,0,12,208,0,0,0,0,88,0,0,0,0,0,181,0,0,0,0,5462\n",
      "1131,0,344,334,0,1528,45,23,182,37,272,0,0,1,0,1748,1,0,2,175,204,0,60,31,0,0,153,0,648,0,0,42,61,0,0,76,44,0,0,2,0,0,0,0,0,0,0,2856\n",
      "417,626,375,141,0,1513,78,0,174,460,478,37,112,0,69,95,69,0,140,1050,0,0,126,39,0,27,14,0,32,0,0,194,0,0,0,0,329,0,0,6,0,0,0,0,0,0,0,3399\n",
      "1195,81,66,103,23,739,39,0,570,7,1045,151,0,10,140,110,7,24,532,1,0,10,436,190,0,46,0,109,91,0,1,449,329,0,0,0,246,0,0,0,5,0,0,0,0,0,0,3245\n",
      "882,0,231,992,8,848,210,42,645,0,1,150,0,167,99,42,155,252,100,4,0,0,1,561,2,3,45,0,121,0,0,29,63,0,0,0,55,627,0,0,0,0,0,0,0,0,0,3665\n",
      "399,0,5,206,0,2685,133,0,617,105,15,479,90,34,5,0,314,26,5,39,4,558,230,0,0,37,0,0,465,0,2,100,14,0,0,0,85,2,1,370,0,0,9,0,2,0,0,2964\n",
      "1096,0,119,191,2,1807,47,17,365,0,2,2,46,91,11,36,34,1,999,230,0,27,228,0,0,2,0,188,828,0,56,110,5,0,0,0,136,0,0,96,52,0,2,0,0,0,0,3174\n",
      "974,479,1491,7,133,553,55,1,104,9,0,4,0,0,252,6,1,100,242,17,969,0,50,0,0,146,111,2,161,45,145,553,0,0,0,0,151,0,0,0,0,0,0,0,0,0,0,3239\n",
      "1207,77,287,759,215,1312,371,30,222,9,908,71,222,97,2,0,209,0,17,115,0,69,690,0,0,0,1,0,118,0,0,845,43,19,0,0,11,0,0,2,0,0,0,0,0,0,0,2072\n",
      "1453,0,2,323,1,1403,44,10,85,64,10,10,0,407,7,0,3,193,3,1,8,4,129,0,0,205,2,0,534,0,0,376,0,0,0,60,2,0,1,0,2,0,3,0,0,0,0,4655\n",
      "999,6,17,290,256,1555,64,18,542,0,45,1,36,40,1,115,659,635,361,4,1,0,323,405,0,4,0,82,9,0,0,15,4,0,0,0,9,0,0,9,0,3,0,0,0,0,0,3492\n",
      "330,0,88,222,9,1092,517,2,21,69,424,0,60,0,469,42,631,0,126,164,156,262,74,0,0,0,0,0,461,0,68,6,318,0,0,0,1,0,0,0,93,0,0,0,0,0,0,4295\n",
      "481,0,316,431,0,576,197,8,487,129,645,90,8,2,568,20,0,1,29,23,0,152,8,173,0,0,0,0,153,0,2,8,76,0,0,0,120,0,0,0,0,0,0,0,0,0,0,5297\n",
      "1574,329,464,1217,0,1010,91,191,63,0,165,448,0,0,2,4,1,0,12,218,0,0,9,568,0,0,0,0,582,0,120,8,0,0,0,0,24,0,0,0,0,0,0,0,0,0,0,2900\n",
      "568,217,460,918,0,794,219,0,273,0,126,786,0,0,1,16,84,7,31,34,137,0,70,1,0,6,0,29,9,0,10,700,54,0,0,0,5,0,0,0,0,0,0,0,0,73,1,4371\n",
      "347,2,363,207,2,2514,488,4,27,42,99,0,0,98,11,7,0,0,472,5,78,23,70,1,1,2,10,0,16,0,13,232,23,0,0,0,2,4,0,0,0,0,0,60,0,0,0,4777\n",
      "591,0,300,351,0,742,31,0,737,2,770,2,1,4,16,0,57,0,21,29,434,0,584,286,17,29,57,98,209,0,126,1,0,0,0,0,576,0,6,0,0,0,0,0,2,0,0,3921\n",
      "642,139,150,1043,77,1074,0,636,56,169,0,1,22,10,30,0,6,0,0,11,89,0,3,36,0,117,265,0,566,0,830,59,2,0,0,3,106,0,0,0,0,0,0,0,0,0,0,3858\n",
      "507,16,816,39,49,900,16,102,1457,67,6,150,23,194,75,0,6,0,3,272,261,41,83,0,0,11,0,35,330,30,0,535,104,0,0,0,317,0,0,0,354,8,0,0,1,0,0,3192\n",
      "857,0,366,581,0,955,29,224,362,0,8,454,33,361,143,4,117,0,1071,312,758,0,23,188,0,0,0,2,176,16,204,68,29,0,0,51,68,0,0,19,0,0,0,0,0,0,2,2519\n",
      "545,1,851,421,0,1743,310,0,42,2,2,559,0,233,0,11,224,349,53,371,1,35,489,0,37,62,0,0,285,0,0,54,2,0,0,13,29,0,0,0,0,0,10,0,0,0,0,3266\n",
      "1846,0,136,121,0,1580,44,77,301,0,46,0,2,12,4,294,135,190,33,242,35,0,33,0,0,0,49,1168,78,0,0,5,0,0,0,39,1,0,0,0,0,0,0,0,0,0,0,3529\n",
      "1027,4,997,1038,17,1472,573,85,85,101,0,150,109,23,246,78,28,0,183,1,1,0,207,103,0,0,98,40,270,0,13,595,185,0,0,0,103,128,0,0,0,0,0,0,0,3,0,2037\n",
      "1711,1,231,1923,0,337,0,22,718,49,3,9,336,0,24,35,11,215,576,0,121,0,108,0,0,0,221,0,48,0,57,23,0,0,0,0,77,0,0,0,0,0,0,0,0,0,0,3144\n",
      "1727,0,30,1315,0,416,252,0,110,9,360,0,0,13,0,81,89,0,833,140,0,1,11,0,0,1,19,234,18,0,0,125,1,0,0,0,15,0,0,0,1,0,0,0,0,0,0,4199\n",
      "1367,0,490,933,0,791,0,0,424,3,81,9,16,45,120,0,3,43,257,1,0,440,1,14,0,1,13,0,176,0,0,152,36,98,0,0,390,0,6,0,39,0,0,0,0,0,0,4051\n",
      "936,5,85,102,85,2309,908,6,385,160,15,0,0,0,5,0,9,0,1,225,652,1,0,195,0,1,107,1,1,0,0,157,115,0,0,0,16,0,0,5,294,0,0,0,0,0,0,3219\n",
      "1959,65,7,258,0,1479,139,6,437,204,20,12,5,0,16,8,124,94,392,12,12,0,10,0,0,29,2,3,230,0,0,214,8,0,0,3,0,0,0,0,0,0,0,0,0,0,0,4252\n",
      "1005,24,439,762,14,1262,302,78,23,8,48,7,26,0,20,162,846,6,146,2,0,91,132,1,0,20,1,4,122,0,0,65,4,0,0,0,142,0,0,1,1,0,360,0,0,0,4,3872\n",
      "316,11,777,128,0,733,125,274,154,0,28,245,19,0,1107,0,126,8,121,289,5,1,8,0,4,233,0,12,309,0,0,250,283,0,0,4,123,0,0,0,0,0,1,0,0,0,0,4306\n",
      "789,4,165,78,0,1833,45,0,843,119,197,16,3,51,52,3,37,374,165,1,2,42,34,0,45,0,23,16,245,0,0,11,59,0,172,41,376,0,0,0,0,11,0,163,0,0,0,3985\n",
      "2325,5,14,195,0,452,7,188,226,47,9,1268,1,333,6,1,0,174,782,59,367,52,1,0,0,197,0,0,65,0,0,0,1,0,0,0,47,0,0,0,7,0,0,0,0,6,4,3161\n",
      "1987,0,501,13,0,807,21,0,272,0,58,1,13,573,2,0,55,594,14,3,278,0,25,5,7,0,1,1,426,0,42,17,167,0,0,0,0,0,0,0,77,0,0,0,0,0,0,4040\n",
      "1832,627,5,826,0,2226,250,0,106,44,461,0,24,194,2,381,346,12,452,136,0,0,10,0,0,55,0,0,138,0,0,188,8,0,0,0,27,0,0,0,0,0,0,0,0,0,0,1650\n",
      "1011,3,73,1329,3,484,1851,2,49,0,189,1,8,30,51,3,28,19,22,0,0,19,21,3,0,0,15,3,166,1,2,11,0,0,0,3,747,0,0,0,0,0,0,0,0,0,0,3853\n",
      "617,0,0,38,141,1434,933,0,352,309,515,7,0,1,4,425,928,2,635,37,154,389,11,1,0,73,182,322,183,0,64,146,111,0,0,0,4,0,0,0,0,0,1,0,0,0,0,1981\n",
      "1823,0,713,364,113,218,2,0,24,4,46,12,1,104,20,3,193,1,50,43,21,0,11,0,1,7,1,0,125,0,0,0,91,0,0,0,234,0,0,21,0,0,0,0,0,0,0,5754\n",
      "1073,0,80,29,0,111,308,35,342,2,76,3,0,17,2,336,0,508,23,256,1,13,6,0,380,0,31,0,334,0,0,667,439,176,0,0,925,0,0,716,0,28,2,0,0,0,0,3081\n",
      "743,10,27,1566,0,1064,407,8,132,0,26,12,79,2,2,177,345,4,251,24,1,0,78,32,0,53,733,21,1,0,0,68,5,0,0,0,21,0,0,0,0,0,8,0,0,0,0,4100\n",
      "1499,49,14,727,0,1488,52,45,204,20,111,1,180,2,51,44,0,0,42,121,180,0,16,30,0,0,432,6,12,0,0,483,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,4188\n",
      "253,0,236,1202,0,1012,543,170,618,0,1148,2,0,1,91,56,0,4,21,1,29,0,159,0,0,95,2,1,22,0,0,0,12,0,0,10,43,0,0,0,2,0,2,0,0,0,0,4265\n",
      "1544,2,130,99,0,1290,343,334,338,137,54,15,16,47,523,45,115,19,11,184,3,15,81,0,0,0,0,0,292,0,5,36,25,0,0,6,106,0,0,78,0,0,0,0,0,0,0,4107\n",
      "955,0,17,345,0,1636,32,5,615,6,19,1,1,5,55,0,1,73,272,38,0,136,343,29,0,119,36,1,373,0,110,37,0,0,0,0,6,0,0,8,141,0,0,0,0,0,0,4585\n",
      "641,0,7,1007,0,1771,235,0,491,57,927,30,5,16,384,0,33,100,125,169,0,0,64,23,0,595,0,0,230,0,0,56,18,0,2,0,7,0,0,0,0,0,0,0,0,0,0,3007\n",
      "947,730,245,910,0,574,679,0,35,0,235,59,8,5,163,583,159,118,138,475,0,72,73,0,0,52,0,0,895,0,0,490,36,46,0,3,336,0,0,39,0,0,0,0,0,0,0,1895\n",
      "284,128,36,468,0,674,1482,40,17,0,45,10,98,2,3,104,66,0,160,205,0,33,1355,22,0,1,40,0,0,0,0,4,1,7,0,3,70,0,67,76,0,0,0,0,0,0,0,4499\n",
      "583,23,980,1188,1,1518,19,93,703,0,145,1,0,0,30,30,8,3,15,117,0,0,53,2,7,0,0,0,37,0,3,119,411,0,0,0,170,0,0,0,0,0,0,0,0,0,0,3741\n",
      "1157,1,121,230,2,351,3,141,26,40,25,58,29,181,70,897,59,621,287,0,0,0,240,231,555,11,3,9,508,0,78,5,79,2,0,0,170,0,10,0,0,0,9,0,0,0,0,3791\n",
      "1056,0,150,340,187,1508,3,2,154,67,1,1,1,3,149,0,25,0,223,362,8,18,37,163,0,1,5,0,46,0,126,770,1,0,0,0,189,0,0,3,0,0,0,0,0,0,0,4401\n",
      "2274,0,851,181,184,1875,9,4,347,0,94,8,0,0,9,43,10,1,0,47,3,1,308,0,0,21,0,3,29,0,0,25,18,0,0,0,9,0,0,0,2,0,3,0,0,0,0,3641\n",
      "623,0,14,386,26,794,495,12,701,360,433,6,7,2,611,4,4,45,61,18,29,0,1459,0,0,1,19,0,73,9,1065,95,6,0,0,89,346,0,0,0,0,0,0,0,7,0,0,2200\n",
      "184,0,28,842,0,836,132,56,597,0,95,31,11,16,195,21,0,75,0,4,252,0,13,28,0,3,10,0,97,0,8,5,98,0,0,0,175,0,0,0,0,13,253,8,0,0,0,5914\n",
      "848,0,477,585,12,655,0,19,1366,3,228,324,345,0,8,11,89,138,6,26,0,11,61,616,217,73,0,0,348,0,0,187,0,0,0,0,10,0,277,0,0,0,0,0,0,0,0,3060\n",
      "965,232,38,50,39,1440,113,303,531,29,26,0,0,75,20,19,0,0,1,6,0,0,7,1,0,0,31,0,209,0,0,75,0,0,0,0,432,0,0,0,0,0,61,0,1,0,0,5296\n",
      "1401,0,406,87,0,300,556,392,6,29,201,10,52,0,265,0,246,9,437,49,26,242,310,0,29,0,1,18,13,0,93,1181,13,0,0,180,92,269,0,0,0,0,0,32,0,0,0,3055\n",
      "695,0,621,257,0,1288,11,0,163,0,945,306,39,41,397,25,34,161,0,11,20,24,0,153,0,1,0,94,0,51,26,0,0,0,0,0,816,0,0,3,0,0,0,0,0,0,0,3818\n",
      "1265,0,696,192,212,722,8,305,12,111,43,291,3,796,29,0,20,12,326,306,0,17,1,0,0,4,2,371,225,0,3,13,11,0,0,0,6,0,0,0,22,0,0,0,0,0,0,3976\n",
      "687,0,631,164,0,830,400,82,1,0,110,19,84,218,68,7,8,131,121,4,11,34,125,31,3,0,1,3,705,0,45,24,5,0,0,0,261,0,0,0,0,0,0,0,0,0,0,5187\n",
      "2983,0,660,140,0,1742,277,36,6,99,49,113,76,45,70,0,1,127,142,1,0,0,1,1,0,74,49,130,154,0,0,13,85,0,0,0,134,0,1,2,1,0,0,0,0,0,0,2788\n",
      "505,16,139,265,5,1501,238,232,124,391,662,0,140,574,14,35,22,999,13,3,126,13,830,0,0,1,12,1,5,0,86,1,108,0,0,0,4,0,0,0,0,0,0,0,0,1,0,2934\n",
      "1204,0,2,989,0,827,362,1,118,241,488,172,76,0,2,0,2,8,197,0,44,8,68,6,0,0,0,6,239,0,20,61,1431,0,0,0,317,0,0,0,0,0,0,0,0,0,0,3111\n",
      "806,16,1439,69,55,436,200,13,117,219,335,1204,52,38,376,0,0,68,15,112,63,0,257,0,0,1,6,0,171,0,1,36,177,0,0,0,108,0,0,133,351,0,0,0,0,0,0,3126\n",
      "419,679,140,383,0,725,21,35,35,14,59,0,93,0,57,0,47,1,10,77,204,7,243,184,101,451,0,84,151,0,0,340,287,0,0,0,10,0,0,0,0,0,0,0,0,0,0,5143\n",
      "596,5,1691,320,1,1011,238,21,247,69,71,178,273,3,1,0,338,200,2,0,110,48,50,1,0,0,0,8,2,0,37,35,0,0,0,0,0,0,0,292,6,0,0,0,0,0,0,4146\n",
      "1341,0,1107,1457,43,876,212,136,133,1,6,21,3,38,26,3,216,0,30,280,0,0,107,12,4,126,8,0,482,0,0,365,2,0,0,0,15,0,0,0,0,0,0,0,0,0,0,2950\n",
      "613,1,284,293,0,1402,295,114,1214,0,23,49,0,13,1689,13,94,0,7,10,0,2,107,4,0,17,61,49,110,0,0,100,65,0,0,0,69,0,3,11,0,0,0,0,0,2,0,3286\n",
      "1818,1,333,394,1,945,465,0,38,100,1,7,36,527,32,0,361,2,2,3,120,139,110,5,8,2,0,0,544,0,493,425,0,0,0,5,101,0,0,5,219,0,0,0,0,0,0,2758\n",
      "1057,0,0,322,217,650,1,969,214,41,1006,0,2,0,7,0,12,1,0,166,0,0,17,7,0,19,5,1,95,2,1,44,0,0,0,30,0,0,0,0,2,0,0,0,0,0,0,5112\n",
      "422,0,250,79,0,1381,225,0,72,0,784,2,1,0,916,0,227,91,307,91,0,10,60,0,0,80,0,0,95,0,0,123,16,0,0,73,1183,0,0,1,0,124,0,0,0,6,0,3381\n",
      "1401,4,30,526,0,923,25,0,127,0,483,11,70,1089,1,3,174,11,4,2,49,0,81,297,0,4,18,0,100,0,35,8,1,0,0,0,46,0,0,0,7,0,0,0,0,0,0,4470\n",
      "2655,6,106,74,0,952,76,13,158,125,736,43,19,6,49,0,2,21,971,138,95,0,548,0,0,5,105,0,223,0,1,4,5,0,0,0,36,0,0,2,0,0,0,0,0,0,0,2826\n",
      "335,0,71,259,67,718,1,4,4,167,6,0,174,12,6,0,2,56,1,104,8,0,124,0,0,0,0,4,861,0,0,0,1,0,0,0,236,0,0,246,0,0,6,0,0,0,0,6527\n",
      "649,69,966,1227,0,508,2,30,550,0,302,159,3,49,195,26,19,180,7,49,2,0,324,32,0,0,0,0,5,0,0,147,87,0,0,0,5,0,0,0,0,0,0,6,0,0,0,4402\n",
      "1258,0,0,1119,0,2348,25,0,137,176,2,12,148,92,35,0,0,10,26,0,336,164,18,0,0,23,0,4,1318,0,102,1,49,0,0,0,10,0,0,2,0,0,0,0,0,0,0,2585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(resource_filename('deepbiome', 'tests/data/count/%s' % list_of_input_files.iloc[0,0])) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the Y (regression)\n",
    "\n",
    "This is an example of the output file for regression problem. One column contains y samples for one repeatition. \n",
    "For each repeatition (column) has outputs of 4 samples for each repeatition. Below example file has 1000 samples in row, `k=1000` repetition in column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x991</th>\n",
       "      <th>x992</th>\n",
       "      <th>x993</th>\n",
       "      <th>x994</th>\n",
       "      <th>x995</th>\n",
       "      <th>x996</th>\n",
       "      <th>x997</th>\n",
       "      <th>x998</th>\n",
       "      <th>x999</th>\n",
       "      <th>x1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.997270</td>\n",
       "      <td>5.492354</td>\n",
       "      <td>5.473725</td>\n",
       "      <td>1.759484</td>\n",
       "      <td>5.313252</td>\n",
       "      <td>1.500044</td>\n",
       "      <td>4.949712</td>\n",
       "      <td>5.493533</td>\n",
       "      <td>3.743509</td>\n",
       "      <td>5.492373</td>\n",
       "      <td>...</td>\n",
       "      <td>2.793883</td>\n",
       "      <td>1.500004</td>\n",
       "      <td>5.487526</td>\n",
       "      <td>5.493518</td>\n",
       "      <td>3.599047</td>\n",
       "      <td>5.491461</td>\n",
       "      <td>5.486244</td>\n",
       "      <td>5.487390</td>\n",
       "      <td>5.493492</td>\n",
       "      <td>3.762523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.004092</td>\n",
       "      <td>1.500002</td>\n",
       "      <td>4.640348</td>\n",
       "      <td>1.538071</td>\n",
       "      <td>5.491065</td>\n",
       "      <td>5.481009</td>\n",
       "      <td>5.492323</td>\n",
       "      <td>2.968531</td>\n",
       "      <td>3.576358</td>\n",
       "      <td>5.491456</td>\n",
       "      <td>...</td>\n",
       "      <td>1.500033</td>\n",
       "      <td>3.369529</td>\n",
       "      <td>1.500016</td>\n",
       "      <td>3.103297</td>\n",
       "      <td>5.493214</td>\n",
       "      <td>3.831125</td>\n",
       "      <td>5.492104</td>\n",
       "      <td>5.474811</td>\n",
       "      <td>5.492416</td>\n",
       "      <td>3.268805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.485126</td>\n",
       "      <td>4.187426</td>\n",
       "      <td>5.491340</td>\n",
       "      <td>5.469662</td>\n",
       "      <td>5.490478</td>\n",
       "      <td>1.953375</td>\n",
       "      <td>5.494656</td>\n",
       "      <td>3.741680</td>\n",
       "      <td>4.862400</td>\n",
       "      <td>5.490701</td>\n",
       "      <td>...</td>\n",
       "      <td>5.491728</td>\n",
       "      <td>2.459981</td>\n",
       "      <td>5.475697</td>\n",
       "      <td>3.114158</td>\n",
       "      <td>1.500004</td>\n",
       "      <td>1.500019</td>\n",
       "      <td>4.113815</td>\n",
       "      <td>5.470539</td>\n",
       "      <td>5.494373</td>\n",
       "      <td>5.481754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.489590</td>\n",
       "      <td>4.863187</td>\n",
       "      <td>1.500003</td>\n",
       "      <td>5.484699</td>\n",
       "      <td>5.492657</td>\n",
       "      <td>5.491270</td>\n",
       "      <td>4.091023</td>\n",
       "      <td>5.495239</td>\n",
       "      <td>5.492804</td>\n",
       "      <td>1.500046</td>\n",
       "      <td>...</td>\n",
       "      <td>1.500034</td>\n",
       "      <td>1.500012</td>\n",
       "      <td>5.483070</td>\n",
       "      <td>2.475049</td>\n",
       "      <td>5.493846</td>\n",
       "      <td>3.287076</td>\n",
       "      <td>3.696412</td>\n",
       "      <td>5.487583</td>\n",
       "      <td>1.500044</td>\n",
       "      <td>2.760404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.500001</td>\n",
       "      <td>5.480769</td>\n",
       "      <td>5.489725</td>\n",
       "      <td>1.500044</td>\n",
       "      <td>2.695212</td>\n",
       "      <td>5.492262</td>\n",
       "      <td>3.381424</td>\n",
       "      <td>4.805420</td>\n",
       "      <td>1.500047</td>\n",
       "      <td>5.474376</td>\n",
       "      <td>...</td>\n",
       "      <td>1.500046</td>\n",
       "      <td>2.586990</td>\n",
       "      <td>5.440610</td>\n",
       "      <td>4.376103</td>\n",
       "      <td>1.500030</td>\n",
       "      <td>4.713223</td>\n",
       "      <td>5.491059</td>\n",
       "      <td>3.230658</td>\n",
       "      <td>1.500045</td>\n",
       "      <td>5.488727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0  4.997270  5.492354  5.473725  1.759484  5.313252  1.500044  4.949712   \n",
       "1  5.004092  1.500002  4.640348  1.538071  5.491065  5.481009  5.492323   \n",
       "2  5.485126  4.187426  5.491340  5.469662  5.490478  1.953375  5.494656   \n",
       "3  5.489590  4.863187  1.500003  5.484699  5.492657  5.491270  4.091023   \n",
       "4  1.500001  5.480769  5.489725  1.500044  2.695212  5.492262  3.381424   \n",
       "\n",
       "         x8        x9       x10  ...      x991      x992      x993      x994  \\\n",
       "0  5.493533  3.743509  5.492373  ...  2.793883  1.500004  5.487526  5.493518   \n",
       "1  2.968531  3.576358  5.491456  ...  1.500033  3.369529  1.500016  3.103297   \n",
       "2  3.741680  4.862400  5.490701  ...  5.491728  2.459981  5.475697  3.114158   \n",
       "3  5.495239  5.492804  1.500046  ...  1.500034  1.500012  5.483070  2.475049   \n",
       "4  4.805420  1.500047  5.474376  ...  1.500046  2.586990  5.440610  4.376103   \n",
       "\n",
       "       x995      x996      x997      x998      x999     x1000  \n",
       "0  3.599047  5.491461  5.486244  5.487390  5.493492  3.762523  \n",
       "1  5.493214  3.831125  5.492104  5.474811  5.492416  3.268805  \n",
       "2  1.500004  1.500019  4.113815  5.470539  5.494373  5.481754  \n",
       "3  5.493846  3.287076  3.696412  5.487583  1.500044  2.760404  \n",
       "4  1.500030  4.713223  5.491059  3.230658  1.500045  5.488727  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(resource_filename('deepbiome', 'tests/data/regression_y.csv'))\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x991</th>\n",
       "      <th>x992</th>\n",
       "      <th>x993</th>\n",
       "      <th>x994</th>\n",
       "      <th>x995</th>\n",
       "      <th>x996</th>\n",
       "      <th>x997</th>\n",
       "      <th>x998</th>\n",
       "      <th>x999</th>\n",
       "      <th>x1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2.609926</td>\n",
       "      <td>5.491258</td>\n",
       "      <td>3.318610</td>\n",
       "      <td>5.444070</td>\n",
       "      <td>2.884154</td>\n",
       "      <td>5.486857</td>\n",
       "      <td>5.496554</td>\n",
       "      <td>1.500019</td>\n",
       "      <td>5.482893</td>\n",
       "      <td>1.824835</td>\n",
       "      <td>...</td>\n",
       "      <td>4.478641</td>\n",
       "      <td>5.485122</td>\n",
       "      <td>4.915985</td>\n",
       "      <td>4.073239</td>\n",
       "      <td>1.500019</td>\n",
       "      <td>5.492295</td>\n",
       "      <td>1.500005</td>\n",
       "      <td>1.559586</td>\n",
       "      <td>5.496415</td>\n",
       "      <td>4.171127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>5.488959</td>\n",
       "      <td>3.739806</td>\n",
       "      <td>5.489474</td>\n",
       "      <td>1.500021</td>\n",
       "      <td>5.492632</td>\n",
       "      <td>1.500019</td>\n",
       "      <td>5.484813</td>\n",
       "      <td>5.467055</td>\n",
       "      <td>5.491282</td>\n",
       "      <td>1.874777</td>\n",
       "      <td>...</td>\n",
       "      <td>5.498820</td>\n",
       "      <td>5.493926</td>\n",
       "      <td>5.487404</td>\n",
       "      <td>3.162812</td>\n",
       "      <td>1.846298</td>\n",
       "      <td>5.492417</td>\n",
       "      <td>1.919107</td>\n",
       "      <td>5.480324</td>\n",
       "      <td>5.467765</td>\n",
       "      <td>5.457627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>3.498418</td>\n",
       "      <td>4.250451</td>\n",
       "      <td>5.488116</td>\n",
       "      <td>4.162031</td>\n",
       "      <td>5.494052</td>\n",
       "      <td>5.472900</td>\n",
       "      <td>1.500057</td>\n",
       "      <td>5.491497</td>\n",
       "      <td>5.491935</td>\n",
       "      <td>1.500033</td>\n",
       "      <td>...</td>\n",
       "      <td>1.966474</td>\n",
       "      <td>5.475258</td>\n",
       "      <td>3.848034</td>\n",
       "      <td>2.863883</td>\n",
       "      <td>4.370685</td>\n",
       "      <td>5.494647</td>\n",
       "      <td>5.478855</td>\n",
       "      <td>2.465739</td>\n",
       "      <td>1.500018</td>\n",
       "      <td>5.486403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>5.486107</td>\n",
       "      <td>1.917414</td>\n",
       "      <td>5.414975</td>\n",
       "      <td>5.492364</td>\n",
       "      <td>2.027914</td>\n",
       "      <td>5.491349</td>\n",
       "      <td>5.494135</td>\n",
       "      <td>5.491245</td>\n",
       "      <td>1.500039</td>\n",
       "      <td>1.500019</td>\n",
       "      <td>...</td>\n",
       "      <td>4.556995</td>\n",
       "      <td>5.457072</td>\n",
       "      <td>2.071106</td>\n",
       "      <td>5.417333</td>\n",
       "      <td>5.491818</td>\n",
       "      <td>5.473390</td>\n",
       "      <td>4.374154</td>\n",
       "      <td>5.489109</td>\n",
       "      <td>4.515340</td>\n",
       "      <td>1.500020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>5.319623</td>\n",
       "      <td>5.482776</td>\n",
       "      <td>1.500035</td>\n",
       "      <td>5.485141</td>\n",
       "      <td>5.491019</td>\n",
       "      <td>3.733982</td>\n",
       "      <td>5.494374</td>\n",
       "      <td>3.077159</td>\n",
       "      <td>5.493188</td>\n",
       "      <td>1.500001</td>\n",
       "      <td>...</td>\n",
       "      <td>5.485356</td>\n",
       "      <td>1.500059</td>\n",
       "      <td>5.400762</td>\n",
       "      <td>5.489606</td>\n",
       "      <td>5.494583</td>\n",
       "      <td>5.490943</td>\n",
       "      <td>5.123794</td>\n",
       "      <td>5.473465</td>\n",
       "      <td>3.274979</td>\n",
       "      <td>3.700653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2        x3        x4        x5        x6        x7  \\\n",
       "995  2.609926  5.491258  3.318610  5.444070  2.884154  5.486857  5.496554   \n",
       "996  5.488959  3.739806  5.489474  1.500021  5.492632  1.500019  5.484813   \n",
       "997  3.498418  4.250451  5.488116  4.162031  5.494052  5.472900  1.500057   \n",
       "998  5.486107  1.917414  5.414975  5.492364  2.027914  5.491349  5.494135   \n",
       "999  5.319623  5.482776  1.500035  5.485141  5.491019  3.733982  5.494374   \n",
       "\n",
       "           x8        x9       x10  ...      x991      x992      x993  \\\n",
       "995  1.500019  5.482893  1.824835  ...  4.478641  5.485122  4.915985   \n",
       "996  5.467055  5.491282  1.874777  ...  5.498820  5.493926  5.487404   \n",
       "997  5.491497  5.491935  1.500033  ...  1.966474  5.475258  3.848034   \n",
       "998  5.491245  1.500039  1.500019  ...  4.556995  5.457072  2.071106   \n",
       "999  3.077159  5.493188  1.500001  ...  5.485356  1.500059  5.400762   \n",
       "\n",
       "         x994      x995      x996      x997      x998      x999     x1000  \n",
       "995  4.073239  1.500019  5.492295  1.500005  1.559586  5.496415  4.171127  \n",
       "996  3.162812  1.846298  5.492417  1.919107  5.480324  5.467765  5.457627  \n",
       "997  2.863883  4.370685  5.494647  5.478855  2.465739  1.500018  5.486403  \n",
       "998  5.417333  5.491818  5.473390  4.374154  5.489109  4.515340  1.500020  \n",
       "999  5.489606  5.494583  5.490943  5.123794  5.473465  3.274979  3.700653  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one repeatition, the deepbiome will use the one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4.997270\n",
       "1    5.004092\n",
       "2    5.485126\n",
       "3    5.489590\n",
       "4    1.500001\n",
       "Name: x1, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[:,0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "995    2.609926\n",
       "996    5.488959\n",
       "997    3.498418\n",
       "998    5.486107\n",
       "999    5.319623\n",
       "Name: x1, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[:,0].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the Y (classification)\n",
    "\n",
    "This is an example of the output file for classification problem. Below example file has 1000 samples in rows, 1000 repetitions in columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V991</th>\n",
       "      <th>V992</th>\n",
       "      <th>V993</th>\n",
       "      <th>V994</th>\n",
       "      <th>V995</th>\n",
       "      <th>V996</th>\n",
       "      <th>V997</th>\n",
       "      <th>V998</th>\n",
       "      <th>V999</th>\n",
       "      <th>V1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    V1   V2   V3   V4   V5   V6   V7   V8   V9  V10  ...  V991  V992  V993  \\\n",
       "0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  1.0  0.0  ...   1.0   1.0   0.0   \n",
       "1  1.0  1.0  1.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  ...   1.0   1.0   1.0   \n",
       "2  0.0  1.0  0.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  ...   0.0   1.0   1.0   \n",
       "3  0.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  ...   1.0   1.0   0.0   \n",
       "4  1.0  1.0  0.0  1.0  1.0  0.0  1.0  1.0  1.0  1.0  ...   1.0   1.0   1.0   \n",
       "\n",
       "   V994  V995  V996  V997  V998  V999  V1000  \n",
       "0   0.0   1.0   0.0   0.0   0.0   0.0    1.0  \n",
       "1   1.0   0.0   1.0   0.0   1.0   0.0    1.0  \n",
       "2   1.0   1.0   1.0   1.0   1.0   0.0    0.0  \n",
       "3   1.0   0.0   1.0   1.0   0.0   1.0    1.0  \n",
       "4   1.0   1.0   1.0   0.0   1.0   1.0    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(resource_filename('deepbiome', 'tests/data/classification_y.csv'))\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V991</th>\n",
       "      <th>V992</th>\n",
       "      <th>V993</th>\n",
       "      <th>V994</th>\n",
       "      <th>V995</th>\n",
       "      <th>V996</th>\n",
       "      <th>V997</th>\n",
       "      <th>V998</th>\n",
       "      <th>V999</th>\n",
       "      <th>V1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      V1   V2   V3   V4   V5   V6   V7   V8   V9  V10  ...  V991  V992  V993  \\\n",
       "995  1.0  0.0  1.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  ...   1.0   1.0   1.0   \n",
       "996  0.0  1.0  0.0  1.0  0.0  1.0  1.0  1.0  0.0  1.0  ...   0.0   0.0   0.0   \n",
       "997  1.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  0.0  1.0  ...   1.0   1.0   1.0   \n",
       "998  0.0  1.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  ...   1.0   1.0   1.0   \n",
       "999  1.0  1.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0  1.0  ...   0.0   1.0   1.0   \n",
       "\n",
       "     V994  V995  V996  V997  V998  V999  V1000  \n",
       "995   1.0   1.0   0.0   1.0   1.0   0.0    1.0  \n",
       "996   1.0   1.0   0.0   1.0   0.0   1.0    1.0  \n",
       "997   1.0   1.0   0.0   1.0   1.0   1.0    0.0  \n",
       "998   1.0   0.0   1.0   1.0   0.0   1.0    1.0  \n",
       "999   0.0   0.0   0.0   1.0   1.0   1.0    1.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one repeatition, the deepbiome will use the one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "2    0.0\n",
       "3    0.0\n",
       "4    1.0\n",
       "Name: V1, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[:,0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "995    1.0\n",
       "996    0.0\n",
       "997    1.0\n",
       "998    0.0\n",
       "999    1.0\n",
       "Name: V1, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[:,0].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exmple of the training index file for repetition\n",
    "\n",
    "For each repeatition, we have to set the training and test set. If the index file is given, the deepbiome library set the training set and test set based on the index file. Below is the example of the index file. Each column has the training indexs for each repeatition. The deepbiome will only use the samples in this index set for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V991</th>\n",
       "      <th>V992</th>\n",
       "      <th>V993</th>\n",
       "      <th>V994</th>\n",
       "      <th>V995</th>\n",
       "      <th>V996</th>\n",
       "      <th>V997</th>\n",
       "      <th>V998</th>\n",
       "      <th>V999</th>\n",
       "      <th>V1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>490</td>\n",
       "      <td>690</td>\n",
       "      <td>62</td>\n",
       "      <td>703</td>\n",
       "      <td>690</td>\n",
       "      <td>845</td>\n",
       "      <td>150</td>\n",
       "      <td>268</td>\n",
       "      <td>488</td>\n",
       "      <td>179</td>\n",
       "      <td>...</td>\n",
       "      <td>675</td>\n",
       "      <td>886</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>781</td>\n",
       "      <td>778</td>\n",
       "      <td>603</td>\n",
       "      <td>222</td>\n",
       "      <td>254</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>498</td>\n",
       "      <td>968</td>\n",
       "      <td>123</td>\n",
       "      <td>913</td>\n",
       "      <td>348</td>\n",
       "      <td>262</td>\n",
       "      <td>705</td>\n",
       "      <td>239</td>\n",
       "      <td>632</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>636</td>\n",
       "      <td>216</td>\n",
       "      <td>495</td>\n",
       "      <td>557</td>\n",
       "      <td>196</td>\n",
       "      <td>516</td>\n",
       "      <td>23</td>\n",
       "      <td>351</td>\n",
       "      <td>472</td>\n",
       "      <td>945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>389</td>\n",
       "      <td>999</td>\n",
       "      <td>335</td>\n",
       "      <td>947</td>\n",
       "      <td>215</td>\n",
       "      <td>696</td>\n",
       "      <td>793</td>\n",
       "      <td>349</td>\n",
       "      <td>734</td>\n",
       "      <td>624</td>\n",
       "      <td>...</td>\n",
       "      <td>626</td>\n",
       "      <td>230</td>\n",
       "      <td>26</td>\n",
       "      <td>330</td>\n",
       "      <td>470</td>\n",
       "      <td>992</td>\n",
       "      <td>329</td>\n",
       "      <td>532</td>\n",
       "      <td>655</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>139</td>\n",
       "      <td>843</td>\n",
       "      <td>491</td>\n",
       "      <td>47</td>\n",
       "      <td>421</td>\n",
       "      <td>892</td>\n",
       "      <td>32</td>\n",
       "      <td>438</td>\n",
       "      <td>996</td>\n",
       "      <td>...</td>\n",
       "      <td>956</td>\n",
       "      <td>706</td>\n",
       "      <td>836</td>\n",
       "      <td>151</td>\n",
       "      <td>80</td>\n",
       "      <td>409</td>\n",
       "      <td>671</td>\n",
       "      <td>772</td>\n",
       "      <td>882</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>592</td>\n",
       "      <td>83</td>\n",
       "      <td>204</td>\n",
       "      <td>810</td>\n",
       "      <td>198</td>\n",
       "      <td>955</td>\n",
       "      <td>357</td>\n",
       "      <td>125</td>\n",
       "      <td>190</td>\n",
       "      <td>162</td>\n",
       "      <td>...</td>\n",
       "      <td>542</td>\n",
       "      <td>108</td>\n",
       "      <td>959</td>\n",
       "      <td>311</td>\n",
       "      <td>771</td>\n",
       "      <td>902</td>\n",
       "      <td>986</td>\n",
       "      <td>481</td>\n",
       "      <td>922</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    V1   V2   V3   V4   V5   V6   V7   V8   V9  V10  ...  V991  V992  V993  \\\n",
       "0  490  690   62  703  690  845  150  268  488  179  ...   675   886   225   \n",
       "1  498  968  123  913  348  262  705  239  632   44  ...   636   216   495   \n",
       "2  389  999  335  947  215  696  793  349  734  624  ...   626   230    26   \n",
       "3   51  139  843  491   47  421  892   32  438  996  ...   956   706   836   \n",
       "4  592   83  204  810  198  955  357  125  190  162  ...   542   108   959   \n",
       "\n",
       "   V994  V995  V996  V997  V998  V999  V1000  \n",
       "0   222   781   778   603   222   254    407  \n",
       "1   557   196   516    23   351   472    945  \n",
       "2   330   470   992   329   532   655    426  \n",
       "3   151    80   409   671   772   882    181  \n",
       "4   311   771   902   986   481   922    305  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = pd.read_csv(resource_filename('deepbiome', 'tests/data/regression_idx.csv'), dtype=np.int)\n",
    "idxs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V991</th>\n",
       "      <th>V992</th>\n",
       "      <th>V993</th>\n",
       "      <th>V994</th>\n",
       "      <th>V995</th>\n",
       "      <th>V996</th>\n",
       "      <th>V997</th>\n",
       "      <th>V998</th>\n",
       "      <th>V999</th>\n",
       "      <th>V1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>599</td>\n",
       "      <td>824</td>\n",
       "      <td>997</td>\n",
       "      <td>216</td>\n",
       "      <td>586</td>\n",
       "      <td>796</td>\n",
       "      <td>806</td>\n",
       "      <td>39</td>\n",
       "      <td>483</td>\n",
       "      <td>518</td>\n",
       "      <td>...</td>\n",
       "      <td>573</td>\n",
       "      <td>861</td>\n",
       "      <td>366</td>\n",
       "      <td>374</td>\n",
       "      <td>585</td>\n",
       "      <td>871</td>\n",
       "      <td>140</td>\n",
       "      <td>597</td>\n",
       "      <td>795</td>\n",
       "      <td>743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>720</td>\n",
       "      <td>633</td>\n",
       "      <td>821</td>\n",
       "      <td>149</td>\n",
       "      <td>339</td>\n",
       "      <td>461</td>\n",
       "      <td>750</td>\n",
       "      <td>194</td>\n",
       "      <td>769</td>\n",
       "      <td>699</td>\n",
       "      <td>...</td>\n",
       "      <td>913</td>\n",
       "      <td>570</td>\n",
       "      <td>670</td>\n",
       "      <td>249</td>\n",
       "      <td>840</td>\n",
       "      <td>889</td>\n",
       "      <td>242</td>\n",
       "      <td>959</td>\n",
       "      <td>791</td>\n",
       "      <td>954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>80</td>\n",
       "      <td>268</td>\n",
       "      <td>661</td>\n",
       "      <td>187</td>\n",
       "      <td>929</td>\n",
       "      <td>469</td>\n",
       "      <td>481</td>\n",
       "      <td>332</td>\n",
       "      <td>781</td>\n",
       "      <td>615</td>\n",
       "      <td>...</td>\n",
       "      <td>985</td>\n",
       "      <td>459</td>\n",
       "      <td>965</td>\n",
       "      <td>888</td>\n",
       "      <td>461</td>\n",
       "      <td>551</td>\n",
       "      <td>465</td>\n",
       "      <td>827</td>\n",
       "      <td>557</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>570</td>\n",
       "      <td>32</td>\n",
       "      <td>750</td>\n",
       "      <td>332</td>\n",
       "      <td>902</td>\n",
       "      <td>107</td>\n",
       "      <td>281</td>\n",
       "      <td>667</td>\n",
       "      <td>917</td>\n",
       "      <td>793</td>\n",
       "      <td>...</td>\n",
       "      <td>924</td>\n",
       "      <td>662</td>\n",
       "      <td>975</td>\n",
       "      <td>199</td>\n",
       "      <td>32</td>\n",
       "      <td>715</td>\n",
       "      <td>668</td>\n",
       "      <td>241</td>\n",
       "      <td>299</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>440</td>\n",
       "      <td>589</td>\n",
       "      <td>607</td>\n",
       "      <td>597</td>\n",
       "      <td>380</td>\n",
       "      <td>961</td>\n",
       "      <td>747</td>\n",
       "      <td>396</td>\n",
       "      <td>649</td>\n",
       "      <td>974</td>\n",
       "      <td>...</td>\n",
       "      <td>867</td>\n",
       "      <td>839</td>\n",
       "      <td>234</td>\n",
       "      <td>99</td>\n",
       "      <td>901</td>\n",
       "      <td>19</td>\n",
       "      <td>821</td>\n",
       "      <td>450</td>\n",
       "      <td>780</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      V1   V2   V3   V4   V5   V6   V7   V8   V9  V10  ...  V991  V992  V993  \\\n",
       "745  599  824  997  216  586  796  806   39  483  518  ...   573   861   366   \n",
       "746  720  633  821  149  339  461  750  194  769  699  ...   913   570   670   \n",
       "747   80  268  661  187  929  469  481  332  781  615  ...   985   459   965   \n",
       "748  570   32  750  332  902  107  281  667  917  793  ...   924   662   975   \n",
       "749  440  589  607  597  380  961  747  396  649  974  ...   867   839   234   \n",
       "\n",
       "     V994  V995  V996  V997  V998  V999  V1000  \n",
       "745   374   585   871   140   597   795    743  \n",
       "746   249   840   889   242   959   791    954  \n",
       "747   888   461   551   465   827   557    662  \n",
       "748   199    32   715   668   241   299    518  \n",
       "749    99   901    19   821   450   780    326  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the index set for 1st repetition. From 1000 samples above, it uses 750 samples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    490\n",
       "1    498\n",
       "2    389\n",
       "3     51\n",
       "4    592\n",
       "Name: V1, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs.iloc[:,0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "745    599\n",
       "746    720\n",
       "747     80\n",
       "748    570\n",
       "749    440\n",
       "Name: V1, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs.iloc[:,0].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the configuration\n",
    "\n",
    "For detailed configuration, we used python dictionary as inputs for the main training function.\n",
    "You can build the configuration information for the network training by:\n",
    "1. the python dictionary format\n",
    "1. the configufation file (.cfg).\n",
    "\n",
    "In this notebook, we showed the dictionary python dictionary format configuration.\n",
    "\n",
    "Please check the detailed information about each options in the [documantation](https://young-won.github.io/deepbiome/prerequisites.html#configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For preparing the configuration about the network information (`network_info`)\n",
    "\n",
    "For giving the information about the training hyper-parameter, you have to provide the dictionary for configuration to the `netowrk_info` field.\n",
    "Your configuration for the network training should include the information about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_info = {\n",
    "    'architecture_info': {\n",
    "        'batch_normalization': 'False',\n",
    "        'drop_out': '0',\n",
    "        'weight_initial': 'glorot_uniform',\n",
    "        'weight_l1_penalty':'0.01',\n",
    "        'weight_decay': 'phylogenetic_tree',\n",
    "    },\n",
    "    'model_info': {\n",
    "        'lr': '0.01',\n",
    "        'decay': '0.001',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': 'binary_accuracy, sensitivity, specificity, gmeasure, auc',\n",
    "        'taxa_selection_metrics': 'accuracy, sensitivity, specificity, gmeasure',\n",
    "        'network_class': 'DeepBiomeNetwork',\n",
    "        'optimizer': 'adam',\n",
    "        'reader_class': 'MicroBiomeClassificationReader',\n",
    "        'normalizer': 'normalize_minmax',\n",
    "    },\n",
    "    'training_info': {\n",
    "        'batch_size': '50', \n",
    "        'epochs': '100'\n",
    "    },\n",
    "    'validation_info': {\n",
    "        'batch_size': 'None', \n",
    "        'validation_size': '0.2'\n",
    "    },\n",
    "    'test_info': {\n",
    "        'batch_size': 'None'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For preparing the configuration about the path information (`path_info`)\n",
    "\n",
    "To give the information about the path of dataset, paths for saving the trained weights and the evaluation results, we provide the dictionary for configuration to the `path_info` feild.\n",
    "Your configuration for the path information should include the information about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_info = {\n",
    "    'data_info': {\n",
    "        'count_list_path': resource_filename('deepbiome', 'tests/data/gcount_list.csv'),\n",
    "        'count_path': resource_filename('deepbiome', 'tests/data/count'),\n",
    "        'data_path': resource_filename('deepbiome', 'tests/data'),\n",
    "        'idx_path': resource_filename('deepbiome', 'tests/data/classification_idx.csv'),\n",
    "        'tree_info_path': resource_filename('deepbiome', 'tests/data/genus48_dic.csv'),\n",
    "        'x_path': '',\n",
    "        'y_path': 'classification_y.csv'\n",
    "    },\n",
    "    'model_info': {\n",
    "        'evaluation': 'eval.npy',\n",
    "        'history': 'hist.json',\n",
    "        'model_dir': './example_result/',\n",
    "        'weight': 'weight.h5'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DeepBiome Training\n",
    "\n",
    "Now we can train the DeepBiome network based on the configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logging, we use the python logging library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '[%(name)-8s|%(levelname)s|%(filename)s:%(lineno)s] %(message)s',\n",
    "                    level=logging.DEBUG)\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deeobiome_train function provide the test evaluation, train evaluation and the deepbiome network instance.\n",
    "\n",
    "If we set `number_of_fold`, then the deepbiome package do the cross-validation based on that value. If not, the deepbiome package do the cross-validation based on the index file. If both `number_of_fold` option and the index file is not given, then the library do leave-one-out-cross-validation (LOOCV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|deepbiome.py:100] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:137] -------1 simulation start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:148] Build network for 1 simulation\n",
      "[root    |INFO|build_network.py:505] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:506] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:510] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:511] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:518]      Genus: 48\n",
      "[root    |INFO|build_network.py:518]     Family: 40\n",
      "[root    |INFO|build_network.py:518]      Order: 23\n",
      "[root    |INFO|build_network.py:518]      Class: 17\n",
      "[root    |INFO|build_network.py:518]     Phylum: 9\n",
      "[root    |INFO|build_network.py:521] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:522] Phylogenetic_tree_dict info: ['Family', 'Class', 'Phylum', 'Number', 'Genus', 'Order']\n",
      "[root    |INFO|build_network.py:523] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:546] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:562] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:563] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:564] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:432: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[tensorflow|WARNING|deprecation.py:328] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:432: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "[root    |INFO|build_network.py:640] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:158] 1 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:133] Training start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:2862: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[tensorflow|WARNING|deprecation.py:328] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:2862: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.6761 - binary_accuracy: 0.6600 - sensitivity: 0.9518 - specificity: 0.0347 - gmeasure: 0.0349 - auc: 0.5224 - val_loss: 0.6473 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.4318\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.6361 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5250 - val_loss: 0.6131 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.4403\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.6213 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5371 - val_loss: 0.6109 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.4506\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.6215 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5688 - val_loss: 0.6110 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.4795\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.6210 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5964 - val_loss: 0.6112 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.4949\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6190 - val_loss: 0.6110 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5249\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.6209 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6580 - val_loss: 0.6114 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5677\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.6213 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6925 - val_loss: 0.6107 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6067\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.6203 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7344 - val_loss: 0.6113 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6641\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 0s 198us/step - loss: 0.6203 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7736 - val_loss: 0.6108 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7101\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.6214 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8026 - val_loss: 0.6116 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7391\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.6198 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8225 - val_loss: 0.6098 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7653\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 0s 218us/step - loss: 0.6192 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8422 - val_loss: 0.6093 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7933\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.6215 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8549 - val_loss: 0.6104 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8120\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.6169 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8629 - val_loss: 0.6079 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8192\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 0s 189us/step - loss: 0.6164 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8688 - val_loss: 0.6067 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8242\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 0s 190us/step - loss: 0.6145 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8702 - val_loss: 0.6052 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8293\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.6125 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8797 - val_loss: 0.6027 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8326\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.6093 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8894 - val_loss: 0.5995 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8381\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.6038 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8859 - val_loss: 0.5964 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8457\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.5985 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8850 - val_loss: 0.5892 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8447\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.5929 - binary_accuracy: 0.6857 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.88 - 0s 207us/step - loss: 0.5900 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8836 - val_loss: 0.5812 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8453\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.5794 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8837 - val_loss: 0.5706 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8436\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 208us/step - loss: 0.5652 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8823 - val_loss: 0.5572 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8429\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.5462 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8857 - val_loss: 0.5406 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8426\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.5266 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8922 - val_loss: 0.5244 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8438\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.5036 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8905 - val_loss: 0.5049 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8453\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.4887 - binary_accuracy: 0.7100 - sensitivity: 0.9859 - specificity: 0.1033 - gmeasure: 0.1559 - auc: 0.8893 - val_loss: 0.4882 - val_binary_accuracy: 0.7400 - val_sensitivity: 0.9396 - val_specificity: 0.2623 - val_gmeasure: 0.4950 - val_auc: 0.8508\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.4691 - binary_accuracy: 0.7633 - sensitivity: 0.9606 - specificity: 0.3336 - gmeasure: 0.5539 - auc: 0.8904 - val_loss: 0.4770 - val_binary_accuracy: 0.7400 - val_sensitivity: 0.9396 - val_specificity: 0.2623 - val_gmeasure: 0.4950 - val_auc: 0.8483\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.4610 - binary_accuracy: 0.7850 - sensitivity: 0.9526 - specificity: 0.4339 - gmeasure: 0.6240 - auc: 0.8925 - val_loss: 0.4696 - val_binary_accuracy: 0.7467 - val_sensitivity: 0.9396 - val_specificity: 0.2926 - val_gmeasure: 0.5216 - val_auc: 0.8549\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.4403 - binary_accuracy: 0.8033 - sensitivity: 0.9497 - specificity: 0.4746 - gmeasure: 0.6605 - auc: 0.8949 - val_loss: 0.4498 - val_binary_accuracy: 0.7533 - val_sensitivity: 0.9132 - val_specificity: 0.3872 - val_gmeasure: 0.5937 - val_auc: 0.8571\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.4254 - binary_accuracy: 0.8117 - sensitivity: 0.9448 - specificity: 0.5138 - gmeasure: 0.6927 - auc: 0.9049 - val_loss: 0.4406 - val_binary_accuracy: 0.7667 - val_sensitivity: 0.8962 - val_specificity: 0.4747 - val_gmeasure: 0.6509 - val_auc: 0.8636\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.4167 - binary_accuracy: 0.8217 - sensitivity: 0.9431 - specificity: 0.5720 - gmeasure: 0.7253 - auc: 0.9128 - val_loss: 0.4340 - val_binary_accuracy: 0.7733 - val_sensitivity: 0.8962 - val_specificity: 0.4985 - val_gmeasure: 0.6677 - val_auc: 0.8664\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 0s 196us/step - loss: 0.4085 - binary_accuracy: 0.8200 - sensitivity: 0.9383 - specificity: 0.5506 - gmeasure: 0.7140 - auc: 0.9032 - val_loss: 0.4245 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8628 - val_specificity: 0.6669 - val_gmeasure: 0.7584 - val_auc: 0.8697\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.3986 - binary_accuracy: 0.8300 - sensitivity: 0.9343 - specificity: 0.6050 - gmeasure: 0.7471 - auc: 0.9077 - val_loss: 0.4193 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8543 - val_specificity: 0.6907 - val_gmeasure: 0.7679 - val_auc: 0.8679\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.3843 - binary_accuracy: 0.8467 - sensitivity: 0.9242 - specificity: 0.6667 - gmeasure: 0.7823 - auc: 0.9030 - val_loss: 0.4245 - val_binary_accuracy: 0.7733 - val_sensitivity: 0.8850 - val_specificity: 0.5152 - val_gmeasure: 0.6750 - val_auc: 0.8695\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.3839 - binary_accuracy: 0.8333 - sensitivity: 0.9259 - specificity: 0.6195 - gmeasure: 0.7537 - auc: 0.9119 - val_loss: 0.4066 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8543 - val_specificity: 0.6907 - val_gmeasure: 0.7679 - val_auc: 0.8763\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 0s 218us/step - loss: 0.3778 - binary_accuracy: 0.8383 - sensitivity: 0.9291 - specificity: 0.6401 - gmeasure: 0.7654 - auc: 0.9105 - val_loss: 0.4071 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8543 - val_specificity: 0.7543 - val_gmeasure: 0.8026 - val_auc: 0.8796\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.3892 - binary_accuracy: 0.8450 - sensitivity: 0.8928 - specificity: 0.7412 - gmeasure: 0.8087 - auc: 0.9128 - val_loss: 0.4130 - val_binary_accuracy: 0.7867 - val_sensitivity: 0.8850 - val_specificity: 0.5556 - val_gmeasure: 0.7012 - val_auc: 0.8813\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.3608 - binary_accuracy: 0.8350 - sensitivity: 0.9083 - specificity: 0.6675 - gmeasure: 0.7769 - auc: 0.9177 - val_loss: 0.3904 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8543 - val_specificity: 0.6907 - val_gmeasure: 0.7679 - val_auc: 0.8783\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 0s 218us/step - loss: 0.3535 - binary_accuracy: 0.8450 - sensitivity: 0.9182 - specificity: 0.6852 - gmeasure: 0.7903 - auc: 0.9176 - val_loss: 0.3868 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8654 - val_specificity: 0.7210 - val_gmeasure: 0.7897 - val_auc: 0.8828\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.3509 - binary_accuracy: 0.8533 - sensitivity: 0.9106 - specificity: 0.7371 - gmeasure: 0.8157 - auc: 0.9199 - val_loss: 0.3870 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8850 - val_specificity: 0.6502 - val_gmeasure: 0.7584 - val_auc: 0.8870\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 0s 187us/step - loss: 0.3425 - binary_accuracy: 0.8550 - sensitivity: 0.9133 - specificity: 0.7208 - gmeasure: 0.8087 - auc: 0.9213 - val_loss: 0.3809 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8654 - val_specificity: 0.7377 - val_gmeasure: 0.7990 - val_auc: 0.8831\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.3378 - binary_accuracy: 0.8433 - sensitivity: 0.8938 - specificity: 0.7267 - gmeasure: 0.8036 - auc: 0.9210 - val_loss: 0.3773 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8765 - val_specificity: 0.7210 - val_gmeasure: 0.7946 - val_auc: 0.8908\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.3362 - binary_accuracy: 0.8500 - sensitivity: 0.8987 - specificity: 0.7504 - gmeasure: 0.8180 - auc: 0.9244 - val_loss: 0.3757 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8765 - val_specificity: 0.7210 - val_gmeasure: 0.7946 - val_auc: 0.8953\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.3268 - binary_accuracy: 0.8533 - sensitivity: 0.9052 - specificity: 0.7499 - gmeasure: 0.8217 - auc: 0.9239 - val_loss: 0.3781 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8654 - val_specificity: 0.8418 - val_gmeasure: 0.8534 - val_auc: 0.8893\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.3305 - binary_accuracy: 0.8517 - sensitivity: 0.9021 - specificity: 0.7412 - gmeasure: 0.8145 - auc: 0.9251 - val_loss: 0.3690 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8654 - val_specificity: 0.7543 - val_gmeasure: 0.8079 - val_auc: 0.8969\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 0s 218us/step - loss: 0.3261 - binary_accuracy: 0.8667 - sensitivity: 0.8923 - specificity: 0.8094 - gmeasure: 0.8458 - auc: 0.9279 - val_loss: 0.3911 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8962 - val_specificity: 0.6335 - val_gmeasure: 0.7525 - val_auc: 0.9026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.3308 - binary_accuracy: 0.8617 - sensitivity: 0.9022 - specificity: 0.7661 - gmeasure: 0.8224 - auc: 0.9277 - val_loss: 0.3619 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8765 - val_specificity: 0.7377 - val_gmeasure: 0.8040 - val_auc: 0.8999\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.3128 - binary_accuracy: 0.8567 - sensitivity: 0.8979 - specificity: 0.7724 - gmeasure: 0.8314 - auc: 0.9295 - val_loss: 0.3604 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8765 - val_specificity: 0.7210 - val_gmeasure: 0.7946 - val_auc: 0.9032\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 0s 229us/step - loss: 0.3116 - binary_accuracy: 0.8650 - sensitivity: 0.8997 - specificity: 0.7744 - gmeasure: 0.8312 - auc: 0.9290 - val_loss: 0.3565 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8765 - val_specificity: 0.7543 - val_gmeasure: 0.8131 - val_auc: 0.9046\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.3118 - binary_accuracy: 0.8633 - sensitivity: 0.8911 - specificity: 0.8076 - gmeasure: 0.8466 - auc: 0.9299 - val_loss: 0.3554 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8876 - val_specificity: 0.7377 - val_gmeasure: 0.8090 - val_auc: 0.9076\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.3229 - binary_accuracy: 0.8517 - sensitivity: 0.9036 - specificity: 0.7494 - gmeasure: 0.8188 - auc: 0.9312 - val_loss: 0.3859 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8013 - val_specificity: 0.9197 - val_gmeasure: 0.8571 - val_auc: 0.9052\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.3323 - binary_accuracy: 0.8583 - sensitivity: 0.9005 - specificity: 0.7746 - gmeasure: 0.8290 - auc: 0.9312 - val_loss: 0.3565 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8476 - val_specificity: 0.8418 - val_gmeasure: 0.8446 - val_auc: 0.9087\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.3190 - binary_accuracy: 0.8750 - sensitivity: 0.8808 - specificity: 0.8663 - gmeasure: 0.8700 - auc: 0.9319 - val_loss: 0.3644 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8876 - val_specificity: 0.7043 - val_gmeasure: 0.7893 - val_auc: 0.9120\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 0s 196us/step - loss: 0.2971 - binary_accuracy: 0.8700 - sensitivity: 0.8971 - specificity: 0.8082 - gmeasure: 0.8486 - auc: 0.9336 - val_loss: 0.3704 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8105 - val_specificity: 0.9197 - val_gmeasure: 0.8624 - val_auc: 0.9068\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 0s 200us/step - loss: 0.3029 - binary_accuracy: 0.8667 - sensitivity: 0.8904 - specificity: 0.7961 - gmeasure: 0.8347 - auc: 0.9301 - val_loss: 0.3483 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8783 - val_specificity: 0.7543 - val_gmeasure: 0.8139 - val_auc: 0.9146\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.2939 - binary_accuracy: 0.8717 - sensitivity: 0.8883 - specificity: 0.8423 - gmeasure: 0.8639 - auc: 0.9321 - val_loss: 0.3474 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8783 - val_specificity: 0.7543 - val_gmeasure: 0.8139 - val_auc: 0.9100\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.2967 - binary_accuracy: 0.8717 - sensitivity: 0.8953 - specificity: 0.8372 - gmeasure: 0.8629 - auc: 0.9361 - val_loss: 0.3462 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8698 - val_specificity: 0.8323 - val_gmeasure: 0.8498 - val_auc: 0.9152\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.2916 - binary_accuracy: 0.8733 - sensitivity: 0.8982 - specificity: 0.8209 - gmeasure: 0.8564 - auc: 0.9347 - val_loss: 0.3451 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8698 - val_specificity: 0.7781 - val_gmeasure: 0.8223 - val_auc: 0.9143\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 0s 196us/step - loss: 0.2897 - binary_accuracy: 0.8717 - sensitivity: 0.8948 - specificity: 0.8161 - gmeasure: 0.8534 - auc: 0.9353 - val_loss: 0.3443 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8587 - val_specificity: 0.8489 - val_gmeasure: 0.8535 - val_auc: 0.9162\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.2847 - binary_accuracy: 0.8800 - sensitivity: 0.8974 - specificity: 0.8315 - gmeasure: 0.8609 - auc: 0.9330 - val_loss: 0.3425 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8587 - val_specificity: 0.8489 - val_gmeasure: 0.8535 - val_auc: 0.9183\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.2917 - binary_accuracy: 0.8833 - sensitivity: 0.8843 - specificity: 0.8842 - gmeasure: 0.8818 - auc: 0.9358 - val_loss: 0.3640 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8969 - val_specificity: 0.6639 - val_gmeasure: 0.7698 - val_auc: 0.9194\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.2896 - binary_accuracy: 0.8750 - sensitivity: 0.9058 - specificity: 0.8004 - gmeasure: 0.8478 - auc: 0.9375 - val_loss: 0.3617 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8217 - val_specificity: 0.9197 - val_gmeasure: 0.8680 - val_auc: 0.9212\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.2930 - binary_accuracy: 0.8700 - sensitivity: 0.8938 - specificity: 0.8231 - gmeasure: 0.8527 - auc: 0.9459 - val_loss: 0.3541 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8791 - val_specificity: 0.7377 - val_gmeasure: 0.8050 - val_auc: 0.9243\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.2857 - binary_accuracy: 0.8917 - sensitivity: 0.8832 - specificity: 0.8999 - gmeasure: 0.8900 - auc: 0.9485 - val_loss: 0.3442 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8698 - val_specificity: 0.8019 - val_gmeasure: 0.8342 - val_auc: 0.9301\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.2875 - binary_accuracy: 0.8683 - sensitivity: 0.9018 - specificity: 0.8026 - gmeasure: 0.8466 - auc: 0.9476 - val_loss: 0.3546 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8309 - val_specificity: 0.8894 - val_gmeasure: 0.8584 - val_auc: 0.9316\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.2796 - binary_accuracy: 0.8800 - sensitivity: 0.8952 - specificity: 0.8564 - gmeasure: 0.8736 - auc: 0.9493 - val_loss: 0.3431 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8605 - val_specificity: 0.8656 - val_gmeasure: 0.8627 - val_auc: 0.9312\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.2703 - binary_accuracy: 0.8900 - sensitivity: 0.8936 - specificity: 0.8790 - gmeasure: 0.8855 - auc: 0.9501 - val_loss: 0.3471 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8698 - val_specificity: 0.7781 - val_gmeasure: 0.8223 - val_auc: 0.9307\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.2709 - binary_accuracy: 0.8767 - sensitivity: 0.8964 - specificity: 0.8307 - gmeasure: 0.8616 - auc: 0.9517 - val_loss: 0.3507 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8105 - val_specificity: 0.8894 - val_gmeasure: 0.8476 - val_auc: 0.9336\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.2713 - binary_accuracy: 0.8850 - sensitivity: 0.8919 - specificity: 0.8624 - gmeasure: 0.8759 - auc: 0.9518 - val_loss: 0.3417 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8605 - val_specificity: 0.8656 - val_gmeasure: 0.8627 - val_auc: 0.9346\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.2677 - binary_accuracy: 0.8850 - sensitivity: 0.8966 - specificity: 0.8681 - gmeasure: 0.8808 - auc: 0.9534 - val_loss: 0.3449 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8402 - val_specificity: 0.8656 - val_gmeasure: 0.8524 - val_auc: 0.9390\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.2669 - binary_accuracy: 0.8900 - sensitivity: 0.8959 - specificity: 0.8763 - gmeasure: 0.8850 - auc: 0.9532 - val_loss: 0.3401 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8605 - val_specificity: 0.8656 - val_gmeasure: 0.8627 - val_auc: 0.9387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.2601 - binary_accuracy: 0.8900 - sensitivity: 0.8882 - specificity: 0.8886 - gmeasure: 0.8878 - auc: 0.9577 - val_loss: 0.3417 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8698 - val_specificity: 0.8019 - val_gmeasure: 0.8342 - val_auc: 0.9381\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.2644 - binary_accuracy: 0.8817 - sensitivity: 0.8937 - specificity: 0.8604 - gmeasure: 0.8744 - auc: 0.9582 - val_loss: 0.3409 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8513 - val_specificity: 0.8656 - val_gmeasure: 0.8579 - val_auc: 0.9414\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.2593 - binary_accuracy: 0.8900 - sensitivity: 0.9019 - specificity: 0.8515 - gmeasure: 0.8724 - auc: 0.9564 - val_loss: 0.3401 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8698 - val_specificity: 0.8019 - val_gmeasure: 0.8342 - val_auc: 0.9400\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.2622 - binary_accuracy: 0.8933 - sensitivity: 0.8832 - specificity: 0.9066 - gmeasure: 0.8934 - auc: 0.9567 - val_loss: 0.3440 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8791 - val_specificity: 0.7781 - val_gmeasure: 0.8268 - val_auc: 0.9400\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.2587 - binary_accuracy: 0.8817 - sensitivity: 0.8940 - specificity: 0.8605 - gmeasure: 0.8751 - auc: 0.9633 - val_loss: 0.3417 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8402 - val_specificity: 0.8894 - val_gmeasure: 0.8635 - val_auc: 0.9489\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.2547 - binary_accuracy: 0.8850 - sensitivity: 0.8980 - specificity: 0.8540 - gmeasure: 0.8744 - auc: 0.9595 - val_loss: 0.3376 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8513 - val_specificity: 0.8656 - val_gmeasure: 0.8579 - val_auc: 0.9442\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.2519 - binary_accuracy: 0.8900 - sensitivity: 0.8924 - specificity: 0.8898 - gmeasure: 0.8893 - auc: 0.9627 - val_loss: 0.3384 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8791 - val_specificity: 0.8019 - val_gmeasure: 0.8389 - val_auc: 0.9416\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.2476 - binary_accuracy: 0.8917 - sensitivity: 0.8979 - specificity: 0.8774 - gmeasure: 0.8872 - auc: 0.9584 - val_loss: 0.3348 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8513 - val_specificity: 0.8656 - val_gmeasure: 0.8579 - val_auc: 0.9424\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.2489 - binary_accuracy: 0.9000 - sensitivity: 0.8967 - specificity: 0.9102 - gmeasure: 0.9023 - auc: 0.9612 - val_loss: 0.3312 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8598 - val_specificity: 0.8656 - val_gmeasure: 0.8621 - val_auc: 0.9432\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.2460 - binary_accuracy: 0.8883 - sensitivity: 0.9025 - specificity: 0.8643 - gmeasure: 0.8824 - auc: 0.9594 - val_loss: 0.3304 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8598 - val_specificity: 0.8656 - val_gmeasure: 0.8621 - val_auc: 0.9431\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.2440 - binary_accuracy: 0.9000 - sensitivity: 0.8973 - specificity: 0.9048 - gmeasure: 0.8997 - auc: 0.9629 - val_loss: 0.3294 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8598 - val_specificity: 0.8656 - val_gmeasure: 0.8621 - val_auc: 0.9437\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.2458 - binary_accuracy: 0.8917 - sensitivity: 0.8990 - specificity: 0.8749 - gmeasure: 0.8850 - auc: 0.9653 - val_loss: 0.3335 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8876 - val_specificity: 0.8019 - val_gmeasure: 0.8429 - val_auc: 0.9450\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.2457 - binary_accuracy: 0.9000 - sensitivity: 0.8941 - specificity: 0.9107 - gmeasure: 0.9006 - auc: 0.9627 - val_loss: 0.3274 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8876 - val_specificity: 0.8186 - val_gmeasure: 0.8519 - val_auc: 0.9445\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.2415 - binary_accuracy: 0.8867 - sensitivity: 0.9009 - specificity: 0.8516 - gmeasure: 0.8740 - auc: 0.9646 - val_loss: 0.3304 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8506 - val_specificity: 0.8656 - val_gmeasure: 0.8572 - val_auc: 0.9457\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.2428 - binary_accuracy: 0.8950 - sensitivity: 0.9046 - specificity: 0.8752 - gmeasure: 0.8862 - auc: 0.9661 - val_loss: 0.3293 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8598 - val_specificity: 0.8656 - val_gmeasure: 0.8621 - val_auc: 0.9457\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.2403 - binary_accuracy: 0.8933 - sensitivity: 0.8967 - specificity: 0.8859 - gmeasure: 0.8901 - auc: 0.9630 - val_loss: 0.3263 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8691 - val_specificity: 0.8656 - val_gmeasure: 0.8670 - val_auc: 0.9450\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.2353 - binary_accuracy: 0.9050 - sensitivity: 0.9088 - specificity: 0.8946 - gmeasure: 0.9004 - auc: 0.9649 - val_loss: 0.3266 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8691 - val_specificity: 0.8656 - val_gmeasure: 0.8670 - val_auc: 0.9438\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 0s 199us/step - loss: 0.2372 - binary_accuracy: 0.9033 - sensitivity: 0.8925 - specificity: 0.9267 - gmeasure: 0.9079 - auc: 0.9623 - val_loss: 0.3473 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8962 - val_specificity: 0.7615 - val_gmeasure: 0.8255 - val_auc: 0.9407\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 0s 226us/step - loss: 0.2432 - binary_accuracy: 0.8983 - sensitivity: 0.9034 - specificity: 0.8749 - gmeasure: 0.8869 - auc: 0.9619 - val_loss: 0.3357 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8105 - val_specificity: 0.9197 - val_gmeasure: 0.8624 - val_auc: 0.9443\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.2381 - binary_accuracy: 0.8933 - sensitivity: 0.9050 - specificity: 0.8506 - gmeasure: 0.8733 - auc: 0.9619 - val_loss: 0.3336 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8217 - val_specificity: 0.9197 - val_gmeasure: 0.8680 - val_auc: 0.9466\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.2358 - binary_accuracy: 0.8967 - sensitivity: 0.8955 - specificity: 0.8998 - gmeasure: 0.8963 - auc: 0.9673 - val_loss: 0.3295 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8420 - val_specificity: 0.9197 - val_gmeasure: 0.8786 - val_auc: 0.9431\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.2333 - binary_accuracy: 0.9067 - sensitivity: 0.8992 - specificity: 0.9140 - gmeasure: 0.9053 - auc: 0.9625 - val_loss: 0.3244 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8691 - val_specificity: 0.8656 - val_gmeasure: 0.8670 - val_auc: 0.9431\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.2261 - binary_accuracy: 0.9017 - sensitivity: 0.9043 - specificity: 0.8917 - gmeasure: 0.8961 - auc: 0.9685 - val_loss: 0.3251 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8691 - val_specificity: 0.8656 - val_gmeasure: 0.8670 - val_auc: 0.9430\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.2238 - binary_accuracy: 0.9067 - sensitivity: 0.9080 - specificity: 0.9017 - gmeasure: 0.9030 - auc: 0.9658 - val_loss: 0.3238 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8691 - val_specificity: 0.8656 - val_gmeasure: 0.8670 - val_auc: 0.9454\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.2215 - binary_accuracy: 0.9167 - sensitivity: 0.9067 - specificity: 0.9346 - gmeasure: 0.9202 - auc: 0.9697 - val_loss: 0.3214 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8691 - val_specificity: 0.8656 - val_gmeasure: 0.8670 - val_auc: 0.9445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.2233 - binary_accuracy: 0.9100 - sensitivity: 0.9180 - specificity: 0.8882 - gmeasure: 0.9016 - auc: 0.9680 - val_loss: 0.3262 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8691 - val_specificity: 0.8656 - val_gmeasure: 0.8670 - val_auc: 0.9417\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.2276 - binary_accuracy: 0.9100 - sensitivity: 0.9071 - specificity: 0.9217 - gmeasure: 0.9125 - auc: 0.9696 - val_loss: 0.3310 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8876 - val_specificity: 0.8186 - val_gmeasure: 0.8519 - val_auc: 0.9358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:143] Training end with time 15.370774507522583!\n",
      "[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_0.h5 \n",
      "[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_0.h5\n",
      "[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_0.json\n",
      "[root    |INFO|build_network.py:169] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "750/750 [==============================] - 0s 7us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:174] Evaluation end with time 0.010320901870727539!\n",
      "[root    |INFO|build_network.py:175] Evaluation: [0.25114068388938904, 0.8880000114440918, 0.9189189076423645, 0.818965494632721, 0.867503821849823, 0.9578118920326233]\n",
      "[root    |INFO|build_network.py:169] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 15us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:174] Evaluation end with time 0.010769367218017578!\n",
      "[root    |INFO|build_network.py:175] Evaluation: [0.23001721501350403, 0.8840000033378601, 0.9349112510681152, 0.7777777910232544, 0.85273277759552, 0.9723865985870361]\n",
      "[root    |INFO|deepbiome.py:179] Compute time : 18.214656829833984\n",
      "[root    |INFO|deepbiome.py:180] 1 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:137] -------2 simulation start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:148] Build network for 2 simulation\n",
      "[root    |INFO|build_network.py:505] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:506] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:510] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:511] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:518]      Genus: 48\n",
      "[root    |INFO|build_network.py:518]     Family: 40\n",
      "[root    |INFO|build_network.py:518]      Order: 23\n",
      "[root    |INFO|build_network.py:518]      Class: 17\n",
      "[root    |INFO|build_network.py:518]     Phylum: 9\n",
      "[root    |INFO|build_network.py:521] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:522] Phylogenetic_tree_dict info: ['Family', 'Class', 'Phylum', 'Number', 'Genus', 'Order']\n",
      "[root    |INFO|build_network.py:523] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:546] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:562] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:563] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:564] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:640] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:158] 2 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:133] Training start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 1s 864us/step - loss: 0.6658 - binary_accuracy: 0.7117 - sensitivity: 0.9699 - specificity: 0.0179 - gmeasure: 0.0308 - auc: 0.4473 - val_loss: 0.6334 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.4256\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.6138 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4587 - val_loss: 0.5930 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.4267\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.5876 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4705 - val_loss: 0.5867 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.4287\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.5853 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4789 - val_loss: 0.5871 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.4294\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.5856 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4797 - val_loss: 0.5867 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.4421\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4969 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.4537\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5330 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5014\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6674 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6733\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.5856 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7140 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6719\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.5852 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6512 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6624\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6338 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6628\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 0s 218us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6396 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6649\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.5856 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6278 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6607\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.5852 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6154 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6639\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 0s 194us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6284 - val_loss: 0.5867 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6671\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 0s 190us/step - loss: 0.5853 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6264 - val_loss: 0.5868 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6684\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 0s 195us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6395 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6736\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6384 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6799\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.5857 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6402 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6806\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.5853 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6357 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6825\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6457 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6874\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6604 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6894\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.5854 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6667 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6961\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.5854 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6623 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.5853 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6605 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6975\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.5854 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6616 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7018\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6674 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7029\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.5859 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6676 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7031\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.5849 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6808 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7068\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.5858 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6765 - val_loss: 0.5867 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7054\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6703 - val_loss: 0.5864 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7032\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6839 - val_loss: 0.5864 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7060\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6771 - val_loss: 0.5864 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7088\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.5854 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6704 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7103\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6764 - val_loss: 0.5863 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7060\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.5852 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6750 - val_loss: 0.5863 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7059\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.5847 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6765 - val_loss: 0.5862 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7046\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6771 - val_loss: 0.5863 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7075\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.5848 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6843 - val_loss: 0.5859 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7060\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.5847 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6816 - val_loss: 0.5859 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7052\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.5843 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6929 - val_loss: 0.5856 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7148\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 0s 221us/step - loss: 0.5843 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6858 - val_loss: 0.5854 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7164\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 0s 218us/step - loss: 0.5840 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6906 - val_loss: 0.5854 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7198\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.5836 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6925 - val_loss: 0.5848 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7268\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 0s 198us/step - loss: 0.5859 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6937 - val_loss: 0.5848 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7319\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.5834 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7023 - val_loss: 0.5841 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7394\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.5826 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6989 - val_loss: 0.5835 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7468\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.5819 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7106 - val_loss: 0.5832 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.5813 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7137 - val_loss: 0.5824 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7532\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.5806 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7246 - val_loss: 0.5817 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7566\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.5799 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7271 - val_loss: 0.5808 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7595\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.5788 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7395 - val_loss: 0.5799 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7624\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 0s 197us/step - loss: 0.5778 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7398 - val_loss: 0.5787 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7664\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.5766 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7468 - val_loss: 0.5773 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7752\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.5743 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7567 - val_loss: 0.5756 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7783\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.5751 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7531 - val_loss: 0.5746 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7478\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.5699 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7575 - val_loss: 0.5722 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7707\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.5659 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7648 - val_loss: 0.5697 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7320\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.5632 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7509 - val_loss: 0.5675 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7182\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.5602 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7385 - val_loss: 0.5630 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7198\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 0s 188us/step - loss: 0.5563 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7416 - val_loss: 0.5605 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7137\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.5524 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7612 - val_loss: 0.5573 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7159\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.5439 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7620 - val_loss: 0.5569 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7153\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.5412 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7685 - val_loss: 0.5514 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7174\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.5338 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7622 - val_loss: 0.5475 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7217\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.5271 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7816 - val_loss: 0.5417 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7343\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.5217 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7799 - val_loss: 0.5370 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7392\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.5130 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8030 - val_loss: 0.5301 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7431\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.5089 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8036 - val_loss: 0.5228 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7581\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.5013 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8078 - val_loss: 0.5169 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7638\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.4927 - binary_accuracy: 0.7317 - sensitivity: 1.0000 - specificity: 0.0125 - gmeasure: 0.0456 - auc: 0.8239 - val_loss: 0.5097 - val_binary_accuracy: 0.7333 - val_sensitivity: 1.0000 - val_specificity: 0.0256 - val_gmeasure: 0.0925 - val_auc: 0.7757\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.4910 - binary_accuracy: 0.7367 - sensitivity: 1.0000 - specificity: 0.0333 - gmeasure: 0.1172 - auc: 0.8223 - val_loss: 0.5042 - val_binary_accuracy: 0.7400 - val_sensitivity: 1.0000 - val_specificity: 0.0465 - val_gmeasure: 0.1758 - val_auc: 0.7835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.4813 - binary_accuracy: 0.7500 - sensitivity: 0.9977 - specificity: 0.0816 - gmeasure: 0.2128 - auc: 0.8243 - val_loss: 0.4978 - val_binary_accuracy: 0.7400 - val_sensitivity: 1.0000 - val_specificity: 0.0465 - val_gmeasure: 0.1758 - val_auc: 0.7946\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.4726 - binary_accuracy: 0.7550 - sensitivity: 1.0000 - specificity: 0.0916 - gmeasure: 0.2373 - auc: 0.8301 - val_loss: 0.4922 - val_binary_accuracy: 0.7467 - val_sensitivity: 0.9912 - val_specificity: 0.0951 - val_gmeasure: 0.3053 - val_auc: 0.7973\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.4661 - binary_accuracy: 0.7617 - sensitivity: 0.9975 - specificity: 0.1329 - gmeasure: 0.3051 - auc: 0.8377 - val_loss: 0.4849 - val_binary_accuracy: 0.7600 - val_sensitivity: 0.9912 - val_specificity: 0.1485 - val_gmeasure: 0.3829 - val_auc: 0.8023\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.4613 - binary_accuracy: 0.7667 - sensitivity: 0.9908 - specificity: 0.1754 - gmeasure: 0.3995 - auc: 0.8433 - val_loss: 0.4792 - val_binary_accuracy: 0.7800 - val_sensitivity: 0.9912 - val_specificity: 0.2228 - val_gmeasure: 0.4689 - val_auc: 0.8101\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.4538 - binary_accuracy: 0.7867 - sensitivity: 0.9785 - specificity: 0.2718 - gmeasure: 0.4769 - auc: 0.8612 - val_loss: 0.4817 - val_binary_accuracy: 0.7600 - val_sensitivity: 1.0000 - val_specificity: 0.1277 - val_gmeasure: 0.3502 - val_auc: 0.8136\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.4504 - binary_accuracy: 0.7733 - sensitivity: 0.9928 - specificity: 0.1824 - gmeasure: 0.4124 - auc: 0.8478 - val_loss: 0.4668 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.9912 - val_specificity: 0.3018 - val_gmeasure: 0.5410 - val_auc: 0.8250\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.4482 - binary_accuracy: 0.7867 - sensitivity: 0.9846 - specificity: 0.2637 - gmeasure: 0.5049 - auc: 0.8594 - val_loss: 0.4641 - val_binary_accuracy: 0.7867 - val_sensitivity: 0.9912 - val_specificity: 0.2505 - val_gmeasure: 0.4944 - val_auc: 0.8270\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.4403 - binary_accuracy: 0.7900 - sensitivity: 0.9806 - specificity: 0.2903 - gmeasure: 0.5187 - auc: 0.8677 - val_loss: 0.4600 - val_binary_accuracy: 0.7867 - val_sensitivity: 0.9912 - val_specificity: 0.2505 - val_gmeasure: 0.4944 - val_auc: 0.8282\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.4342 - binary_accuracy: 0.7867 - sensitivity: 0.9907 - specificity: 0.2329 - gmeasure: 0.4553 - auc: 0.8678 - val_loss: 0.4567 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.9814 - val_specificity: 0.3552 - val_gmeasure: 0.5810 - val_auc: 0.8346\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.4299 - binary_accuracy: 0.8100 - sensitivity: 0.9700 - specificity: 0.3848 - gmeasure: 0.6021 - auc: 0.8780 - val_loss: 0.4600 - val_binary_accuracy: 0.7933 - val_sensitivity: 1.0000 - val_specificity: 0.2505 - val_gmeasure: 0.4969 - val_auc: 0.8391\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.4334 - binary_accuracy: 0.7917 - sensitivity: 0.9820 - specificity: 0.2943 - gmeasure: 0.5106 - auc: 0.8740 - val_loss: 0.4629 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.9255 - val_specificity: 0.5572 - val_gmeasure: 0.7098 - val_auc: 0.8444\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.4363 - binary_accuracy: 0.8050 - sensitivity: 0.9687 - specificity: 0.3958 - gmeasure: 0.5843 - auc: 0.8799 - val_loss: 0.4406 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.9912 - val_specificity: 0.3552 - val_gmeasure: 0.5831 - val_auc: 0.8453\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.4179 - binary_accuracy: 0.8217 - sensitivity: 0.9723 - specificity: 0.4240 - gmeasure: 0.6344 - auc: 0.8820 - val_loss: 0.4394 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.9912 - val_specificity: 0.3296 - val_gmeasure: 0.5634 - val_auc: 0.8481\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.4146 - binary_accuracy: 0.8067 - sensitivity: 0.9887 - specificity: 0.3035 - gmeasure: 0.5343 - auc: 0.8802 - val_loss: 0.4344 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.9814 - val_specificity: 0.4274 - val_gmeasure: 0.6380 - val_auc: 0.8502\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.4179 - binary_accuracy: 0.8183 - sensitivity: 0.9639 - specificity: 0.4399 - gmeasure: 0.6406 - auc: 0.8838 - val_loss: 0.4360 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.9912 - val_specificity: 0.3296 - val_gmeasure: 0.5634 - val_auc: 0.8526\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 0s 218us/step - loss: 0.4079 - binary_accuracy: 0.8217 - sensitivity: 0.9743 - specificity: 0.4115 - gmeasure: 0.6263 - auc: 0.8773 - val_loss: 0.4274 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.9912 - val_specificity: 0.3552 - val_gmeasure: 0.5831 - val_auc: 0.8572\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.4047 - binary_accuracy: 0.8183 - sensitivity: 0.9795 - specificity: 0.3833 - gmeasure: 0.6004 - auc: 0.8776 - val_loss: 0.4269 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.9912 - val_specificity: 0.3296 - val_gmeasure: 0.5634 - val_auc: 0.8583\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.3992 - binary_accuracy: 0.8300 - sensitivity: 0.9744 - specificity: 0.4394 - gmeasure: 0.6507 - auc: 0.8903 - val_loss: 0.4215 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9912 - val_specificity: 0.4274 - val_gmeasure: 0.6405 - val_auc: 0.8597\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.4023 - binary_accuracy: 0.8317 - sensitivity: 0.9698 - specificity: 0.4722 - gmeasure: 0.6579 - auc: 0.8836 - val_loss: 0.4192 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.9636 - val_specificity: 0.4482 - val_gmeasure: 0.6509 - val_auc: 0.8575\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.3932 - binary_accuracy: 0.8300 - sensitivity: 0.9770 - specificity: 0.4520 - gmeasure: 0.6547 - auc: 0.8924 - val_loss: 0.4176 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9912 - val_specificity: 0.4274 - val_gmeasure: 0.6405 - val_auc: 0.8617\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 0s 197us/step - loss: 0.3897 - binary_accuracy: 0.8300 - sensitivity: 0.9745 - specificity: 0.4388 - gmeasure: 0.6506 - auc: 0.8917 - val_loss: 0.4157 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9636 - val_specificity: 0.5502 - val_gmeasure: 0.7225 - val_auc: 0.8636\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.3884 - binary_accuracy: 0.8367 - sensitivity: 0.9407 - specificity: 0.5668 - gmeasure: 0.7259 - auc: 0.8943 - val_loss: 0.4259 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.9912 - val_specificity: 0.3296 - val_gmeasure: 0.5634 - val_auc: 0.8669\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.3929 - binary_accuracy: 0.8283 - sensitivity: 0.9633 - specificity: 0.4641 - gmeasure: 0.6534 - auc: 0.8919 - val_loss: 0.4109 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9636 - val_specificity: 0.5502 - val_gmeasure: 0.7225 - val_auc: 0.8632\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.3813 - binary_accuracy: 0.8467 - sensitivity: 0.9751 - specificity: 0.5013 - gmeasure: 0.6950 - auc: 0.8937 - val_loss: 0.4084 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9912 - val_specificity: 0.4274 - val_gmeasure: 0.6405 - val_auc: 0.8692\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.3834 - binary_accuracy: 0.8350 - sensitivity: 0.9482 - specificity: 0.5476 - gmeasure: 0.7143 - auc: 0.9007 - val_loss: 0.4123 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9912 - val_specificity: 0.4274 - val_gmeasure: 0.6405 - val_auc: 0.8705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.3883 - binary_accuracy: 0.8267 - sensitivity: 0.9634 - specificity: 0.4571 - gmeasure: 0.6538 - auc: 0.8909 - val_loss: 0.4067 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9451 - val_specificity: 0.5780 - val_gmeasure: 0.7328 - val_auc: 0.8680\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.3771 - binary_accuracy: 0.8383 - sensitivity: 0.9504 - specificity: 0.5442 - gmeasure: 0.7139 - auc: 0.8989 - val_loss: 0.3999 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9822 - val_specificity: 0.4482 - val_gmeasure: 0.6566 - val_auc: 0.8716\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 0s 201us/step - loss: 0.3713 - binary_accuracy: 0.8467 - sensitivity: 0.9676 - specificity: 0.5203 - gmeasure: 0.7052 - auc: 0.9038 - val_loss: 0.3970 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9636 - val_specificity: 0.4947 - val_gmeasure: 0.6841 - val_auc: 0.8708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:143] Training end with time 14.741532564163208!\n",
      "[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_1.h5 \n",
      "[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_1.h5\n",
      "[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_1.json\n",
      "[root    |INFO|build_network.py:169] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "750/750 [==============================] - 0s 7us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:174] Evaluation end with time 0.010555028915405273!\n",
      "[root    |INFO|build_network.py:175] Evaluation: [0.3745288550853729, 0.843999981880188, 0.9597069621086121, 0.5343137383460999, 0.7160897850990295, 0.8955729603767395]\n",
      "[root    |INFO|build_network.py:169] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 19us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:174] Evaluation end with time 0.009517908096313477!\n",
      "[root    |INFO|build_network.py:175] Evaluation: [0.4316432476043701, 0.8080000281333923, 0.9367815852165222, 0.5131579041481018, 0.6933375000953674, 0.8602162599563599]\n",
      "[root    |INFO|deepbiome.py:179] Compute time : 16.03988289833069\n",
      "[root    |INFO|deepbiome.py:180] 2 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:137] -------3 simulation start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:148] Build network for 3 simulation\n",
      "[root    |INFO|build_network.py:505] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:506] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:510] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:511] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:518]      Genus: 48\n",
      "[root    |INFO|build_network.py:518]     Family: 40\n",
      "[root    |INFO|build_network.py:518]      Order: 23\n",
      "[root    |INFO|build_network.py:518]      Class: 17\n",
      "[root    |INFO|build_network.py:518]     Phylum: 9\n",
      "[root    |INFO|build_network.py:521] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:522] Phylogenetic_tree_dict info: ['Family', 'Class', 'Phylum', 'Number', 'Genus', 'Order']\n",
      "[root    |INFO|build_network.py:523] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:546] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:562] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:563] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:564] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:640] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:158] 3 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:133] Training start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 0s 813us/step - loss: 0.6796 - binary_accuracy: 0.6550 - sensitivity: 0.9167 - specificity: 0.0833 - gmeasure: 0.0000e+00 - auc: 0.4907 - val_loss: 0.6685 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5349\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.6524 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5010 - val_loss: 0.6518 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5094\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.6309 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4962 - val_loss: 0.6458 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5177\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.6223 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4732 - val_loss: 0.6462 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5058\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.6200 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5072 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5068\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.6207 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5001 - val_loss: 0.6504 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.4769\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.6208 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4956 - val_loss: 0.6492 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5208\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 0s 199us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4950 - val_loss: 0.6488 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5351\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.6207 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5242 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5370\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5026 - val_loss: 0.6485 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5351\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.6209 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5028 - val_loss: 0.6475 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5227\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.6210 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4990 - val_loss: 0.6488 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5282\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4909 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.4999\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4957 - val_loss: 0.6485 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5268\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5065 - val_loss: 0.6478 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5210\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.6207 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5080 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5027\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 0s 197us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5215 - val_loss: 0.6476 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5152\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4891 - val_loss: 0.6477 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5240\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.6207 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5102 - val_loss: 0.6489 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5149\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4904 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5421\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.6207 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5247 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5372\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 0s 226us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5310 - val_loss: 0.6478 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5335\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5123 - val_loss: 0.6477 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5336\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 213us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5027 - val_loss: 0.6478 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5277\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5057 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5132\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4964 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5204\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5047 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5389\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4968 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5200\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5146 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5256\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.6211 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5098 - val_loss: 0.6491 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5226\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 0s 221us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5094 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5404\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 0s 221us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5142 - val_loss: 0.6477 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5155\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5088 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5231\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5042 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5369\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.6208 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5260 - val_loss: 0.6486 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5165\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5022 - val_loss: 0.6478 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5505\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5047 - val_loss: 0.6477 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5361\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5123 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5219\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5084 - val_loss: 0.6478 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5273\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5067 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5307\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5009 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5213\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.6208 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5149 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5078\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 0s 221us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5093 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5392\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5046 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5695\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5261 - val_loss: 0.6478 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5359\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.6207 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5047 - val_loss: 0.6486 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5382\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 0s 218us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5078 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      "600/600 [==============================] - 0s 221us/step - loss: 0.6207 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5129 - val_loss: 0.6476 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5245\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5202 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5493\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5077 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5117\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5050 - val_loss: 0.6485 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5386\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5022 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5232\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.6208 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5049 - val_loss: 0.6490 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5408\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.6207 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5071 - val_loss: 0.6479 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5218\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 0s 222us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5158 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5189\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4998 - val_loss: 0.6479 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5365\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5095 - val_loss: 0.6478 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5474\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.6207 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5261 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5386\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5073 - val_loss: 0.6479 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5257\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 0s 221us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5237 - val_loss: 0.6478 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5198\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 0s 218us/step - loss: 0.6207 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5158 - val_loss: 0.6487 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5281\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5015 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5250\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5190 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5316\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5060 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5490\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5205 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5355\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4992 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5280\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5178 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5471\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5094 - val_loss: 0.6485 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5116\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 0s 235us/step - loss: 0.6210 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5164 - val_loss: 0.6491 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5291\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5119 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5256\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5130 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "600/600 [==============================] - 0s 218us/step - loss: 0.6208 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5036 - val_loss: 0.6477 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5364\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 0s 218us/step - loss: 0.6207 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5119 - val_loss: 0.6474 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5385\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5163 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5339\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5221 - val_loss: 0.6485 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5316\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5142 - val_loss: 0.6486 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5339\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 0s 221us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5107 - val_loss: 0.6479 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5285\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.6207 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5062 - val_loss: 0.6485 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5395\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5144 - val_loss: 0.6479 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5294\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5217 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5183\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5140 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5184\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5099 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5421\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.6208 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5210 - val_loss: 0.6487 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5249\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5057 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5240\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5102 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5399\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5160 - val_loss: 0.6477 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5366\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5212 - val_loss: 0.6476 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5293\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5219 - val_loss: 0.6478 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5321\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5312 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5358\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5185 - val_loss: 0.6485 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5636\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.6113 - binary_accuracy: 0.7000 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.53 - 0s 209us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5157 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5203\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5233 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5531\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.6208 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5248 - val_loss: 0.6477 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5546\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5197 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5187\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 215us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5212 - val_loss: 0.6485 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5306\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5223 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5311\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5266 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5189\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5260 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5343\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5161 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5582\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 0s 218us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5223 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:143] Training end with time 14.821483135223389!\n",
      "[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_2.h5 \n",
      "[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_2.h5\n",
      "[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_2.json\n",
      "[root    |INFO|build_network.py:169] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "750/750 [==============================] - 0s 7us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:174] Evaluation end with time 0.013065338134765625!\n",
      "[root    |INFO|build_network.py:175] Evaluation: [0.6259680390357971, 0.6813333630561829, 1.0, 0.0, 0.0, 0.521493673324585]\n",
      "[root    |INFO|build_network.py:169] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 17us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:174] Evaluation end with time 0.011275291442871094!\n",
      "[root    |INFO|build_network.py:175] Evaluation: [0.6016963124275208, 0.7120000123977661, 1.0, 0.0, 0.0, 0.5388966798782349]\n",
      "[root    |INFO|deepbiome.py:179] Compute time : 16.203693151474\n",
      "[root    |INFO|deepbiome.py:180] 3 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:183] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:185] Train Evaluation : ['loss' 'binary_accuracy' 'sensitivity' 'specificity' 'gmeasure' 'auc']\n",
      "[root    |INFO|deepbiome.py:188]       mean : [0.41721253 0.80444445 0.95954196 0.45109308 0.52786454 0.79162618]\n",
      "[root    |INFO|deepbiome.py:189]        std : [0.15597074 0.08888665 0.03310142 0.33948037 0.37834048 0.19269509]\n",
      "[root    |INFO|deepbiome.py:190] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:192] Test Evaluation : ['loss' 'binary_accuracy' 'sensitivity' 'specificity' 'gmeasure' 'auc']\n",
      "[root    |INFO|deepbiome.py:195]       mean : [0.42111893 0.80133335 0.95723095 0.4303119  0.51535676 0.79049985]\n",
      "[root    |INFO|deepbiome.py:196]        std : [0.15191974 0.07037676 0.03025193 0.32288509 0.37017667 0.1837093 ]\n",
      "[root    |INFO|deepbiome.py:197] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:206] Total Computing Ended\n",
      "[root    |INFO|deepbiome.py:207] -----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_evaluation, train_evaluation, network = deepbiome.deepbiome_train(log, network_info, path_info, number_of_fold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`deepbiome_train` saves the trained model weights, evaluation results and history based on the path information from the configuration.\n",
    "\n",
    "From the example above, we can check that `hist_*.json`, `weight_*.h5`, `test_eval.npy`, `train_eval.npy` files were saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hist_0.json',\n",
       " 'weight_2.h5',\n",
       " 'test_eval.npy',\n",
       " 'weight_0.h5',\n",
       " 'train_eval.npy',\n",
       " 'hist_2.json',\n",
       " 'weight_1.h5',\n",
       " 'hist_1.json']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path_info['model_info']['model_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the history files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAELCAYAAAA2mZrgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl4VOX1wPHvyWTfFxK2BBKQLewQ\nNkEWQQQXUEQEwQWr1N2ftrVUba20tta2ilZq665VQVxQXAAVcQFkC0LYIewJSxaykIUkk3l/f9wx\nBEhIgEwmyZzP88yTzJ1775zLaM7cdzmvGGNQSimlALzcHYBSSqmGQ5OCUkqpCpoUlFJKVdCkoJRS\nqoImBaWUUhU0KSillKrg0qQgImNEZIeIpIrIzCpef1ZENjgfO0Uk15XxKKWUOjtx1TwFEbEBO4HL\ngDRgLTDFGLO1mv3vA3obY25zSUBKKaVq5Mo7hf5AqjFmjzGmFJgHjD/L/lOAuS6MRymlVA1cmRRa\nAwcrPU9zbjuDiLQFEoBvXBiPUkqpGni7OwCnycAHxpjyql4UkRnADICgoKC+nTt3rs/YlFKq0UtO\nTs4yxkTXtJ8rk0I6EFfpeaxzW1UmA/dUdyJjzEvASwBJSUlm3bp1dRWjUkp5BBHZX5v9XNl8tBbo\nICIJIuKL9Yd/4ek7iUhnIAL40YWxKKWUqgWXJQVjjB24F1gCbAPmG2O2iMgsERlXadfJwDyj5VqV\nUsrtXNqnYIz5AvjitG1/OO35H10Zg1JKqdprKB3NSikPVFZWRlpaGidOnHB3KE2Gv78/sbGx+Pj4\nnNfxmhSUUm6TlpZGSEgI8fHxiIi7w2n0jDFkZ2eTlpZGQkLCeZ1Dax8ppdzmxIkTREVFaUKoIyJC\nVFTUBd15aVJQSrmVJoS6daH/np6TFA6sgq8eBx3kpJRS1fKcpHB4I6yYDcePuDsSpVQDMWLECJYs\nWXLKttmzZ3PXXXdVe0xwcDAAhw4dYuLEiVXuM3z4cGqaZDt79myKiooqnl9xxRXk5rq/ULTnJIWY\nROtnxhb3xqGUajCmTJnCvHnzTtk2b948pkyZUuOxrVq14oMPPjjv9z49KXzxxReEh4ef9/nqiuck\nheZdrZ9Hq6zcrZTyQBMnTuTzzz+ntLQUgH379nHo0CF69+7NyJEj6dOnD927d+eTTz4549h9+/bR\nrVs3AIqLi5k8eTJdunTh2muvpbi4uGK/u+66i6SkJLp27crjjz8OwPPPP8+hQ4cYMWIEI0aMACA+\nPp6srCwAnnnmGbp160a3bt2YPXt2xft16dKFO+64g65duzJ69OhT3qeueM6Q1MBICGkJGZoUlGqI\nnvh0C1sP5dfpORNbhfL41V2rfT0yMpL+/fuzaNEixo8fz7x585g0aRIBAQEsWLCA0NBQsrKyGDhw\nIOPGjau2E/fFF18kMDCQbdu2kZKSQp8+fSpee/LJJ4mMjKS8vJyRI0eSkpLC/fffzzPPPMOyZcto\n1qzZKedKTk7m9ddfZ/Xq1RhjGDBgAMOGDSMiIoJdu3Yxd+5cXn75ZSZNmsSHH37ItGnT6uYfy8lz\n7hTAakI6utndUSilGpDKTUg/Nx0ZY3jkkUfo0aMHo0aNIj09naNHj1Z7ju+//77ij3OPHj3o0aNH\nxWvz58+nT58+9O7dmy1btrB169m/mC5fvpxrr72WoKAggoODmTBhAj/88AMACQkJ9OrVC4C+ffuy\nb9++C7n0KnnOnQJA80RYvRzK7WDzrEtXqqE72zd6Vxo/fjwPPvgg69evp6ioiL59+/LGG2+QmZlJ\ncnIyPj4+xMfHn9fY/7179/KPf/yDtWvXEhERwa233npBcwj8/PwqfrfZbC5pPvKwO4WuUF4Cx3a7\nOxKlVAMRHBzMiBEjuO222yo6mPPy8oiJicHHx4dly5axf//Zq04PHTqUd999F4DNmzeTkpICQH5+\nPkFBQYSFhXH06FEWLVpUcUxISAjHjx8/41yXXHIJH3/8MUVFRRQWFrJgwQIuueSSurrcGnnW1+WK\nzuYtEN3JvbEopRqMKVOmcO2111Y0I02dOpWrr76a7t27k5SURE0Le911111Mnz6dLl260KVLF/r2\n7QtAz5496d27N507dyYuLo7BgwdXHDNjxgzGjBlDq1atWLZsWcX2Pn36cOutt9K/f38Abr/9dnr3\n7u2SpqKqSGOrWH1Bi+zYS+DJljDkQRj5+7oNTCl1zrZt20aXLl3cHUaTU9W/q4gkG2OSajrWs5qP\nvP0g6iIdgaSUUtXwrKQAVmfzUZ3AppRSVfGopFBqd1idzbn7oeTMDh6llPJ0HpMUXvlhD71mfUlZ\ntLOdLWObewNSSqkGyGOSQsuwAIpKy0mVNtYGbUJSSqkzeExS6BEbBsC63BDwDdbOZqWUqoLHJIXY\niAAig3xJScuHmC5aGE8pD5ednU2vXr3o1asXLVq0oHXr1hXPfy6QV5Pp06ezY8eOs+4zZ84c3nnn\nnboIuV54zOQ1EaFHbBgpaXnQPhG2fmItuKOrPinlkaKiotiwYQMAf/zjHwkODubXv/71KfsYYzDG\n4OVV9ffn119/vcb3ueeeey482HrkMXcKAD1iw9mVcZzSZolwIheydrk7JKVUA5OamkpiYiJTp06l\na9euHD58mBkzZlSUv541a1bFvkOGDGHDhg3Y7XbCw8OZOXMmPXv2ZNCgQWRkZADw2GOPVZS/HjJk\nCDNnzqR///506tSJlStXAlBYWMh1111HYmIiEydOJCkpqSJh1TePuVMA6BkbhsPAlvAR9Lb5wcrn\nYfwL7g5LKQWwaCYc2VS352zRHcY+dc6Hbd++nbfeeoukJGsC8FNPPUVkZCR2u50RI0YwceJEEhMT\nTzkmLy+PYcOG8dRTT/HQQw/x2muvMXPmzDPObYxhzZo1LFy4kFmzZrF48WL+9a9/0aJFCz788EM2\nbtx4Sunt+uZxdwoAydk+0Ocm2DgP8tLcHJVSqqFp3759RUIAmDt3Ln369KFPnz5s27atyvLXAQEB\njB07Fjh7WesJEyacsc/y5cuZPHkyYNVL6trVPRVjwcPuFKJD/GgV5s/GtDwY+wAkvwEr/wVj/+bu\n0JRS5/GN3lWCgoIqft+1axfPPfcca9asITw8nGnTplVZ/trX17fid5vNht1ur/LcP5e/Pts+7uRR\ndwpg3S2kpOVCeBvocQMkvwkFme4OSynVQOXn5xMSEkJoaCiHDx9myZIldf4egwcPZv78+QBs2rSp\nxoV4XMnzkkJcGPuzi8gtKrWqpdpPwKp/uzsspVQD1adPHxITE+ncuTM333zzKeWv68p9991Heno6\niYmJPPHEEyQmJhIWFlbn71MbnlU6G1iRmsXUV1bz1m39GdoxGubfAru+gvjBUF4KjnLwCwH/cAiI\nAP8wCAgHv1BrgZ6S41BSYP0sPQ6lhdZ+ke0hqr31OwACBUcgcwdk7QQvb4jtB20GQrNO8PMQN2Mg\nPRm2LIDCLBj2sHUepTyAls622O127HY7/v7+7Nq1i9GjR7Nr1y68vc+vhf9CSmd7VJ8CQLfWVvZN\nScu1ksKIRyD3ABRmgs0XxMt6XpxiDVstLaj6RD5B4BcMvkHWH/OSsyw4HtrauiPZ4JzA4uUDQdEQ\nHA2F2ZCfZm2z+VrzJ0b+HgbcCV62Or56pVRDVFBQwMiRI7Hb7Rhj+O9//3veCeFCeVxSCAvwoV2z\nIKuzGawV2GYsq/6A8jI4kQ8leWDzs+4ifINPftMH69t+YZa1zGdpARgAAwGR0KwD+Ida+xzbAwdX\nW3cOBZlQmAFhcXDpY9BprHXX8flDsOQRWP+WtfaDfzgENYO2F0PbwVYiUko1KeHh4SQnJ7s7DMAD\nkwJYdZB+3JNdu51tPhAUZT2qI2J96w+OPvs+Ue3P3jQUEA5T5sGmDyD5dSuJnMiz7mJWzLbuJuIG\nwEUjocNoa3lRnZGtGjljDKL/HdeZC+0S8Mik0LtNBB9vOMQ1c1ZwWWJzLunQDH8fG/Zyg93hoKzc\nQand+t3H5oWftxd+3jaKy8opKLFTWGIn0NdGs2A/ooJ9KbMbjhWVcqywBC8RIoN8iQj0pcRezsGc\nYtJzivH3sTGwXSSxEYFnD04EelxvPX5WdgIO/Ah7lsHub2DpE9YjtDX0vRX63Q6BkS79N1PKFfz9\n/cnOziYqKkoTQx0wxpCdnY2/v/95n8PjOprBWmzn5R/28OWWIyebkepJm8hAurcOIzzQh/BAH8od\nsC+rkD1ZBZTaHVzerQXX9m5N5xah1Z8k/zCkfg1bP7Z++gRC75tg2G/PfkejVANTVlZGWlpaleP+\n1fnx9/cnNjYWHx+fU7bXtqPZI5NCZUfzT5C8PwcALxG8vQQfby98bV5424Qyu4MSu4MSezn+PjZC\n/L0J9PWmqNROVkEp2QWl+NiEqGBfwgN9cTgMOUVl5BSW4uvtRWxEAK0jAsgrLmNlajY/7slmd0YB\necVl5BaX4SVWokhoFozd4WD5rizsDkP76CC6tgqjQ0wwHZqH0D02jFZh/md+mzq61ZqAt2k+BLeA\nSW9CbI2fu1LKwzSIpCAiY4DnABvwijHmjCmLIjIJ+CNW9+xGY8yNZztnXScFdzLG4DBg8zr5hz67\noITPNx1m2fYMdmUUkJZTXPFas2A/esWFM6ZbCy7v2pwQ/0rfBA79BPNvtu4ixvzValLS23GllJPb\nk4KI2ICdwGVAGrAWmGKM2Vppnw7AfOBSY0yOiMQYYzLOdt6mlBRqo6jUzo4jx9mUnsfGg3ms3ptN\nWk4xft5eXJbYnJsHxdMvPsK6gyg6Bgt+Cbu+hG7XwVWzrZFPSimP1xDmKfQHUo0xe5wBzQPGA5Xn\nb98BzDHG5ADUlBA8UaCvN73bRNC7TQQMsu4u1h/I5ZMN6SzceIjPUg7TKy6cGUPbcXnXFtimvAfL\nn4FlT1p3D9e/AS17uvsylFKNhCvLXLQGDlZ6nubcVllHoKOIrBCRVc7mpjOIyAwRWSci6zIzPbtO\nkYjQt20Es8Z348eZI/nT+K7kFJVy9zvrGfb3ZbyyYh/5/R+AWz+3Ri29Mgo2f+TusJVSjYS7ax95\nAx2A4cAU4GURCT99J2PMS8aYJGNMUnT0WeYCeJgAXxs3DYrnm18N5z/T+tAqLIA/f76NQX9ZykfZ\nbeDO5dC6Lyy4Ew6udXe4SqlGwJVJIR2Iq/Q81rmtsjRgoTGmzBizF6sPooMLY2qSbF7CmG4tmX/n\nID69dwhdW4fxq/c3smBnMUx+F0JbwbwbIfdgzSdTSnk0VyaFtUAHEUkQEV9gMrDwtH0+xrpLQESa\nYTUn7XFhTE1e99gw3pzen4EJUfxq/kYW7joBN75n1V6aO8Uq5qeUUtVwWVIwxtiBe4ElwDZgvjFm\ni4jMEpFxzt2WANkishVYBvzGGFPL+hOqOgG+Nl69NYmk+EgefG8Di4+GwsTXIWMLfPagu8NTSjVg\nHj95rSkrLLFz06ur2Xwon3duH0C/fS/Bt3+1mpQ6X+nu8JRS9ai2Q1Ld3dGsXCjIz5tXb+lHbHgA\nt7+5jtROd0Dz7tbdQtExd4enlGqANCk0cRFBvrx5W398bF7c8uZGjl32LBRlw+LfuTs0pVQDpEnB\nA8RFBvL6rf3IKSrlrqV2zOAHIWUe7Fjs7tCUUg2MJgUP0T02jD9e3ZXVe4/xjv8NEN3ZWsyn3O7u\n0JRSDYgmBQ9yfVIsQztG85clu8lM+rW1UtzmD90dllKqAdGk4EFEhKcmdMdLhAc2tMY07wrfPw2O\ncneHppRqIDQpeJhW4QE8emUXVu7N4fuWt0F2qt4tKKUqaFLwQJP7xTGoXRQPboylPDoRvtO7BaWU\nRZOCBxIRfndFZ44Vl7Mo8mbI3qWVVJVSgCYFj9UjNpyx3Vowc1tb7M26wMrn3B2SUqoB0KTgwX41\nuiNFZYavAsfCkU3WQynl0TQpeLCLYkKY2DeWx/d0wXj5wIa57g5JKeVmmhQ83AOjOpJrQtgSfDFs\nmg/lZe4OSSnlRpoUPFzr8AAm9YvlXzn9oTATUr92d0hKKTfSpKC49eJ4lpZ1p8gnAja86+5wlFJu\npElBcVFMCIM6tOATxxDMjkVaVlspD6ZJQQFwy6B43iq6GHGU6QxnpTyYJgUFwIjOMRREdGavd3tt\nQlLKg2lSUADYvISbB8bzdvEgOLQesna5OySllBtoUlAVJiXF8ZXXEBx4Qcp77g5HKeUGmhRUhbBA\nH4b06cYKR3fKN74HDoe7Q1JK1TNNCuoU0y+O50P7YGx5B+DganeHo5SqZ5oU1Ck6NA+hIOFyivCn\nfKOWvVDK02hSUGeYekkii8uTKN+0AOwl7g5HKVWPNCmoMwzrGM2qoJH4luXDziXuDkcpVY80Kagz\neHkJ3S8ZR4YJJ3fV/9wdjlKqHmlSUFWakBTPErmYoIPLoKTA3eEopeqJJgVVpSA/bxwdr8THlFGw\nRZuQlPIUmhRUtfoNvYIcE8zRdbp+s1KeQpOCqlZibCTJvv2IOfwdlNvdHY5Sqh5oUlBn5eh4BSHm\nOIc3f+vuUJRS9UCTgjqrnsMnUGK8ObRay2kr5Qk0Kaizah7djG0BvWl++BuM1kJSqsnTpKBqZDpe\nQaw5wuaNa9wdilLKxVyaFERkjIjsEJFUEZlZxeu3ikimiGxwPm53ZTzq/HQaej2ANiEp5QG8XXVi\nEbEBc4DLgDRgrYgsNMZsPW3X94wx97oqDnXhApvFsd+/My2PfENxaTkBvjZ3h6SUchFX3in0B1KN\nMXuMMaXAPGC8C99PuZB0HU8PUvnu+2/cHYpSyoVcmRRaAwcrPU9zbjvddSKSIiIfiEicC+NRFyBu\n5F0UEoD/6ucxxrg7HKWUi7i7o/lTIN4Y0wP4Cnizqp1EZIaIrBORdZmZmfUaoLJIYAT7203mktIf\nSElZ7+5wlFIu4sqkkA5U/uYf69xWwRiTbYz5uWD/K0Dfqk5kjHnJGJNkjEmKjo52SbCqZglXPYxd\nvClY+k93h6KUchFXJoW1QAcRSRARX2AysLDyDiLSstLTccA2F8ajLlBAZCs2xVxNv7zFZKTvdXc4\nSikXcFlSMMbYgXuBJVh/7OcbY7aIyCwRGefc7X4R2SIiG4H7gVtdFY+qGy3G/hYvDAc/+5u7Q1FK\nuYA0tk7DpKQks27dOneH4dFW/H0ifQq/xzy0ncDQSHeHo5SqBRFJNsYk1bSfuzuaVSMUOfxuAijh\nmwWvuDsUpVQd06SgzlmXpBFk+cYStXsBWw7luTscpVQd0qSgzp0Iwf2mMsBrG/98/xvKHY2rCVIp\nVT1NCuq8+PedjBeGjhmLeXPlPneHo5SqI5oU1PmJbIeJ7c+0wFX848vtbD2U7+6IlFJ1QJOCOm/S\n8wZiy/bR1y+dm15dTWpGgbtDUkpdIE0K6vx1nQBe3vyr6y5EYNorqzl4rMjdUSmlLoAmBXX+AiOh\nw2jCd3/C/6b3pbisnBtfWUVecZm7I1NKnadaJQURaS8ifs7fh4vI/SIS7trQVKPQ91Y4fpgu65/g\nv9P6cPBYMQs3HnJ3VEqp81TbO4UPgXIRuQh4CavQ3bsui0o1Hh0vhyEPQfIbDEh/k47Ng/n4p/Sa\nj1NKNUi1TQoOZy2ja4F/GWN+A7Ss4RjlKS79PXS/HvlmFr9ptZHk/TkcyNa+BaUao9omhTIRmQLc\nAnzm3ObjmpBUo+PlBePnQPwljNoxi7ZyhE826N2CUo1RbZPCdGAQ8KQxZq+IJAD/c11YqtHx9oMJ\nLyEOO/dGrWfBhnRdoU2pRqhWScEYs9UYc78xZq6IRAAhxhitnaxOFdoK2g5mNCvZk1nA5nSd0KZU\nY1Pb0UffikioiEQC64GXReQZ14amGqVu1xJWsIdutnQWaIezUo1ObZuPwowx+cAE4C1jzABglOvC\nUo1Wl/EgNu6J3sjCjYewlzvcHZFS6hzUNil4O5fOnMTJjmalzhQcDQlDGVr2A1kFJ/ggOc3dESml\nzkFtk8IsrGU1dxtj1opIO2CX68JSjVq3CQQVHmBqXA6PL9yixfKUakRq29H8vjGmhzHmLufzPcaY\n61wbmmq0Ol8FXj482nYbYQE+3P1OMvkntPSFUo1BbTuaY0VkgYhkOB8fikisq4NTjVRgJLS/lMBd\nC3lhSm8O5hTz8PspOkRVqUagts1HrwMLgVbOx6fObUpVrdsEyDtIf9sOZo7pzOItR5j12VYcukqb\nUg1abZNCtDHmdWOM3fl4A4h2YVyqsetyNQREwornuf2SBKYPjuf1Fft4+MMUHZGkVANW26SQLSLT\nRMTmfEwDsl0ZmGrkfINg4F2wcxFydAt/uCqRB0d15IPkNO55dz0l9nJ3R6iUqkJtk8JtWMNRjwCH\ngYnArS6KSTUV/e8A32BY/iwiwgOjOvD41Yks2XKUP3y8xd3RKaWqUNvRR/uNMeOMMdHGmBhjzDWA\njj5SZxcQAf1+AVs+guzdAEwfnMC9Iy7ivXUHtcS2Ug3Qhay89lCdRaGaroH3gJcPrHiuYtP/jepA\n//hIHlmwid2Zuq6zUg3JhSQFqbMoVNMV0hx6T4MN70KeNbvZ2+bFc1N64eftxT3vrOdE2Zn9C/Zy\nh3ZIK+UGF5IUdGyhqp3BD4CXDT68A+ylALQMC+CZSb3YfuQ4t7y2hqP5Jyp2/25nJgP/upTHPt7s\nroiV8lhnTQoiclxE8qt4HMear6BUzSLaWovwHFgJix6u2Dyicwz/vL4nKWl5XPHcDyzddpS/LtrG\nLa+tIb/Yzkfr08kpLHVj4Ep5nrMmBWNMiDEmtIpHiDHGu76CVE1A94kw+P8g+XVY+2rF5uv6xvLp\nfYNpFuzHL95cx3+/28PUAW1475cDKS138JF2RitVr/QPu6o/I/8AGVutu4VmHSHhEgAuignhk3sH\n8+9lqSS2CmVMN2v5755x4cxbc4DbBscjol1YStWHC+lTUOrceNngulcgsh28Nw2yThba9fex8dDo\nThUJAWBKvzh2ZRSw/kBOlaczxugkOKXqmCYFVb/8w+DG+eDlDe9cD4XVT4y/umcrgnxtzF1z8IzX\njuaf4Jo5K7h2zkottKdUHdKkoOpfZAJMmQv5h2DejVB2osrdgvy8GderFZ+lHDql9Pbm9DzGv7CC\njWl5bD2cz46jx+srcqWaPE0Kyj3i+sO1/4GDq2DxzGp3m9yvDSfKHDy9eDtvrNjLP5bs4Pr//IiX\nwFu39UcEvtxytB4DV6ppc2lHs4iMAZ4DbMArxpinqtnvOuADoJ8xZp0rY1INSLcJcOgnWPk8XDQK\nulx1xi49YsPoERvG26sOVGzrHx/JC1N7ExPiT582EXy59Qj3j+xQn5Er1WS5LCmIiA2YA1wGpAFr\nRWShMWbrafuFAA8Aq10Vi2rALv097P0OFt4HrftCaMtTXhYR3r59AJnHS4gI9CXU3xtv28kb3NGJ\nzfnrou2k5xbTOjygvqNXqslxZfNRfyDVuXRnKTAPGF/Ffn8C/gZU3bCsmjZvX7juVbCfgI/vBMeZ\npS1C/X1oHx1MZJDvKQkBYHTXFgB8teVIvYSrVFPnyqTQGqg8bCTNua2CiPQB4owxn5/tRCIyQ0TW\nici6zMzMuo9UuVezDjDmr7DnW1j78jkdmtAsiA4xwXy5VfsVlKoLbutoFhEv4BngVzXta4x5yRiT\nZIxJio7WBd+apD63QMIw+O5pKC06p0NHd23O6r3HyC3SkhhKXShXJoV0IK7S81jntp+FAN2Ab0Vk\nHzAQWCgiSS6MSTVUIjD8d1CUBeteO6dDRye2oNxh+GZ7houCU8pzuDIprAU6iEiCiPgCk4GFP79o\njMkzxjQzxsQbY+KBVcA4HX3kwdoOgoSh1toLZcW1Pqx76zBahPqzeLNr+xXeXX2AHUd0ToRq2lyW\nFIwxduBeYAmwDZhvjNkiIrNEZJyr3lc1csN+C4UZkPxmzfv+9A4snYWXl3BF95Z8ufUot7y2hpS0\n3DoPKz23mEcWbOLf36bW+bmVakhcOk/BGPMF8MVp2/5Qzb7DXRmLaiTih0DbIbBiNvS9FXz8q97v\n2F74/CFw2GHIQzw8phMxoX7857vdjHthBb3bhOPj5UWJvZzQAB/+fE032kYFnXdYS5x3Iav3HMMY\nc84F+vKKysguLKFddPB5x6BUfdAZzarhGfYwHD8M798KX/4evv0bHFxz8nVj4ItfW8NYHXY4uBp/\nHxt3DmvPDw+P4KHLOiJY3RThgb6kpOUx4d8rqy2sVxuLnUNej+SfIC2n9k1bP/vLF9uY8OJKyh1a\np0k1bJoUVMOTMNRawjNtLax5Gb79C7w25mST0taPIfVruPQxEBvsX1FxaIi/D/eP7MBHdw/mvV8O\n4s3b+rPg7osJ8vNmykurWLz58DmHk1VQwtp9xxjjnBOxeu+xcz7Hj3uyyS0qY9vh/HM+Vqn6pElB\nNTwi1kptD++Gx47AzAPQbjh8ej8seRQWzYSWPWHwg9C6D+xbftbTtYsOZsHdF5PYKpQ7317PjLfW\nseVQXq3D+XrrUYyBey+9iLAAH9aeY1LIyD/BgWPWMNu1+849oShVnzQpqIbPPwxufM/qY/jxBSg4\nClc9CzZvaDsY0pOhtPCsp4gK9mPuHQN5cFRHVu3J5srnl3P7m2uZt+YAO48ep6zcwfJdWfzuoxQG\nP/UNn6Ucqjh28ZYjtIkMpGurUPrFR7LmHP+wr9tvNVv52IQ153GXoVR90pXXVONg84GrZlt3CGDV\nSQKIv8TqlD64BtqPOOsp/H1sPDCqA7cOjueNFfv436p9fL3Nmttg8xLKHYYgXxsRQb785v0UOsSE\n0DLcnxWpWUwfnICIMCAhkq+3HSUj/wQxodV0gp9m3b4c/Ly9GN21BStTs86ro1qp+qJJQTUeIpB0\n26nb2gyw+hX2La8xKfwsLMCHB0Z14P6RF7E3q5D1B3LZcSSfvm0jGN4phvziMq7813LufDuZ24Yk\nUFZuuNzZn9A/IRKANfuOcVWPVrV6v+T9x+gZF87g9lF8uvEQe7IKaa+jkFQDpc1HqnHzC4FWvU7p\nbK4tEaFddDAT+8by6JWJjOnWEn8fGzGh/vx7ah8OHivijwu3EBPiR++4cAC6tgol0NdW62agolI7\nmw/l0y8+4mRCqebY/dmFvL6+P+KTAAAb4ElEQVRir45QUm6lSUE1fvFDIG3dOddMOpt+8ZE8ckUX\nyh2G0V2b4+VlNfd427zo2zai1klhw8Fcyh2GpLaRJDQLolmwX5Ud1av2ZDN+zgqe+HQrK1Kz6uw6\nlDpXmhRU4xd/CTjKIG1Nzfueg+mD4/nn9T25d8SpC/gMSIhk+5HjtSrAt25fDiLQp01ERZ/E6UNa\n5687yE2vriYqyJcgX1vFnAil3EGTgmr84gaAeMG+c29COhsR4bq+sbQIO7VDuX9CFABr99U8GW7d\n/hw6xoQQFugDQL/4CNJzi0nPtSbAvfT9bh7+IIWB7aL46O7BDO8cw5dbjtZpE9Lfl2zngXk/8cWm\nwxSW2OvsvKpp0o5m1fj5h0LLXrDvh3p5ux6xYfh6e/Hk51ZTT6+4cC7p0IyoYL9T9it3GNbvz2F8\nr5Md0hUJZe8xVjkMf/liO1f1aMmzN/TCx+bFmK4t+DzlMOsP5NAvPvKCY80uKOHFb3fjJcInGw7h\n6+3FLYPa8sgVXXQElKqSJgXVNHQYDd89BWtfgX63u/St/H1s/OGqRBZuPMR7aw/yxsp9BPnamDG0\nPXcMTSDQ1/rfaseR4xSU2EmKj6g4tlOLEEL9vXll+R62Hz7O4Iui+Oeknvg4V5Qb0TkGX28vFm06\nUidJYcmWozgMfHLPYApL7cxbc4CXf9iLzcuLmWM7X/D5VdOjSUE1DZf8Cg5vhM9/BTZf6HOzS99u\n2sC2TBvYFnu5g62H83nx2908+/VO3l69nyn929AhJpitzpIWSW1P/nG3eQlJ8ZF8sz2DxJah/Gda\nX/y8bRWvB/t5M7RDM5ZsOcLvr7rwb/OLNh8mPiqQbq1DK/o0Av28+c93u4kI9OGXw9pf0PlV06NJ\nQTUN3r4w6U2YOwUW3m/1MfSaas1tcOXb2rzoERvOi9P6krw/h78t2s7zS3dVvN4yzJ/YiIBTjhnf\nqxXZhaW8fHNfQvx9zjjn5V1b8PW2DDan59M9Nuy8Y8spLGXl7mxmDG1XkVxEhD+N70ZecRl/XbSd\niEBfJvWLO+W4lbuzWLXnGA+O6qBNTB5Ik4JqOrz9YPI78O4k+OQeWP4s9LoRek6B0NpNNLsQfdtG\nMP/OQRSXlrM3q5A9WQW0iQw84w/r+F6tGd+rdTVngVFdmmPzEhZvOXxBSeGrrVaH9RXdWp6y3eYl\nPDupF3lFZTz28Wa6x4bRpWUoABnHT3D3O+vJLSojoVkg1/aOrfb8OjO7adLRR6pp8QmAG9+HcS9A\nUAwsnQXP94YDq+othABfG4mtQrmqRyt6xIaf8/ERQb4MbBd5wSvJfb7pMHGRAXRrHXrGa77eXjw3\nuRdhgT48MO8nTpSVY4zhdx9uori0nE7NQ/jzZ9vIKyo749jUjALueWc93R5forWcmiBNCqrp8fGH\nPjfBbYvgvvUQ2hrmTYWcfe6OrNbGdG3B7sxChv19GTe9uprff7yZN1fuY+XuLLIKSjDm7ENW84rK\nWJGaxRXdWlb7bT4q2I9/XN+TnUcL+OsX2/ggOY2l2zP4zeWdePaGXuQWl/H0ku0V+6fnFvOr+RsZ\n/ex3fLsjg2B/b+55dz0Z+Sfq9Nobix93Z/Pj7mx3h1HntPlINW1R7eHG+fDKSHj3BvjFl1bV1QZu\nQp9YsgpK2Z1ZwIFjRXy8IZ3jJ07OMQgP9KF9dDDtmgURGxFIyzB/Wob707VVGJFBvny17Sh2h2Fs\n95ZneRcY1jGa2wYn8NqKvfj7eNE/IZLbBifg5SXcenE8r63Yy9huLflxTxav/LAXA9w2OIG7hrcn\nq6CUa+as4O531vPuHQPx9fbiSN4Jvt+ZyfDO0cSE1K5gYGWldgcGc0rne0NU7jA8MO8nikvLWfqr\nYbUujtgYSE3fOBqapKQks27dOneHoRqbPd/B2xOsBXxueAd8A90d0TkxxnA0v4SdR4+zK6OA3ZkF\n7M4oYE9WIZnHS07Zt3OLEE6UlVNqd7Bi5qU1tvufKCvnmjkrOHCsiMUPDKVNlPVvU1BiZ9Q/v+OI\n807gml6t+M2YzrQOP9lx/unGQ9w39ycm9G6Nwxg+SzmM3WEID/ThiXFdGdezVcX7Hyssxc/biyC/\nM7+LlpU7eGfVfmYv3UWZ3cHILs25ontLhneKxt+n4SWIVXuymfyS1SQ5vlcrnpvc280R1UxEko0x\nSTXup0lBeYz1b8HC+yAiwVrEJ36wuyOqEyX2cjLyS0jLKWb9gRxW7s5i3b4c7hrenv8b1bFW58gr\nKiOnqJT4ZqeuY70iNYu3ftzHncPa07tNRJXH/umzrby6fC9BvjYm9YtjVJfm/OPLHfx0IJdLO8cQ\n4u9N8v6cimVMwwN9aB0eQOvwAFqFBxAd4seHyWnsySpk8EVRtIkMZPHmI+QUlRET4sdvx3Tm2t6t\nK+pPNQSPLtjER+vTmTawDS//sJd3bh/A4IuauTuss9KkoFRV9v5gjUzK3W9NcusxGVr2sEYuNSEO\nh6m3P6L2cgff7cykX0Ikoc4htuUOw6vL9/DMVzsJC/AhqW0kveLCKXM4SM+xynwczj1Bem4xBSV2\n2kcH8eiVXRjRKQYRoazcwYrULJ79aicb0/LoGRfOpKRYso6XciS/mCBfb6YPSTjlrqW+2Msd9P/L\nUi5uH8U/ru/J6Ge/x9smLHrgkgbd7KVJQanqlBZao5JW/xcwYPOD2H5w9XPQ7CJ3R9ek2Msd2Lzk\nrE1Yx0+UEeTrXWUSczgMC35K52+Lt5PhbCZrFuxHXrFVjHBSUhx3j7ioXpPD9zszufm1Nfz3pr5c\n3rUFy3ZkMP31tdw8qC13DmtPKzckqtrQpKBUTY4fhYOrrceGdyGkBdy+tH76G4xx+cS6puREWTmZ\nx0uICfXDz9tGem4xL36byntrDyIIU/rHcc+Ii2rs8E3PLeaj5DRu6Bd33p3Dv3l/I4s3H2HtY6Mq\n+jsemr+Bj9anA1afzlU9WnLbkJMlTxoCTQpKnYvd38D/JlizoK+Zc3J7zj4IbmENc60sPRlCYyGk\n+bm/13dPw+aP4PavrEWC1HlLzy3mhW9SeX/dQbxtwsS+sXRqEUrrcH9ahQcQFxFIkJ83J8rK+e93\ne3jxu1ROlDloHR7AG9P70aH5yX//tJwiAnxsZxQ2rKzEXk7Sn79mdGIL/jmpZ8V2YwypGQV8sz2D\npdszWLP3GC1C/Zk5tjPje7W64El+JfZy5izbzdQBbWh+nslMk4JS5+qbJ+H7p+GaFyG6M3zzZ9i9\n1OqYvupZa7nPE3mw+BHY8DZcNAqmfXhu77HzS3j3euv34Y/A8N/W/XV4oP3Zhcz+ehefpRyirPzU\nv2nNgv0AQ1ZBKVf2aMmE3q2Z+dEmTpSV8+LUvhw/UcbrK/dVTMSLCPShQ0wIl3aJYVJSHJFBvhXn\n+nrrUW5/ax1vTO/H8E4x1cazdt8xnvh0C5vTrVX3npnUi7jI87sD3XY4nwff28D2I8d5YlxXbrk4\n/rzOo0lBqXPlKIe3xsOBH8Fhh4AISPoFbFkAx3ZD4nhrhbfjh6FFD6sA3/+lQHib2p0/Lw3+M8S6\nwwiLtUp9P7ARghr2qJXGpNxhyCoosdasyCnmwLEiDh4rIq+4jFsujmdgO6t0+cFjRUx/Yy2pGQUA\nxEYEMHVAW3xswu7MArYeymdjWh6+3l5c1b0lHVuEUFRazrc7Mjh4rIg1j46qqGxbHYfD8H7yQf78\n2TYQ+Nt1Pbiihnkjp1/LS9/v4ZmvdhAW4MvfruvOyC7ncWfqpElBqfNx/Ch8+Atric+Bd1trNZSd\ngB/+ActnQ2Q7604iOBpm94BhD8OIR2o+b3kZvH4FZGyFX35vJaB/D4ABd8KYv7r+utQZ8orKmPNt\nKn3bRlTUm6ps59HjvL1qPx+tT6fAuThRgI+NGUPb8eBltRvqC1YCunfuT2w8aA3RdRjDkbwTzlFX\nwXRpGUq31qEM7RhdMXprd2YBv35/Iz8dyGVstxY8eW33U+5YzocmBaXqWkEG+IdbFVnB6oPI3A7/\ntwm8ahiKuHQW/PBPuO5V6D7R2vbJvZDyHtyXXPu7DVXvSu0O7A4H/t628x7mW1bu4J9f7uSTDelE\nBfvSIjSAQF8bO48eZ3dmAWXlBl9vL0Z0iqZ9dDCvLt+Lv4+NWeNPnQB4ITQpKOVqWz+B+TdbBfg6\njq5+v8Mp8NJw6DkZrvn3ye156Vaxvm7XwbUvujxc1TCV2h1sSs/ls5TDfJZymMzjJYzqEsNfru1e\np+UzNCko5Wr2UnimC7QZaJXsrkq53aq7lJ8O96yBwNNWU1vyKPw4Bx7aWi/lvVXDVu4wHMotJjYi\noM7Lktc2KWiVVKXOl7evtV7DzsVWX0RVVv8HDm+AsU+fmRDAuUKcge2fuzRU1TjYvIS4KtbgqE+a\nFJS6EH1utkYqffmoNdw0L83qmC46Boc2WMNaO46FrtdWfXx0J2jWyWqKUqoBaDjT7ZRqjJp1sOon\npcyDTe+f+bpvCFz5z7PPXk4cZ3VCF2bp8FTldpoUlLpQE/4LY5+CjG1wdIs1wc03yFoFLm4ghFW/\n9CYAXcbB93+HHV84m5OUch+XJgURGQM8B9iAV4wxT532+p3APUA5UADMMMZsdWVMSrlEQAS0vdh6\nnKsW3SG8LWxdqElBuZ3L+hRExAbMAcYCicAUEUk8bbd3jTHdjTG9gKeBZ1wVj1INlojVhLTnWyjO\ntbY5HJC1y61hKc/kyo7m/kCqMWaPMaYUmAeMr7yDMSa/0tMgoHGNj1WqrnQZD44y2PUllByH96bB\nC0mw5mV3R6Y8jCubj1oDBys9TwMGnL6TiNwDPAT4Ape6MB6lGq7WfSGkFax7HZY/C5k7ICYRFs+0\n6iy1OeN/HaVcwu1DUo0xc4wx7YHfAo9VtY+IzBCRdSKyLjMzs34DVKo+eHlBl6vgwErIP2RVX52+\nCMLirFnTP8+DyNhmTXbL2efWcFXT5bIZzSIyCPijMeZy5/PfARhjqqz+JSJeQI4xJuxs59UZzarJ\nyt4Ny/5iFdiLam9tO7oFXhllDX318rbWcQBrqOtVz0KP690Xr2pUGsKM5rVABxFJEBFfYDKwsPIO\nItKh0tMrAe1ZU54rqj1MfPVkQgBo3hXG/csq011aBJf/BWZ8Z23/6HZYcGfNdw1ZqZCWbHVeg7Xq\n255v4a1rYO4U67xKObm09pGIXAHMxhqS+pox5kkRmQWsM8YsFJHngFFAGZAD3GuM2XK2c+qdgvJI\nhdlWmYyfJ8GV260Fgb7/OxiHtRBQ+0thwC+tWdI/27EI3rvJ6sQObGYtDJS9y7rjCIqBwkxr8aDJ\nc89cXU41KVoQTylPkL0bUr+2lhPd+wOYcrjsT9D/Dmsy3PxbrHkQA+6E1K+sff3DYfADVt2mTe/D\nJ/dAxzEw6X9w/JB1Hgz0vknXkW5CNCko5WkKMqw/8Lu+tGZSp6+Dlr2sTuuAcGsfY878Q7/2Vfj8\nIfALg5K8k9tH/RGGPFhf0SsXq21S0DIXSjUVwTFw43xY+wp8+Ri06m0lBP9KYzeq+ubf7xdWSY7U\nr61kEj/EqsX09R8huAX0mlJvl6DcT+8UlGqKio6BXwjYfM7veHspvDMR9q+AKe9Bh1F1G5+qdw1h\n9JFSyl0CI88/IYC1VsQNb0NMF2ueROaOuotNNWiaFJRSVfMPtZqjfAKsxFBa6O6IVD3QpKCUql5o\nK7juFetO4bOHrI5q1aRpR7NS6uzaj4DhM+Hbv0LrPlZHdGGmtSjQiTwoyQdHOXS/HiLaujtadYE0\nKSilajb0N3BgFSx6uPp9vn0K+twEl/y65oWFVIOlSUEpVTMvG0x6C7Z9Cr6BEBRtzZD2D7P6Hopz\nrWGs69+yHhEJEBZr3TlcfP+ppTtUg6ZDUpVSdSf3gFX++9huyEuDjO1WErnpY2jRzdqnvAw2zoWW\nPa2Hqhc6eU0pVf/C28Cox08+z9wJb42HN66Emz6yiu998WvI3A5ePta+A++xSoeXHIctH1t3Hp2v\nsu5OVL3TpKCUcp3ojnDbInhzHLw2FspLrPWoJ74Omz+0Zl7vXmYlk03vQ2mBdVxUB6sfo9t1YNM/\nU/VJm4+UUq6Xfwg+vhvi+lv1lHwCrOGt616DJY9Y+3SdAEnTIT8dvvs7ZGyBZh1h9J+hw2gtzneB\ntCCeUqpxKDoG4nWyaB9Yaz9s/wyWPgHZqdBuOIx6Alr1cleUjZ6WuVBKNQ6BkacmBLD6GBLHwd2r\nYOzT1iJDLw2Dl0dao5tKCtwTqwfQpKCUarhsPtbCQfdvgDFPWX0OC++D53rAT2+fXE1O1RltPlJK\nNR7GWJPovn4cDq6GNoPg4vvg2F7rbqI4x5pZnTheV5I7jfYpKKWaLocDNrwDX/0Bio9Z20JaWXcW\nufshMMpaWa7z1RCbZA1vdTiszuu0tVZZDvECb3/oNNZqwmriNCkopZq+omNwZJNV4js4xvrDv/c7\na6GhHYus5Un9w60Fh46kQFH2mecIjLI6sXtNtfoyfj6vf1jt50oUZIK92Bpa20BpUlBKebbiHGsO\nROrXcGiDNXs6YSi0HQQ+QVbCyD0IXz5qNUW17gsBkVbyKDhqrTrXfSL0nGytc12dA6tg3lRr8t3o\nP0H/GQ1y+KwmBaWUqg2HA1LmWVVgfYOt5BHdGQ6usda7dpRZ8yTGvQAhzU89duN7sPBeCIuDyHaQ\n+hVcdBlc82/rzqUB0aSglFIXqjAbfvqfM2EEWYkhfgjs/d6aR7FxLsRfYhULDIg4uT42wEWjrA7v\njpefuk62m2hSUEqpupKxHT663eq/EJvV9OQbbPVDjP6ztXzpzzJ3wNpXrYqyxw+Bzc9KDD1ugA6X\ngbefWy5Bk4JSStUleyms+re1qFD7SyG2/6nJ4HQOB6Svg80fweYPrIWJQlrCTQusjvF6pklBKaUa\ninI77FkGn9xr3WXc+jlEd6rXELTMhVJKNRQ2b6vp6JZPAYE3r4asXbU/vrQIvvmzNVrKxTQpKKVU\nfYnuaCUGRzm8NsaafJeebM3UrooxsP1zmDMAvv877Fzs8hC1ULlSStWnmM5w62ew5FH4cQ6seA7C\n2lgFALtOgNZ9rPLhe7+31pxI/RpiEmH6Imh7scvD0z4FpZRyl6Jj1szrrR9bE+0cZeAXBiV51uuB\nUdb6EwPutEp4XABdjlMppRq6wEjoPdV6FOdYCWL/Cojpas2+jkk8WXqjnmhSUEqphiAgwiri1+tG\nt4ahHc1KKaUqaFJQSilVQZOCUkqpCpoUlFJKVXBpUhCRMSKyQ0RSRWRmFa8/JCJbRSRFRJaKSFtX\nxqOUUursXJYURMQGzAHGAonAFBFJPG23n4AkY0wP4APgaVfFo5RSqmauvFPoD6QaY/YYY0qBecD4\nyjsYY5YZY4qcT1cBsS6MRymlVA1cmRRaA5WrN6U5t1XnF8Ciql4QkRkisk5E1mVmZtZhiEoppSpr\nEJPXRGQakAQMq+p1Y8xLwEvOfTNFZP95vlUzIOs8j23MPPG6PfGawTOv2xOvGc79umvVZ+vKpJAO\nxFV6HuvcdgoRGQU8CgwzxpTUdFJjTPT5BiQi62pT+6Op8cTr9sRrBs+8bk+8ZnDddbuy+Wgt0EFE\nEkTEF5gMLKy8g4j0Bv4LjDPGZLgwFqWUUrXgsqRgjLED9wJLgG3AfGPMFhGZJSLjnLv9HQgG3heR\nDSKysJrTKaWUqgcu7VMwxnwBfHHatj9U+n2UK9+/Ci/V8/s1FJ543Z54zeCZ1+2J1wwuuu5Gt56C\nUkop19EyF0oppSp4TFKoqeRGUyAicSKyzFk6ZIuIPODcHikiX4nILufPCHfHWtdExCYiP4nIZ87n\nCSKy2vl5v+cc7NCkiEi4iHwgIttFZJuIDPKQz/pB53/fm0Vkroj4N7XPW0ReE5EMEdlcaVuVn61Y\nnndee4qI9LmQ9/aIpFDLkhtNgR34lTEmERgI3OO8zpnAUmNMB2Cp83lT8wDWgIaf/Q141hhzEZCD\nNTmyqXkOWGyM6Qz0xLr+Jv1Zi0hr4H6s8jjdABvWyMam9nm/AYw5bVt1n+1YoIPzMQN48ULe2COS\nArUoudEUGGMOG2PWO38/jvVHojXWtb7p3O1N4Br3ROgaIhILXAm84nwuwKVY9bSgaV5zGDAUeBXA\nGFNqjMmliX/WTt5AgIh4A4HAYZrY522M+R44dtrm6j7b8cBbxrIKCBeRluf73p6SFM615EajJyLx\nQG9gNdDcGHPY+dIRoLmbwnKV2cDDgMP5PArIdQ6Lhqb5eScAmcDrzmazV0QkiCb+WRtj0oF/AAew\nkkEekEzT/7yh+s+2Tv++eUpS8CgiEgx8CPyfMSa/8mvGGm7WZIacichVQIYxJtndsdQzb6AP8KIx\npjdQyGlNRU3tswZwtqOPx0qKrYAgzmxmafJc+dl6SlKoVcmNpkBEfLASwjvGmI+cm4/+fDvp/NmU\nZo8PBsaJyD6sZsFLsdraw53NC9A0P+80IM0Ys9r5/AOsJNGUP2uAUcBeY0ymMaYM+Ajrv4Gm/nlD\n9Z9tnf5985SkUGPJjabA2Zb+KrDNGPNMpZcWArc4f78F+KS+Y3MVY8zvjDGxxph4rM/1G2PMVGAZ\nMNG5W5O6ZgBjzBHgoIh0cm4aCWylCX/WTgeAgSIS6Pzv/efrbtKft1N1n+1C4GbnKKSBQF6lZqZz\n5jGT10TkCqy2ZxvwmjHmSTeHVOdEZAjwA7CJk+3rj2D1K8wH2gD7gUnGmNM7sRo9ERkO/NoYc5WI\ntMO6c4jEWsxpWm0KLjYmItILq3PdF9gDTMf6otekP2sReQK4AWu03U/A7Vht6E3m8xaRucBwrEqo\nR4HHgY+p4rN1JscXsJrRioDpxph15/3enpIUlFJK1cxTmo+UUkrVgiYFpZRSFTQpKKWUqqBJQSml\nVAVNCkoppSpoUlDKSUTKnSsA/vyos2JyIhJfueKlUg2VS1deU6qRKTbG9HJ3EEq5k94pKFUDEdkn\nIk+LyCYRWSMiFzm3x4vIN84a9ktFpI1ze3MRWSAiG52Pi52nsonIy861AL4UkQDn/veLtQZGiojM\nc9NlKgVoUlCqsoDTmo9uqPRanjGmO9bM0dnObf8C3jTG9ADeAZ53bn8e+M4Y0xOrHtEW5/YOwBxj\nTFcgF7jOuX0m0Nt5njtddXFK1YbOaFbKSUQKjDHBVWzfB1xqjNnjLDh4xBgTJSJZQEtjTJlz+2Fj\nTDMRyQRiK5dZcJYy/8q5QAoi8lvAxxjzZxFZDBRglTH42BhT4OJLVapaeqegVO2Yan4/F5Vr8ZRz\nsk/vSqyVAfsAaytV+1Sq3mlSUKp2bqj080fn7yuxKrMCTMUqRgjWUol3QcXa0WHVnVREvIA4Y8wy\n4LdAGHDG3YpS9UW/kSh1UoCIbKj0fLEx5udhqREikoL1bX+Kc9t9WCuf/QZrFbTpzu0PAC+JyC+w\n7gjuwlolrCo24G1n4hDgeeeymkq5hfYpKFUDZ59CkjEmy92xKOVq2nyklFKqgt4pKKWUqqB3Ckop\npSpoUlBKKVVBk4JSSqkKmhSUUkpV0KSglFKqgiYFpZRSFf4fPxabiOCgcW0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f35e0333e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('./%s/hist_0.json' % path_info['model_info']['model_dir'], 'r') as f:\n",
    "    history = json.load(f)\n",
    "    \n",
    "plt.plot(history['val_loss'], label='Validation')\n",
    "plt.plot(history['loss'], label='Training')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test evauation and train evauation is the numpy array of the shape (number of fold, number of evaluation measures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23001722, 0.884     , 0.93491125, 0.77777779, 0.85273278,\n",
       "        0.9723866 ],\n",
       "       [0.43164325, 0.80800003, 0.93678159, 0.5131579 , 0.6933375 ,\n",
       "        0.86021626],\n",
       "       [0.60169631, 0.71200001, 1.        , 0.        , 0.        ,\n",
       "        0.53889668]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25114068, 0.88800001, 0.91891891, 0.81896549, 0.86750382,\n",
       "        0.95781189],\n",
       "       [0.37452886, 0.84399998, 0.95970696, 0.53431374, 0.71608979,\n",
       "        0.89557296],\n",
       "       [0.62596804, 0.68133336, 1.        , 0.        , 0.        ,\n",
       "        0.52149367]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load the pre-trained network for training\n",
    "\n",
    "If you have pre-trianed model, you can use the pre-trained weight for next training. For using pre-trained weights, you have to use `warm_start` option in `training_inro` with addding the file path of the pre-trained weights in the `warm_start_model` option. Below is the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warm_start_network_info = {\n",
    "    'architecture_info': {\n",
    "        'batch_normalization': 'False',\n",
    "        'drop_out': '0',\n",
    "        'weight_initial': 'glorot_uniform',\n",
    "        'weight_l1_penalty':'0.01',\n",
    "        'weight_decay': 'phylogenetic_tree',\n",
    "    },\n",
    "    'model_info': {\n",
    "        'decay': '0.001',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'lr': '0.01',\n",
    "        'metrics': 'binary_accuracy, sensitivity, specificity, gmeasure, auc',\n",
    "        'network_class': 'DeepBiomeNetwork',\n",
    "        'normalizer': 'normalize_minmax',\n",
    "        'optimizer': 'adam',\n",
    "        'reader_class': 'MicroBiomeClassificationReader',\n",
    "        'taxa_selection_metrics': 'accuracy, sensitivity, specificity, gmeasure'\n",
    "    },\n",
    "    'training_info': {\n",
    "        'warm_start':'True',\n",
    "        'warm_start_model':'./example_result/weight.h5',\n",
    "        'batch_size': '200',\n",
    "        'epochs': '100'\n",
    "    },\n",
    "    'validation_info': {\n",
    "        'batch_size': 'None', \n",
    "        'validation_size': '0.2'\n",
    "    },\n",
    "    'test_info': {\n",
    "        'batch_size': 'None'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|deepbiome.py:100] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:137] -------1 simulation start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:148] Build network for 1 simulation\n",
      "[root    |INFO|build_network.py:505] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:506] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:510] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:511] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:518]      Genus: 48\n",
      "[root    |INFO|build_network.py:518]     Family: 40\n",
      "[root    |INFO|build_network.py:518]      Order: 23\n",
      "[root    |INFO|build_network.py:518]      Class: 17\n",
      "[root    |INFO|build_network.py:518]     Phylum: 9\n",
      "[root    |INFO|build_network.py:521] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:522] Phylogenetic_tree_dict info: ['Family', 'Class', 'Phylum', 'Number', 'Genus', 'Order']\n",
      "[root    |INFO|build_network.py:523] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:546] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:562] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:563] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:564] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:640] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|build_network.py:83] Load trained model weight at ./example_result/weight_0.h5 \n",
      "[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:158] 1 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:133] Training start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 0s 739us/step - loss: 0.2269 - binary_accuracy: 0.9117 - sensitivity: 0.9152 - specificity: 0.9014 - gmeasure: 0.9072 - auc: 0.9684 - val_loss: 0.3253 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8476 - val_specificity: 0.8889 - val_gmeasure: 0.8680 - val_auc: 0.9331\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2289 - binary_accuracy: 0.9000 - sensitivity: 0.9133 - specificity: 0.8750 - gmeasure: 0.8921 - auc: 0.9686 - val_loss: 0.3295 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8857 - val_specificity: 0.8222 - val_gmeasure: 0.8534 - val_auc: 0.9291\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2198 - binary_accuracy: 0.9133 - sensitivity: 0.9276 - specificity: 0.8858 - gmeasure: 0.9062 - auc: 0.9677 - val_loss: 0.3335 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8095 - val_specificity: 0.9111 - val_gmeasure: 0.8588 - val_auc: 0.9364\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2253 - binary_accuracy: 0.9200 - sensitivity: 0.8960 - specificity: 0.9729 - gmeasure: 0.9334 - auc: 0.9705 - val_loss: 0.3272 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8476 - val_specificity: 0.8667 - val_gmeasure: 0.8571 - val_auc: 0.9332\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2161 - binary_accuracy: 0.9083 - sensitivity: 0.9152 - specificity: 0.8949 - gmeasure: 0.9043 - auc: 0.9686 - val_loss: 0.3282 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8857 - val_specificity: 0.8222 - val_gmeasure: 0.8534 - val_auc: 0.9308\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2186 - binary_accuracy: 0.9050 - sensitivity: 0.9281 - specificity: 0.8537 - gmeasure: 0.8901 - auc: 0.9671 - val_loss: 0.3242 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9329\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2167 - binary_accuracy: 0.9183 - sensitivity: 0.9085 - specificity: 0.9416 - gmeasure: 0.9247 - auc: 0.9698 - val_loss: 0.3301 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8190 - val_specificity: 0.9111 - val_gmeasure: 0.8639 - val_auc: 0.9350\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2112 - binary_accuracy: 0.9250 - sensitivity: 0.9100 - specificity: 0.9594 - gmeasure: 0.9344 - auc: 0.9715 - val_loss: 0.3222 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8762 - val_specificity: 0.8444 - val_gmeasure: 0.8602 - val_auc: 0.9338\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2136 - binary_accuracy: 0.9050 - sensitivity: 0.9200 - specificity: 0.8715 - gmeasure: 0.8950 - auc: 0.9700 - val_loss: 0.3236 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8762 - val_specificity: 0.8222 - val_gmeasure: 0.8488 - val_auc: 0.9325\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2099 - binary_accuracy: 0.9133 - sensitivity: 0.9203 - specificity: 0.9005 - gmeasure: 0.9096 - auc: 0.9708 - val_loss: 0.3258 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8476 - val_specificity: 0.8667 - val_gmeasure: 0.8571 - val_auc: 0.9365\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.2094 - binary_accuracy: 0.9250 - sensitivity: 0.9057 - specificity: 0.9684 - gmeasure: 0.9364 - auc: 0.9721 - val_loss: 0.3241 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8476 - val_specificity: 0.8667 - val_gmeasure: 0.8571 - val_auc: 0.9363\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2089 - binary_accuracy: 0.9117 - sensitivity: 0.9134 - specificity: 0.9118 - gmeasure: 0.9122 - auc: 0.9720 - val_loss: 0.3223 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8762 - val_specificity: 0.8222 - val_gmeasure: 0.8488 - val_auc: 0.9333\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2053 - binary_accuracy: 0.9133 - sensitivity: 0.9205 - specificity: 0.8977 - gmeasure: 0.9090 - auc: 0.9707 - val_loss: 0.3210 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9359\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2042 - binary_accuracy: 0.9217 - sensitivity: 0.9085 - specificity: 0.9492 - gmeasure: 0.9286 - auc: 0.9708 - val_loss: 0.3239 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8476 - val_specificity: 0.8667 - val_gmeasure: 0.8571 - val_auc: 0.9361\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2025 - binary_accuracy: 0.9200 - sensitivity: 0.9105 - specificity: 0.9414 - gmeasure: 0.9257 - auc: 0.9728 - val_loss: 0.3202 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9346\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2014 - binary_accuracy: 0.9167 - sensitivity: 0.9198 - specificity: 0.9098 - gmeasure: 0.9147 - auc: 0.9716 - val_loss: 0.3207 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9350\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.2010 - binary_accuracy: 0.9250 - sensitivity: 0.9195 - specificity: 0.9389 - gmeasure: 0.9291 - auc: 0.9723 - val_loss: 0.3238 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8571 - val_specificity: 0.8667 - val_gmeasure: 0.8619 - val_auc: 0.9374\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1996 - binary_accuracy: 0.9233 - sensitivity: 0.9124 - specificity: 0.9462 - gmeasure: 0.9291 - auc: 0.9735 - val_loss: 0.3212 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9359\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1981 - binary_accuracy: 0.9233 - sensitivity: 0.9204 - specificity: 0.9306 - gmeasure: 0.9254 - auc: 0.9722 - val_loss: 0.3212 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9365\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1990 - binary_accuracy: 0.9233 - sensitivity: 0.9113 - specificity: 0.9503 - gmeasure: 0.9305 - auc: 0.9725 - val_loss: 0.3226 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8571 - val_specificity: 0.8667 - val_gmeasure: 0.8619 - val_auc: 0.9370\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1950 - binary_accuracy: 0.9250 - sensitivity: 0.9137 - specificity: 0.9522 - gmeasure: 0.9325 - auc: 0.9740 - val_loss: 0.3203 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8762 - val_specificity: 0.8444 - val_gmeasure: 0.8602 - val_auc: 0.9363\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1976 - binary_accuracy: 0.9133 - sensitivity: 0.9225 - specificity: 0.8931 - gmeasure: 0.9077 - auc: 0.9723 - val_loss: 0.3204 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8762 - val_specificity: 0.8444 - val_gmeasure: 0.8602 - val_auc: 0.9361\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.1955 - binary_accuracy: 0.9250 - sensitivity: 0.9163 - specificity: 0.9474 - gmeasure: 0.9314 - auc: 0.9741 - val_loss: 0.3254 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8286 - val_specificity: 0.9111 - val_gmeasure: 0.8689 - val_auc: 0.9384\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1949 - binary_accuracy: 0.9267 - sensitivity: 0.9084 - specificity: 0.9661 - gmeasure: 0.9368 - auc: 0.9748 - val_loss: 0.3204 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9363\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1940 - binary_accuracy: 0.9217 - sensitivity: 0.9235 - specificity: 0.9187 - gmeasure: 0.9209 - auc: 0.9728 - val_loss: 0.3205 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8762 - val_specificity: 0.8444 - val_gmeasure: 0.8602 - val_auc: 0.9361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1934 - binary_accuracy: 0.9217 - sensitivity: 0.9165 - specificity: 0.9374 - gmeasure: 0.9267 - auc: 0.9746 - val_loss: 0.3234 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8286 - val_specificity: 0.8889 - val_gmeasure: 0.8582 - val_auc: 0.9378\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1913 - binary_accuracy: 0.9250 - sensitivity: 0.9075 - specificity: 0.9579 - gmeasure: 0.9319 - auc: 0.9756 - val_loss: 0.3205 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9371\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1895 - binary_accuracy: 0.9217 - sensitivity: 0.9204 - specificity: 0.9242 - gmeasure: 0.9223 - auc: 0.9734 - val_loss: 0.3203 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8762 - val_specificity: 0.8444 - val_gmeasure: 0.8602 - val_auc: 0.9361\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1890 - binary_accuracy: 0.9200 - sensitivity: 0.9175 - specificity: 0.9234 - gmeasure: 0.9202 - auc: 0.9743 - val_loss: 0.3216 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8476 - val_specificity: 0.8889 - val_gmeasure: 0.8680 - val_auc: 0.9380\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.1882 - binary_accuracy: 0.9267 - sensitivity: 0.9100 - specificity: 0.9617 - gmeasure: 0.9351 - auc: 0.9756 - val_loss: 0.3206 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8571 - val_specificity: 0.8667 - val_gmeasure: 0.8619 - val_auc: 0.9384\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.1868 - binary_accuracy: 0.9283 - sensitivity: 0.9173 - specificity: 0.9509 - gmeasure: 0.9339 - auc: 0.9735 - val_loss: 0.3203 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9384\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1862 - binary_accuracy: 0.9217 - sensitivity: 0.9186 - specificity: 0.9291 - gmeasure: 0.9237 - auc: 0.9757 - val_loss: 0.3208 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9374\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.1852 - binary_accuracy: 0.9233 - sensitivity: 0.9105 - specificity: 0.9527 - gmeasure: 0.9313 - auc: 0.9760 - val_loss: 0.3213 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8571 - val_specificity: 0.8889 - val_gmeasure: 0.8729 - val_auc: 0.9374\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1843 - binary_accuracy: 0.9267 - sensitivity: 0.9186 - specificity: 0.9460 - gmeasure: 0.9321 - auc: 0.9750 - val_loss: 0.3201 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9382\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1860 - binary_accuracy: 0.9217 - sensitivity: 0.9096 - specificity: 0.9460 - gmeasure: 0.9270 - auc: 0.9780 - val_loss: 0.3199 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8571 - val_specificity: 0.8889 - val_gmeasure: 0.8729 - val_auc: 0.9395\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1842 - binary_accuracy: 0.9217 - sensitivity: 0.9175 - specificity: 0.9299 - gmeasure: 0.9234 - auc: 0.9765 - val_loss: 0.3205 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8857 - val_specificity: 0.8222 - val_gmeasure: 0.8534 - val_auc: 0.9374\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1825 - binary_accuracy: 0.9283 - sensitivity: 0.9204 - specificity: 0.9468 - gmeasure: 0.9334 - auc: 0.9762 - val_loss: 0.3230 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8286 - val_specificity: 0.9111 - val_gmeasure: 0.8689 - val_auc: 0.9399\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1849 - binary_accuracy: 0.9267 - sensitivity: 0.9072 - specificity: 0.9746 - gmeasure: 0.9398 - auc: 0.9790 - val_loss: 0.3190 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9386\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.1818 - binary_accuracy: 0.9233 - sensitivity: 0.9227 - specificity: 0.9247 - gmeasure: 0.9236 - auc: 0.9753 - val_loss: 0.3186 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8857 - val_specificity: 0.8444 - val_gmeasure: 0.8648 - val_auc: 0.9390\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.1797 - binary_accuracy: 0.9233 - sensitivity: 0.9159 - specificity: 0.9417 - gmeasure: 0.9281 - auc: 0.9786 - val_loss: 0.3218 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8286 - val_specificity: 0.9333 - val_gmeasure: 0.8794 - val_auc: 0.9414\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1812 - binary_accuracy: 0.9283 - sensitivity: 0.9084 - specificity: 0.9728 - gmeasure: 0.9400 - auc: 0.9785 - val_loss: 0.3180 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9399\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.1789 - binary_accuracy: 0.9283 - sensitivity: 0.9219 - specificity: 0.9409 - gmeasure: 0.9313 - auc: 0.9767 - val_loss: 0.3188 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8857 - val_specificity: 0.8444 - val_gmeasure: 0.8648 - val_auc: 0.9390\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.1779 - binary_accuracy: 0.9250 - sensitivity: 0.9121 - specificity: 0.9526 - gmeasure: 0.9321 - auc: 0.9767 - val_loss: 0.3218 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8381 - val_specificity: 0.9333 - val_gmeasure: 0.8844 - val_auc: 0.9416\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1775 - binary_accuracy: 0.9267 - sensitivity: 0.9081 - specificity: 0.9682 - gmeasure: 0.9377 - auc: 0.9781 - val_loss: 0.3191 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8857 - val_specificity: 0.8444 - val_gmeasure: 0.8648 - val_auc: 0.9399\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1770 - binary_accuracy: 0.9300 - sensitivity: 0.9204 - specificity: 0.9435 - gmeasure: 0.9313 - auc: 0.9766 - val_loss: 0.3170 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9401\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.1751 - binary_accuracy: 0.9317 - sensitivity: 0.9223 - specificity: 0.9514 - gmeasure: 0.9366 - auc: 0.9773 - val_loss: 0.3175 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8667 - val_specificity: 0.8889 - val_gmeasure: 0.8777 - val_auc: 0.9399\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.1746 - binary_accuracy: 0.9317 - sensitivity: 0.9153 - specificity: 0.9677 - gmeasure: 0.9411 - auc: 0.9792 - val_loss: 0.3192 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8571 - val_specificity: 0.8889 - val_gmeasure: 0.8729 - val_auc: 0.9412\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1733 - binary_accuracy: 0.9350 - sensitivity: 0.9198 - specificity: 0.9676 - gmeasure: 0.9433 - auc: 0.9785 - val_loss: 0.3182 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8762 - val_specificity: 0.8444 - val_gmeasure: 0.8602 - val_auc: 0.9399\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.1724 - binary_accuracy: 0.9333 - sensitivity: 0.9249 - specificity: 0.9517 - gmeasure: 0.9382 - auc: 0.9783 - val_loss: 0.3180 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8476 - val_specificity: 0.9111 - val_gmeasure: 0.8788 - val_auc: 0.9410\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.1717 - binary_accuracy: 0.9333 - sensitivity: 0.9129 - specificity: 0.9788 - gmeasure: 0.9452 - auc: 0.9797 - val_loss: 0.3173 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8571 - val_specificity: 0.8889 - val_gmeasure: 0.8729 - val_auc: 0.9401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1745 - binary_accuracy: 0.9250 - sensitivity: 0.9279 - specificity: 0.9214 - gmeasure: 0.9242 - auc: 0.9788 - val_loss: 0.3175 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8667 - val_specificity: 0.8444 - val_gmeasure: 0.8555 - val_auc: 0.9403\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1745 - binary_accuracy: 0.9350 - sensitivity: 0.9154 - specificity: 0.9784 - gmeasure: 0.9463 - auc: 0.9804 - val_loss: 0.3225 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8381 - val_specificity: 0.9333 - val_gmeasure: 0.8844 - val_auc: 0.9426\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1790 - binary_accuracy: 0.9283 - sensitivity: 0.9191 - specificity: 0.9557 - gmeasure: 0.9363 - auc: 0.9796 - val_loss: 0.3240 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8952 - val_specificity: 0.8222 - val_gmeasure: 0.8580 - val_auc: 0.9380\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1711 - binary_accuracy: 0.9317 - sensitivity: 0.9268 - specificity: 0.9381 - gmeasure: 0.9320 - auc: 0.9771 - val_loss: 0.3275 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8381 - val_specificity: 0.9333 - val_gmeasure: 0.8844 - val_auc: 0.9435\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1713 - binary_accuracy: 0.9300 - sensitivity: 0.9107 - specificity: 0.9728 - gmeasure: 0.9412 - auc: 0.9805 - val_loss: 0.3195 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8857 - val_specificity: 0.8222 - val_gmeasure: 0.8534 - val_auc: 0.9390\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1704 - binary_accuracy: 0.9300 - sensitivity: 0.9298 - specificity: 0.9297 - gmeasure: 0.9293 - auc: 0.9788 - val_loss: 0.3170 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8571 - val_specificity: 0.9111 - val_gmeasure: 0.8837 - val_auc: 0.9418\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1654 - binary_accuracy: 0.9367 - sensitivity: 0.9249 - specificity: 0.9624 - gmeasure: 0.9433 - auc: 0.9806 - val_loss: 0.3169 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8571 - val_specificity: 0.8444 - val_gmeasure: 0.8508 - val_auc: 0.9416\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1640 - binary_accuracy: 0.9400 - sensitivity: 0.9272 - specificity: 0.9665 - gmeasure: 0.9466 - auc: 0.9800 - val_loss: 0.3152 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8571 - val_specificity: 0.9111 - val_gmeasure: 0.8837 - val_auc: 0.9431\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1637 - binary_accuracy: 0.9417 - sensitivity: 0.9259 - specificity: 0.9774 - gmeasure: 0.9512 - auc: 0.9812 - val_loss: 0.3151 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8571 - val_specificity: 0.9111 - val_gmeasure: 0.8837 - val_auc: 0.9433\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1617 - binary_accuracy: 0.9417 - sensitivity: 0.9276 - specificity: 0.9728 - gmeasure: 0.9499 - auc: 0.9810 - val_loss: 0.3140 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8952 - val_specificity: 0.8444 - val_gmeasure: 0.8695 - val_auc: 0.9418\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1628 - binary_accuracy: 0.9333 - sensitivity: 0.9295 - specificity: 0.9434 - gmeasure: 0.9360 - auc: 0.9799 - val_loss: 0.3139 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8571 - val_specificity: 0.9111 - val_gmeasure: 0.8837 - val_auc: 0.9439\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1622 - binary_accuracy: 0.9367 - sensitivity: 0.9184 - specificity: 0.9787 - gmeasure: 0.9479 - auc: 0.9823 - val_loss: 0.3136 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8571 - val_specificity: 0.9111 - val_gmeasure: 0.8837 - val_auc: 0.9443\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1628 - binary_accuracy: 0.9317 - sensitivity: 0.9272 - specificity: 0.9425 - gmeasure: 0.9346 - auc: 0.9817 - val_loss: 0.3103 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8667 - val_specificity: 0.8889 - val_gmeasure: 0.8777 - val_auc: 0.9437\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1592 - binary_accuracy: 0.9400 - sensitivity: 0.9231 - specificity: 0.9783 - gmeasure: 0.9503 - auc: 0.9831 - val_loss: 0.3179 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8381 - val_specificity: 0.9333 - val_gmeasure: 0.8844 - val_auc: 0.9448\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1593 - binary_accuracy: 0.9367 - sensitivity: 0.9243 - specificity: 0.9656 - gmeasure: 0.9446 - auc: 0.9824 - val_loss: 0.3161 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9048 - val_specificity: 0.8222 - val_gmeasure: 0.8625 - val_auc: 0.9426\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.1575 - binary_accuracy: 0.9350 - sensitivity: 0.9345 - specificity: 0.9324 - gmeasure: 0.9331 - auc: 0.9822 - val_loss: 0.3129 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8667 - val_specificity: 0.9333 - val_gmeasure: 0.8994 - val_auc: 0.9452\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1552 - binary_accuracy: 0.9417 - sensitivity: 0.9256 - specificity: 0.9776 - gmeasure: 0.9512 - auc: 0.9843 - val_loss: 0.3084 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8762 - val_specificity: 0.8889 - val_gmeasure: 0.8825 - val_auc: 0.9454\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1532 - binary_accuracy: 0.9383 - sensitivity: 0.9287 - specificity: 0.9602 - gmeasure: 0.9443 - auc: 0.9815 - val_loss: 0.3086 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9452\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.1527 - binary_accuracy: 0.9383 - sensitivity: 0.9244 - specificity: 0.9672 - gmeasure: 0.9454 - auc: 0.9834 - val_loss: 0.3128 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8571 - val_specificity: 0.9333 - val_gmeasure: 0.8944 - val_auc: 0.9462\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1513 - binary_accuracy: 0.9400 - sensitivity: 0.9298 - specificity: 0.9629 - gmeasure: 0.9462 - auc: 0.9834 - val_loss: 0.3104 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9048 - val_specificity: 0.8667 - val_gmeasure: 0.8855 - val_auc: 0.9454\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1508 - binary_accuracy: 0.9383 - sensitivity: 0.9348 - specificity: 0.9475 - gmeasure: 0.9411 - auc: 0.9820 - val_loss: 0.3138 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8381 - val_specificity: 0.9333 - val_gmeasure: 0.8844 - val_auc: 0.9477\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.1525 - binary_accuracy: 0.9400 - sensitivity: 0.9203 - specificity: 0.9835 - gmeasure: 0.9514 - auc: 0.9855 - val_loss: 0.3082 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8762 - val_specificity: 0.9111 - val_gmeasure: 0.8935 - val_auc: 0.9469\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.1556 - binary_accuracy: 0.9333 - sensitivity: 0.9385 - specificity: 0.9271 - gmeasure: 0.9323 - auc: 0.9831 - val_loss: 0.3071 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.8952 - val_specificity: 0.8889 - val_gmeasure: 0.8921 - val_auc: 0.9467\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1509 - binary_accuracy: 0.9417 - sensitivity: 0.9200 - specificity: 0.9899 - gmeasure: 0.9542 - auc: 0.9852 - val_loss: 0.3168 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8476 - val_specificity: 0.9333 - val_gmeasure: 0.8894 - val_auc: 0.9484\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1463 - binary_accuracy: 0.9433 - sensitivity: 0.9342 - specificity: 0.9580 - gmeasure: 0.9458 - auc: 0.9859 - val_loss: 0.3148 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9143 - val_specificity: 0.8444 - val_gmeasure: 0.8787 - val_auc: 0.9454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.1478 - binary_accuracy: 0.9367 - sensitivity: 0.9369 - specificity: 0.9368 - gmeasure: 0.9368 - auc: 0.9827 - val_loss: 0.3102 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8571 - val_specificity: 0.9333 - val_gmeasure: 0.8944 - val_auc: 0.9484\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1450 - binary_accuracy: 0.9433 - sensitivity: 0.9299 - specificity: 0.9755 - gmeasure: 0.9524 - auc: 0.9857 - val_loss: 0.3065 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.8762 - val_specificity: 0.9333 - val_gmeasure: 0.9043 - val_auc: 0.9484\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1433 - binary_accuracy: 0.9467 - sensitivity: 0.9320 - specificity: 0.9781 - gmeasure: 0.9546 - auc: 0.9851 - val_loss: 0.3039 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8857 - val_specificity: 0.8889 - val_gmeasure: 0.8873 - val_auc: 0.9477\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 0s 72us/step - loss: 0.1419 - binary_accuracy: 0.9450 - sensitivity: 0.9312 - specificity: 0.9750 - gmeasure: 0.9528 - auc: 0.9847 - val_loss: 0.3044 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.8952 - val_specificity: 0.8889 - val_gmeasure: 0.8921 - val_auc: 0.9473\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.1420 - binary_accuracy: 0.9433 - sensitivity: 0.9309 - specificity: 0.9679 - gmeasure: 0.9491 - auc: 0.9845 - val_loss: 0.3044 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8762 - val_specificity: 0.9111 - val_gmeasure: 0.8935 - val_auc: 0.9486\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1394 - binary_accuracy: 0.9450 - sensitivity: 0.9343 - specificity: 0.9680 - gmeasure: 0.9510 - auc: 0.9838 - val_loss: 0.3045 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8857 - val_specificity: 0.8889 - val_gmeasure: 0.8873 - val_auc: 0.9479\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1387 - binary_accuracy: 0.9450 - sensitivity: 0.9322 - specificity: 0.9728 - gmeasure: 0.9522 - auc: 0.9858 - val_loss: 0.3045 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.8762 - val_specificity: 0.9333 - val_gmeasure: 0.9043 - val_auc: 0.9488\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.1372 - binary_accuracy: 0.9467 - sensitivity: 0.9351 - specificity: 0.9734 - gmeasure: 0.9540 - auc: 0.9859 - val_loss: 0.3052 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.8952 - val_specificity: 0.8889 - val_gmeasure: 0.8921 - val_auc: 0.9473\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1379 - binary_accuracy: 0.9467 - sensitivity: 0.9375 - specificity: 0.9687 - gmeasure: 0.9529 - auc: 0.9854 - val_loss: 0.3091 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.8762 - val_specificity: 0.9333 - val_gmeasure: 0.9043 - val_auc: 0.9492\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.1366 - binary_accuracy: 0.9467 - sensitivity: 0.9323 - specificity: 0.9787 - gmeasure: 0.9552 - auc: 0.9874 - val_loss: 0.3104 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8667 - val_specificity: 0.9333 - val_gmeasure: 0.8994 - val_auc: 0.9492\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.1357 - binary_accuracy: 0.9483 - sensitivity: 0.9346 - specificity: 0.9786 - gmeasure: 0.9563 - auc: 0.9866 - val_loss: 0.3074 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8857 - val_specificity: 0.8889 - val_gmeasure: 0.8873 - val_auc: 0.9486\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1340 - binary_accuracy: 0.9500 - sensitivity: 0.9391 - specificity: 0.9737 - gmeasure: 0.9563 - auc: 0.9857 - val_loss: 0.3038 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.8762 - val_specificity: 0.9333 - val_gmeasure: 0.9043 - val_auc: 0.9496\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.1364 - binary_accuracy: 0.9500 - sensitivity: 0.9374 - specificity: 0.9790 - gmeasure: 0.9578 - auc: 0.9870 - val_loss: 0.3008 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8857 - val_specificity: 0.8889 - val_gmeasure: 0.8873 - val_auc: 0.9496\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1334 - binary_accuracy: 0.9533 - sensitivity: 0.9417 - specificity: 0.9794 - gmeasure: 0.9603 - auc: 0.9870 - val_loss: 0.3032 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8762 - val_specificity: 0.9111 - val_gmeasure: 0.8935 - val_auc: 0.9505\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1313 - binary_accuracy: 0.9517 - sensitivity: 0.9394 - specificity: 0.9785 - gmeasure: 0.9587 - auc: 0.9869 - val_loss: 0.3046 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8857 - val_specificity: 0.8889 - val_gmeasure: 0.8873 - val_auc: 0.9501\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.1301 - binary_accuracy: 0.9483 - sensitivity: 0.9361 - specificity: 0.9714 - gmeasure: 0.9535 - auc: 0.9868 - val_loss: 0.3036 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8762 - val_specificity: 0.9111 - val_gmeasure: 0.8935 - val_auc: 0.9509\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1310 - binary_accuracy: 0.9467 - sensitivity: 0.9391 - specificity: 0.9626 - gmeasure: 0.9506 - auc: 0.9866 - val_loss: 0.3021 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8857 - val_specificity: 0.8889 - val_gmeasure: 0.8873 - val_auc: 0.9503\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1271 - binary_accuracy: 0.9533 - sensitivity: 0.9424 - specificity: 0.9798 - gmeasure: 0.9607 - auc: 0.9870 - val_loss: 0.3069 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8667 - val_specificity: 0.9333 - val_gmeasure: 0.8994 - val_auc: 0.9520\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.1288 - binary_accuracy: 0.9500 - sensitivity: 0.9370 - specificity: 0.9780 - gmeasure: 0.9572 - auc: 0.9881 - val_loss: 0.3009 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8857 - val_specificity: 0.8889 - val_gmeasure: 0.8873 - val_auc: 0.9513\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1279 - binary_accuracy: 0.9533 - sensitivity: 0.9416 - specificity: 0.9788 - gmeasure: 0.9599 - auc: 0.9866 - val_loss: 0.3001 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.8762 - val_specificity: 0.9333 - val_gmeasure: 0.9043 - val_auc: 0.9515\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.1266 - binary_accuracy: 0.9533 - sensitivity: 0.9410 - specificity: 0.9801 - gmeasure: 0.9603 - auc: 0.9876 - val_loss: 0.3002 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8762 - val_specificity: 0.8889 - val_gmeasure: 0.8825 - val_auc: 0.9526\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1250 - binary_accuracy: 0.9517 - sensitivity: 0.9380 - specificity: 0.9854 - gmeasure: 0.9613 - auc: 0.9883 - val_loss: 0.2984 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8762 - val_specificity: 0.8889 - val_gmeasure: 0.8825 - val_auc: 0.9522\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1240 - binary_accuracy: 0.9517 - sensitivity: 0.9427 - specificity: 0.9731 - gmeasure: 0.9577 - auc: 0.9877 - val_loss: 0.2997 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.8762 - val_specificity: 0.9333 - val_gmeasure: 0.9043 - val_auc: 0.9522\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1254 - binary_accuracy: 0.9517 - sensitivity: 0.9370 - specificity: 0.9833 - gmeasure: 0.9599 - auc: 0.9879 - val_loss: 0.2982 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8762 - val_specificity: 0.9111 - val_gmeasure: 0.8935 - val_auc: 0.9528\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.1218 - binary_accuracy: 0.9550 - sensitivity: 0.9458 - specificity: 0.9736 - gmeasure: 0.9596 - auc: 0.9885 - val_loss: 0.2986 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9048 - val_specificity: 0.8889 - val_gmeasure: 0.8968 - val_auc: 0.9517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:143] Training end with time 5.879864931106567!\n",
      "[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_0.h5 \n",
      "[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_0.h5\n",
      "[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_0.json\n",
      "[root    |INFO|build_network.py:169] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "750/750 [==============================] - 0s 7us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:174] Evaluation end with time 0.010180473327636719!\n",
      "[root    |INFO|build_network.py:175] Evaluation: [0.15661081671714783, 0.9440000057220459, 0.9382239580154419, 0.9568965435028076, 0.9475142359733582, 0.9810194969177246]\n",
      "[root    |INFO|build_network.py:169] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 22us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:174] Evaluation end with time 0.010858297348022461!\n",
      "[root    |INFO|build_network.py:175] Evaluation: [0.14900001883506775, 0.9480000138282776, 0.9704142212867737, 0.9012345671653748, 0.9351849555969238, 0.9853166937828064]\n",
      "[root    |INFO|deepbiome.py:179] Compute time : 7.261319160461426\n",
      "[root    |INFO|deepbiome.py:180] 1 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:137] -------2 simulation start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:148] Build network for 2 simulation\n",
      "[root    |INFO|build_network.py:505] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:506] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:510] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:511] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:518]      Genus: 48\n",
      "[root    |INFO|build_network.py:518]     Family: 40\n",
      "[root    |INFO|build_network.py:518]      Order: 23\n",
      "[root    |INFO|build_network.py:518]      Class: 17\n",
      "[root    |INFO|build_network.py:518]     Phylum: 9\n",
      "[root    |INFO|build_network.py:521] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:522] Phylogenetic_tree_dict info: ['Family', 'Class', 'Phylum', 'Number', 'Genus', 'Order']\n",
      "[root    |INFO|build_network.py:523] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:546] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:562] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:563] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:564] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:640] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|build_network.py:83] Load trained model weight at ./example_result/weight_1.h5 \n",
      "[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:158] 2 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:133] Training start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 0s 686us/step - loss: 0.3767 - binary_accuracy: 0.8333 - sensitivity: 0.9704 - specificity: 0.4761 - gmeasure: 0.6727 - auc: 0.9018 - val_loss: 0.3953 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9725 - val_specificity: 0.4878 - val_gmeasure: 0.6888 - val_auc: 0.8693\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.3705 - binary_accuracy: 0.8467 - sensitivity: 0.9448 - specificity: 0.5815 - gmeasure: 0.7395 - auc: 0.9018 - val_loss: 0.3981 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9450 - val_specificity: 0.5610 - val_gmeasure: 0.7281 - val_auc: 0.8682\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.3702 - binary_accuracy: 0.8450 - sensitivity: 0.9364 - specificity: 0.5987 - gmeasure: 0.7485 - auc: 0.9026 - val_loss: 0.3937 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9725 - val_specificity: 0.4878 - val_gmeasure: 0.6888 - val_auc: 0.8687\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.3654 - binary_accuracy: 0.8500 - sensitivity: 0.9724 - specificity: 0.5217 - gmeasure: 0.7121 - auc: 0.9018 - val_loss: 0.4023 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9908 - val_specificity: 0.4390 - val_gmeasure: 0.6595 - val_auc: 0.8700\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.3675 - binary_accuracy: 0.8450 - sensitivity: 0.9747 - specificity: 0.4967 - gmeasure: 0.6957 - auc: 0.9029 - val_loss: 0.3929 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9817 - val_specificity: 0.4878 - val_gmeasure: 0.6920 - val_auc: 0.8704\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.3643 - binary_accuracy: 0.8450 - sensitivity: 0.9570 - specificity: 0.5484 - gmeasure: 0.7241 - auc: 0.9027 - val_loss: 0.3918 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8684\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.3650 - binary_accuracy: 0.8383 - sensitivity: 0.9370 - specificity: 0.5841 - gmeasure: 0.7360 - auc: 0.9014 - val_loss: 0.3894 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9633 - val_specificity: 0.5122 - val_gmeasure: 0.7024 - val_auc: 0.8711\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.3594 - binary_accuracy: 0.8417 - sensitivity: 0.9519 - specificity: 0.5445 - gmeasure: 0.7196 - auc: 0.9050 - val_loss: 0.3903 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9725 - val_specificity: 0.4878 - val_gmeasure: 0.6888 - val_auc: 0.8718\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 0s 56us/step - loss: 0.3591 - binary_accuracy: 0.8467 - sensitivity: 0.9635 - specificity: 0.5322 - gmeasure: 0.7150 - auc: 0.9053 - val_loss: 0.3902 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9725 - val_specificity: 0.4878 - val_gmeasure: 0.6888 - val_auc: 0.8720\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.3570 - binary_accuracy: 0.8450 - sensitivity: 0.9542 - specificity: 0.5519 - gmeasure: 0.7255 - auc: 0.9056 - val_loss: 0.3869 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9633 - val_specificity: 0.5366 - val_gmeasure: 0.7190 - val_auc: 0.8722\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.3570 - binary_accuracy: 0.8467 - sensitivity: 0.9432 - specificity: 0.5918 - gmeasure: 0.7453 - auc: 0.9056 - val_loss: 0.3859 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8725\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.3542 - binary_accuracy: 0.8467 - sensitivity: 0.9435 - specificity: 0.5860 - gmeasure: 0.7430 - auc: 0.9045 - val_loss: 0.3865 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9633 - val_specificity: 0.5122 - val_gmeasure: 0.7024 - val_auc: 0.8738\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.3559 - binary_accuracy: 0.8417 - sensitivity: 0.9572 - specificity: 0.5358 - gmeasure: 0.7151 - auc: 0.9082 - val_loss: 0.3884 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9725 - val_specificity: 0.4878 - val_gmeasure: 0.6888 - val_auc: 0.8747\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.3512 - binary_accuracy: 0.8483 - sensitivity: 0.9589 - specificity: 0.5509 - gmeasure: 0.7253 - auc: 0.9057 - val_loss: 0.3831 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8749\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.3506 - binary_accuracy: 0.8467 - sensitivity: 0.9375 - specificity: 0.6022 - gmeasure: 0.7514 - auc: 0.9044 - val_loss: 0.3822 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8760\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.3494 - binary_accuracy: 0.8467 - sensitivity: 0.9383 - specificity: 0.6019 - gmeasure: 0.7513 - auc: 0.9075 - val_loss: 0.3816 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8767\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.3475 - binary_accuracy: 0.8467 - sensitivity: 0.9523 - specificity: 0.5626 - gmeasure: 0.7310 - auc: 0.9079 - val_loss: 0.3835 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9633 - val_specificity: 0.5122 - val_gmeasure: 0.7024 - val_auc: 0.8767\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.3470 - binary_accuracy: 0.8433 - sensitivity: 0.9486 - specificity: 0.5620 - gmeasure: 0.7296 - auc: 0.9080 - val_loss: 0.3797 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8783\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.3445 - binary_accuracy: 0.8450 - sensitivity: 0.9426 - specificity: 0.5794 - gmeasure: 0.7380 - auc: 0.9065 - val_loss: 0.3791 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8796\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.3438 - binary_accuracy: 0.8450 - sensitivity: 0.9427 - specificity: 0.5775 - gmeasure: 0.7365 - auc: 0.9096 - val_loss: 0.3792 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8803\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.3420 - binary_accuracy: 0.8450 - sensitivity: 0.9450 - specificity: 0.5788 - gmeasure: 0.7394 - auc: 0.9107 - val_loss: 0.3774 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8812\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.3405 - binary_accuracy: 0.8450 - sensitivity: 0.9359 - specificity: 0.5984 - gmeasure: 0.7472 - auc: 0.9094 - val_loss: 0.3758 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8821\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.3391 - binary_accuracy: 0.8433 - sensitivity: 0.9336 - specificity: 0.6013 - gmeasure: 0.7492 - auc: 0.9117 - val_loss: 0.3765 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8823\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.3376 - binary_accuracy: 0.8467 - sensitivity: 0.9477 - specificity: 0.5753 - gmeasure: 0.7381 - auc: 0.9120 - val_loss: 0.3761 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9541 - val_specificity: 0.5122 - val_gmeasure: 0.6991 - val_auc: 0.8830\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.3361 - binary_accuracy: 0.8433 - sensitivity: 0.9356 - specificity: 0.5896 - gmeasure: 0.7408 - auc: 0.9136 - val_loss: 0.3733 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.3354 - binary_accuracy: 0.8467 - sensitivity: 0.9411 - specificity: 0.5954 - gmeasure: 0.7485 - auc: 0.9137 - val_loss: 0.3726 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8841\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.3334 - binary_accuracy: 0.8467 - sensitivity: 0.9384 - specificity: 0.6031 - gmeasure: 0.7509 - auc: 0.9145 - val_loss: 0.3713 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8859\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.3322 - binary_accuracy: 0.8483 - sensitivity: 0.9410 - specificity: 0.6031 - gmeasure: 0.7527 - auc: 0.9165 - val_loss: 0.3707 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8861\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.3311 - binary_accuracy: 0.8433 - sensitivity: 0.9279 - specificity: 0.6168 - gmeasure: 0.7556 - auc: 0.9127 - val_loss: 0.3689 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9541 - val_specificity: 0.5610 - val_gmeasure: 0.7316 - val_auc: 0.8877\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.3290 - binary_accuracy: 0.8483 - sensitivity: 0.9356 - specificity: 0.6106 - gmeasure: 0.7543 - auc: 0.9176 - val_loss: 0.3713 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9541 - val_specificity: 0.5122 - val_gmeasure: 0.6991 - val_auc: 0.8881\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.3287 - binary_accuracy: 0.8533 - sensitivity: 0.9453 - specificity: 0.6103 - gmeasure: 0.7585 - auc: 0.9177 - val_loss: 0.3681 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8890\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.3267 - binary_accuracy: 0.8533 - sensitivity: 0.9475 - specificity: 0.6040 - gmeasure: 0.7553 - auc: 0.9170 - val_loss: 0.3674 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8904\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.3241 - binary_accuracy: 0.8433 - sensitivity: 0.9312 - specificity: 0.6057 - gmeasure: 0.7506 - auc: 0.9184 - val_loss: 0.3642 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9541 - val_specificity: 0.5610 - val_gmeasure: 0.7316 - val_auc: 0.8913\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.3237 - binary_accuracy: 0.8533 - sensitivity: 0.9361 - specificity: 0.6342 - gmeasure: 0.7703 - auc: 0.9181 - val_loss: 0.3648 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8924\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.3221 - binary_accuracy: 0.8533 - sensitivity: 0.9478 - specificity: 0.6006 - gmeasure: 0.7545 - auc: 0.9201 - val_loss: 0.3642 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8939\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.3202 - binary_accuracy: 0.8467 - sensitivity: 0.9341 - specificity: 0.6137 - gmeasure: 0.7571 - auc: 0.9194 - val_loss: 0.3614 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9541 - val_specificity: 0.5610 - val_gmeasure: 0.7316 - val_auc: 0.8939\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.3188 - binary_accuracy: 0.8517 - sensitivity: 0.9312 - specificity: 0.6370 - gmeasure: 0.7693 - auc: 0.9217 - val_loss: 0.3621 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9541 - val_specificity: 0.5610 - val_gmeasure: 0.7316 - val_auc: 0.8946\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.3180 - binary_accuracy: 0.8533 - sensitivity: 0.9475 - specificity: 0.6011 - gmeasure: 0.7543 - auc: 0.9216 - val_loss: 0.3629 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8946\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.3160 - binary_accuracy: 0.8533 - sensitivity: 0.9405 - specificity: 0.6199 - gmeasure: 0.7636 - auc: 0.9237 - val_loss: 0.3578 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9541 - val_specificity: 0.5854 - val_gmeasure: 0.7473 - val_auc: 0.8966\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.3154 - binary_accuracy: 0.8567 - sensitivity: 0.9292 - specificity: 0.6642 - gmeasure: 0.7847 - auc: 0.9240 - val_loss: 0.3588 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8975\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.3172 - binary_accuracy: 0.8533 - sensitivity: 0.9495 - specificity: 0.5984 - gmeasure: 0.7513 - auc: 0.9257 - val_loss: 0.3624 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9541 - val_specificity: 0.5122 - val_gmeasure: 0.6991 - val_auc: 0.8982\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.3145 - binary_accuracy: 0.8583 - sensitivity: 0.9362 - specificity: 0.6546 - gmeasure: 0.7810 - auc: 0.9238 - val_loss: 0.3537 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9541 - val_specificity: 0.6098 - val_gmeasure: 0.7627 - val_auc: 0.8961\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.3122 - binary_accuracy: 0.8517 - sensitivity: 0.9155 - specificity: 0.6793 - gmeasure: 0.7878 - auc: 0.9232 - val_loss: 0.3599 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.9541 - val_specificity: 0.5366 - val_gmeasure: 0.7155 - val_auc: 0.8989\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.3114 - binary_accuracy: 0.8567 - sensitivity: 0.9544 - specificity: 0.5960 - gmeasure: 0.7535 - auc: 0.9250 - val_loss: 0.3602 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9541 - val_specificity: 0.5610 - val_gmeasure: 0.7316 - val_auc: 0.8991\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.3100 - binary_accuracy: 0.8567 - sensitivity: 0.9362 - specificity: 0.6454 - gmeasure: 0.7759 - auc: 0.9282 - val_loss: 0.3525 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9541 - val_specificity: 0.6098 - val_gmeasure: 0.7627 - val_auc: 0.8993\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.3059 - binary_accuracy: 0.8667 - sensitivity: 0.9261 - specificity: 0.7006 - gmeasure: 0.8040 - auc: 0.9265 - val_loss: 0.3573 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9541 - val_specificity: 0.5610 - val_gmeasure: 0.7316 - val_auc: 0.8998\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.3084 - binary_accuracy: 0.8550 - sensitivity: 0.9519 - specificity: 0.5991 - gmeasure: 0.7540 - auc: 0.9275 - val_loss: 0.3586 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9541 - val_specificity: 0.5610 - val_gmeasure: 0.7316 - val_auc: 0.9013\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.3033 - binary_accuracy: 0.8650 - sensitivity: 0.9405 - specificity: 0.6587 - gmeasure: 0.7836 - auc: 0.9284 - val_loss: 0.3505 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9541 - val_specificity: 0.6098 - val_gmeasure: 0.7627 - val_auc: 0.8985\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.3039 - binary_accuracy: 0.8717 - sensitivity: 0.9199 - specificity: 0.7422 - gmeasure: 0.8258 - auc: 0.9293 - val_loss: 0.3537 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9541 - val_specificity: 0.5610 - val_gmeasure: 0.7316 - val_auc: 0.9022\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 0s 71us/step - loss: 0.3049 - binary_accuracy: 0.8567 - sensitivity: 0.9539 - specificity: 0.5951 - gmeasure: 0.7524 - auc: 0.9310 - val_loss: 0.3616 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9633 - val_specificity: 0.5610 - val_gmeasure: 0.7351 - val_auc: 0.9036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2977 - binary_accuracy: 0.8600 - sensitivity: 0.9491 - specificity: 0.6219 - gmeasure: 0.7683 - auc: 0.9314 - val_loss: 0.3465 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9541 - val_specificity: 0.6098 - val_gmeasure: 0.7627 - val_auc: 0.9017\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.3010 - binary_accuracy: 0.8700 - sensitivity: 0.9185 - specificity: 0.7479 - gmeasure: 0.8284 - auc: 0.9326 - val_loss: 0.3470 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9541 - val_specificity: 0.5854 - val_gmeasure: 0.7473 - val_auc: 0.9014\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2958 - binary_accuracy: 0.8617 - sensitivity: 0.9376 - specificity: 0.6561 - gmeasure: 0.7839 - auc: 0.9317 - val_loss: 0.3558 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9541 - val_specificity: 0.5610 - val_gmeasure: 0.7316 - val_auc: 0.9040\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2958 - binary_accuracy: 0.8583 - sensitivity: 0.9497 - specificity: 0.6139 - gmeasure: 0.7632 - auc: 0.9315 - val_loss: 0.3491 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9541 - val_specificity: 0.5610 - val_gmeasure: 0.7316 - val_auc: 0.9042\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2939 - binary_accuracy: 0.8767 - sensitivity: 0.9360 - specificity: 0.7188 - gmeasure: 0.8186 - auc: 0.9341 - val_loss: 0.3446 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9541 - val_specificity: 0.6098 - val_gmeasure: 0.7627 - val_auc: 0.9027\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2936 - binary_accuracy: 0.8717 - sensitivity: 0.9288 - specificity: 0.7250 - gmeasure: 0.8199 - auc: 0.9357 - val_loss: 0.3540 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9541 - val_specificity: 0.5610 - val_gmeasure: 0.7316 - val_auc: 0.9047\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.2936 - binary_accuracy: 0.8600 - sensitivity: 0.9522 - specificity: 0.6076 - gmeasure: 0.7600 - auc: 0.9331 - val_loss: 0.3477 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9541 - val_specificity: 0.5610 - val_gmeasure: 0.7316 - val_auc: 0.9049\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.2904 - binary_accuracy: 0.8767 - sensitivity: 0.9259 - specificity: 0.7377 - gmeasure: 0.8256 - auc: 0.9375 - val_loss: 0.3425 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9541 - val_specificity: 0.5854 - val_gmeasure: 0.7473 - val_auc: 0.9047\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.2882 - binary_accuracy: 0.8717 - sensitivity: 0.9334 - specificity: 0.7116 - gmeasure: 0.8141 - auc: 0.9362 - val_loss: 0.3493 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9633 - val_specificity: 0.5610 - val_gmeasure: 0.7351 - val_auc: 0.9056\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2865 - binary_accuracy: 0.8650 - sensitivity: 0.9476 - specificity: 0.6427 - gmeasure: 0.7799 - auc: 0.9371 - val_loss: 0.3412 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9633 - val_specificity: 0.5610 - val_gmeasure: 0.7351 - val_auc: 0.9067\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.2845 - binary_accuracy: 0.8767 - sensitivity: 0.9335 - specificity: 0.7213 - gmeasure: 0.8188 - auc: 0.9357 - val_loss: 0.3352 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.9633 - val_specificity: 0.6341 - val_gmeasure: 0.7816 - val_auc: 0.9076\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2900 - binary_accuracy: 0.8583 - sensitivity: 0.9411 - specificity: 0.6466 - gmeasure: 0.7760 - auc: 0.9416 - val_loss: 0.3359 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9633 - val_specificity: 0.5610 - val_gmeasure: 0.7351 - val_auc: 0.9083\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2832 - binary_accuracy: 0.8767 - sensitivity: 0.9336 - specificity: 0.7239 - gmeasure: 0.8212 - auc: 0.9394 - val_loss: 0.3292 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.9450 - val_specificity: 0.6829 - val_gmeasure: 0.8033 - val_auc: 0.9078\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.2827 - binary_accuracy: 0.8800 - sensitivity: 0.9334 - specificity: 0.7335 - gmeasure: 0.8267 - auc: 0.9405 - val_loss: 0.3411 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9725 - val_specificity: 0.5366 - val_gmeasure: 0.7224 - val_auc: 0.9109\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2824 - binary_accuracy: 0.8717 - sensitivity: 0.9657 - specificity: 0.6189 - gmeasure: 0.7731 - auc: 0.9398 - val_loss: 0.3271 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9633 - val_specificity: 0.6098 - val_gmeasure: 0.7664 - val_auc: 0.9118\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2832 - binary_accuracy: 0.8833 - sensitivity: 0.9213 - specificity: 0.7811 - gmeasure: 0.8463 - auc: 0.9406 - val_loss: 0.3210 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9541 - val_specificity: 0.6829 - val_gmeasure: 0.8072 - val_auc: 0.9132\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2794 - binary_accuracy: 0.8850 - sensitivity: 0.9455 - specificity: 0.7246 - gmeasure: 0.8276 - auc: 0.9419 - val_loss: 0.3349 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9725 - val_specificity: 0.5366 - val_gmeasure: 0.7224 - val_auc: 0.9147\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.2819 - binary_accuracy: 0.8800 - sensitivity: 0.9416 - specificity: 0.7344 - gmeasure: 0.8293 - auc: 0.9441 - val_loss: 0.3183 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9541 - val_specificity: 0.6829 - val_gmeasure: 0.8072 - val_auc: 0.9154\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2734 - binary_accuracy: 0.8833 - sensitivity: 0.9405 - specificity: 0.7297 - gmeasure: 0.8279 - auc: 0.9433 - val_loss: 0.3245 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9725 - val_specificity: 0.5854 - val_gmeasure: 0.7545 - val_auc: 0.9156\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2743 - binary_accuracy: 0.8833 - sensitivity: 0.9485 - specificity: 0.7127 - gmeasure: 0.8207 - auc: 0.9466 - val_loss: 0.3171 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9541 - val_specificity: 0.6829 - val_gmeasure: 0.8072 - val_auc: 0.9168\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2686 - binary_accuracy: 0.8967 - sensitivity: 0.9496 - specificity: 0.7551 - gmeasure: 0.8466 - auc: 0.9453 - val_loss: 0.3213 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9633 - val_specificity: 0.6098 - val_gmeasure: 0.7664 - val_auc: 0.9163\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2695 - binary_accuracy: 0.8900 - sensitivity: 0.9536 - specificity: 0.7273 - gmeasure: 0.8322 - auc: 0.9467 - val_loss: 0.3168 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9633 - val_specificity: 0.6585 - val_gmeasure: 0.7965 - val_auc: 0.9161\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.2670 - binary_accuracy: 0.8883 - sensitivity: 0.9452 - specificity: 0.7408 - gmeasure: 0.8364 - auc: 0.9455 - val_loss: 0.3168 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9633 - val_specificity: 0.6585 - val_gmeasure: 0.7965 - val_auc: 0.9168\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2649 - binary_accuracy: 0.8917 - sensitivity: 0.9480 - specificity: 0.7467 - gmeasure: 0.8410 - auc: 0.9455 - val_loss: 0.3154 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9633 - val_specificity: 0.6585 - val_gmeasure: 0.7965 - val_auc: 0.9170\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2634 - binary_accuracy: 0.8917 - sensitivity: 0.9383 - specificity: 0.7681 - gmeasure: 0.8489 - auc: 0.9486 - val_loss: 0.3142 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.9541 - val_specificity: 0.6585 - val_gmeasure: 0.7927 - val_auc: 0.9168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2619 - binary_accuracy: 0.8917 - sensitivity: 0.9452 - specificity: 0.7491 - gmeasure: 0.8410 - auc: 0.9485 - val_loss: 0.3178 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9725 - val_specificity: 0.6341 - val_gmeasure: 0.7853 - val_auc: 0.9183\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2611 - binary_accuracy: 0.8950 - sensitivity: 0.9566 - specificity: 0.7294 - gmeasure: 0.8353 - auc: 0.9478 - val_loss: 0.3117 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9633 - val_specificity: 0.6585 - val_gmeasure: 0.7965 - val_auc: 0.9190\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2590 - binary_accuracy: 0.8933 - sensitivity: 0.9411 - specificity: 0.7628 - gmeasure: 0.8471 - auc: 0.9477 - val_loss: 0.3097 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9633 - val_specificity: 0.6585 - val_gmeasure: 0.7965 - val_auc: 0.9197\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.2586 - binary_accuracy: 0.8950 - sensitivity: 0.9432 - specificity: 0.7669 - gmeasure: 0.8502 - auc: 0.9481 - val_loss: 0.3118 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9725 - val_specificity: 0.6585 - val_gmeasure: 0.8003 - val_auc: 0.9203\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2578 - binary_accuracy: 0.8967 - sensitivity: 0.9544 - specificity: 0.7429 - gmeasure: 0.8416 - auc: 0.9504 - val_loss: 0.3078 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9633 - val_specificity: 0.6585 - val_gmeasure: 0.7965 - val_auc: 0.9199\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2615 - binary_accuracy: 0.8983 - sensitivity: 0.9384 - specificity: 0.7930 - gmeasure: 0.8620 - auc: 0.9522 - val_loss: 0.3118 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9725 - val_specificity: 0.6585 - val_gmeasure: 0.8003 - val_auc: 0.9208\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2585 - binary_accuracy: 0.8950 - sensitivity: 0.9589 - specificity: 0.7297 - gmeasure: 0.8354 - auc: 0.9521 - val_loss: 0.3097 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9725 - val_specificity: 0.6585 - val_gmeasure: 0.8003 - val_auc: 0.9208\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 0s 57us/step - loss: 0.2536 - binary_accuracy: 0.8950 - sensitivity: 0.9389 - specificity: 0.7759 - gmeasure: 0.8534 - auc: 0.9503 - val_loss: 0.3045 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9633 - val_specificity: 0.6585 - val_gmeasure: 0.7965 - val_auc: 0.9215\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.2527 - binary_accuracy: 0.8933 - sensitivity: 0.9404 - specificity: 0.7663 - gmeasure: 0.8485 - auc: 0.9519 - val_loss: 0.3140 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9725 - val_specificity: 0.6585 - val_gmeasure: 0.8003 - val_auc: 0.9221\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2545 - binary_accuracy: 0.8983 - sensitivity: 0.9482 - specificity: 0.7716 - gmeasure: 0.8539 - auc: 0.9531 - val_loss: 0.3034 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9633 - val_specificity: 0.6585 - val_gmeasure: 0.7965 - val_auc: 0.9226\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.2515 - binary_accuracy: 0.9050 - sensitivity: 0.9518 - specificity: 0.7782 - gmeasure: 0.8600 - auc: 0.9530 - val_loss: 0.3105 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9725 - val_specificity: 0.6585 - val_gmeasure: 0.8003 - val_auc: 0.9230\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2487 - binary_accuracy: 0.8983 - sensitivity: 0.9518 - specificity: 0.7551 - gmeasure: 0.8477 - auc: 0.9527 - val_loss: 0.3013 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9633 - val_specificity: 0.6829 - val_gmeasure: 0.8111 - val_auc: 0.9239\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2482 - binary_accuracy: 0.9033 - sensitivity: 0.9407 - specificity: 0.8000 - gmeasure: 0.8671 - auc: 0.9536 - val_loss: 0.3075 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9725 - val_specificity: 0.6585 - val_gmeasure: 0.8003 - val_auc: 0.9246\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2461 - binary_accuracy: 0.9050 - sensitivity: 0.9520 - specificity: 0.7796 - gmeasure: 0.8614 - auc: 0.9551 - val_loss: 0.3026 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9725 - val_specificity: 0.6585 - val_gmeasure: 0.8003 - val_auc: 0.9246\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2485 - binary_accuracy: 0.9017 - sensitivity: 0.9577 - specificity: 0.7641 - gmeasure: 0.8542 - auc: 0.9554 - val_loss: 0.3007 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9725 - val_specificity: 0.6585 - val_gmeasure: 0.8003 - val_auc: 0.9253\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2451 - binary_accuracy: 0.8983 - sensitivity: 0.9336 - specificity: 0.8036 - gmeasure: 0.8655 - auc: 0.9557 - val_loss: 0.2967 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9633 - val_specificity: 0.6829 - val_gmeasure: 0.8111 - val_auc: 0.9255\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.2435 - binary_accuracy: 0.9000 - sensitivity: 0.9428 - specificity: 0.7864 - gmeasure: 0.8609 - auc: 0.9553 - val_loss: 0.3079 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9725 - val_specificity: 0.6585 - val_gmeasure: 0.8003 - val_auc: 0.9266\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.2444 - binary_accuracy: 0.9017 - sensitivity: 0.9550 - specificity: 0.7612 - gmeasure: 0.8515 - auc: 0.9560 - val_loss: 0.2948 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9725 - val_specificity: 0.6829 - val_gmeasure: 0.8149 - val_auc: 0.9266\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2434 - binary_accuracy: 0.9033 - sensitivity: 0.9434 - specificity: 0.8045 - gmeasure: 0.8700 - auc: 0.9574 - val_loss: 0.3012 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9725 - val_specificity: 0.6585 - val_gmeasure: 0.8003 - val_auc: 0.9275\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2396 - binary_accuracy: 0.9067 - sensitivity: 0.9470 - specificity: 0.7998 - gmeasure: 0.8702 - auc: 0.9551 - val_loss: 0.2955 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9725 - val_specificity: 0.6585 - val_gmeasure: 0.8003 - val_auc: 0.9279\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2392 - binary_accuracy: 0.9017 - sensitivity: 0.9408 - specificity: 0.7970 - gmeasure: 0.8658 - auc: 0.9565 - val_loss: 0.3018 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9725 - val_specificity: 0.6585 - val_gmeasure: 0.8003 - val_auc: 0.9279\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2380 - binary_accuracy: 0.9083 - sensitivity: 0.9514 - specificity: 0.7949 - gmeasure: 0.8695 - auc: 0.9558 - val_loss: 0.2977 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9725 - val_specificity: 0.6585 - val_gmeasure: 0.8003 - val_auc: 0.9275\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2390 - binary_accuracy: 0.9083 - sensitivity: 0.9547 - specificity: 0.7927 - gmeasure: 0.8692 - auc: 0.9589 - val_loss: 0.2940 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9725 - val_specificity: 0.6829 - val_gmeasure: 0.8149 - val_auc: 0.9279\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2492 - binary_accuracy: 0.8850 - sensitivity: 0.9105 - specificity: 0.8279 - gmeasure: 0.8671 - auc: 0.9602 - val_loss: 0.2970 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9725 - val_specificity: 0.6585 - val_gmeasure: 0.8003 - val_auc: 0.9282\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2415 - binary_accuracy: 0.9050 - sensitivity: 0.9636 - specificity: 0.7440 - gmeasure: 0.8462 - auc: 0.9580 - val_loss: 0.3260 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9817 - val_specificity: 0.6098 - val_gmeasure: 0.7737 - val_auc: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:143] Training end with time 5.811546802520752!\n",
      "[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_1.h5 \n",
      "[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_1.h5\n",
      "[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_1.json\n",
      "[root    |INFO|build_network.py:169] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "750/750 [==============================] - 0s 7us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:174] Evaluation end with time 0.010724306106567383!\n",
      "[root    |INFO|build_network.py:175] Evaluation: [0.2627646028995514, 0.8933333158493042, 0.9688644409179688, 0.6911764740943909, 0.8183253407478333, 0.9525425434112549]\n",
      "[root    |INFO|build_network.py:169] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 14us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:174] Evaluation end with time 0.010526657104492188!\n",
      "[root    |INFO|build_network.py:175] Evaluation: [0.37938588857650757, 0.8519999980926514, 0.9482758641242981, 0.6315789222717285, 0.773893415927887, 0.9102767705917358]\n",
      "[root    |INFO|deepbiome.py:179] Compute time : 7.311078310012817\n",
      "[root    |INFO|deepbiome.py:180] 2 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:137] -------3 simulation start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:148] Build network for 3 simulation\n",
      "[root    |INFO|build_network.py:505] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:506] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:510] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:511] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:518]      Genus: 48\n",
      "[root    |INFO|build_network.py:518]     Family: 40\n",
      "[root    |INFO|build_network.py:518]      Order: 23\n",
      "[root    |INFO|build_network.py:518]      Class: 17\n",
      "[root    |INFO|build_network.py:518]     Phylum: 9\n",
      "[root    |INFO|build_network.py:521] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:522] Phylogenetic_tree_dict info: ['Family', 'Class', 'Phylum', 'Number', 'Genus', 'Order']\n",
      "[root    |INFO|build_network.py:523] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:546] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:562] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:563] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:564] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:640] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|build_network.py:83] Load trained model weight at ./example_result/weight_2.h5 \n",
      "[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:158] 3 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:133] Training start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 0s 634us/step - loss: 0.6207 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5188 - val_loss: 0.6475 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5278\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5340 - val_loss: 0.6479 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5234\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.6208 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5145 - val_loss: 0.6476 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5487\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5106 - val_loss: 0.6479 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5321\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5203 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5422\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5182 - val_loss: 0.6486 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5239\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5210 - val_loss: 0.6485 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5199\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5215 - val_loss: 0.6486 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5349\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5178 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5280\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5208 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5505\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5291 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5469\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5321 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5371\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5280 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5263\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5281 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5376\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5225 - val_loss: 0.6485 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5370\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5296 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5236\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5187 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5584\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5172 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5557\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5233 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5202\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5254 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5219\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5258 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5562\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5271 - val_loss: 0.6479 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5520\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5149 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5314\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5223 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5210 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5596\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5251 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5287\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5204 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5195\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5248 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5263\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5316 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5084\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5244 - val_loss: 0.6478 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5524\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5183 - val_loss: 0.6479 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5484\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5210 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5040\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5226 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5315\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5290 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5445\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5259 - val_loss: 0.6485 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5564\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5334 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5713\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5252 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5155\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5274 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5460\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5412 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5287\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5261 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5314\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5229 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5322\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5201 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5217\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5342 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5260\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5265 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5318\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5183 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5381\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5285 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5063\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5245 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5064\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5173 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5272 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5236\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5408 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5299\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5219 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5243\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5340 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5407\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5240 - val_loss: 0.6478 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5271\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5264 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5301\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5229 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5253\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5190 - val_loss: 0.6484 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5508\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5269 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5236\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5260 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5464\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5216 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5217\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5213 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5270\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5199 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5350\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5289 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5198\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5186 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5286\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5262 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5217\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5324 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5374\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5302 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5258\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5203 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5286\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5237 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5362\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5398 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5253\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5282 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5480\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5239 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5139\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5292 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5337 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5354\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5268 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5123\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5308 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5293\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5238 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5142\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5292 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5349\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 0s 58us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5318 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5315\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 0s 58us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5230 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5192\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5258 - val_loss: 0.6485 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5292\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5255 - val_loss: 0.6485 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5258\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5231 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5296\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5237 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5510\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5324 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5312\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5241 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5234\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5280 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5212\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5263 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5215\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5266 - val_loss: 0.6479 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5167\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5243 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5481\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5293 - val_loss: 0.6479 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5350\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 0s 71us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5232 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5356\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5300 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5207\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5305 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5294\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5319 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5253\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5269 - val_loss: 0.6482 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5181\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5268 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5274 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5347\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5290 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5260\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5270 - val_loss: 0.6481 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5357\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5243 - val_loss: 0.6480 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:143] Training end with time 5.78258204460144!\n",
      "[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_2.h5 \n",
      "[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_2.h5\n",
      "[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_2.json\n",
      "[root    |INFO|build_network.py:169] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "750/750 [==============================] - 0s 5us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:174] Evaluation end with time 0.01048731803894043!\n",
      "[root    |INFO|build_network.py:175] Evaluation: [0.625942587852478, 0.6813333630561829, 1.0, 0.0, 0.0, 0.5268486738204956]\n",
      "[root    |INFO|build_network.py:169] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 16us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:174] Evaluation end with time 0.011304616928100586!\n",
      "[root    |INFO|build_network.py:175] Evaluation: [0.6017928123474121, 0.7120000123977661, 1.0, 0.0, 0.0, 0.5361267328262329]\n",
      "[root    |INFO|deepbiome.py:179] Compute time : 7.283475637435913\n",
      "[root    |INFO|deepbiome.py:180] 3 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:183] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:185] Train Evaluation : ['loss' 'binary_accuracy' 'sensitivity' 'specificity' 'gmeasure' 'auc']\n",
      "[root    |INFO|deepbiome.py:188]       mean : [0.34843934 0.83955556 0.96902947 0.54935767 0.58861319 0.8201369 ]\n",
      "[root    |INFO|deepbiome.py:189]        std : [0.20095306 0.11377603 0.02522023 0.40331723 0.41954067 0.2077117 ]\n",
      "[root    |INFO|deepbiome.py:190] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:192] Test Evaluation : ['loss' 'binary_accuracy' 'sensitivity' 'specificity' 'gmeasure' 'auc']\n",
      "[root    |INFO|deepbiome.py:195]       mean : [0.37672624 0.83733334 0.9728967  0.51093783 0.56969279 0.8105734 ]\n",
      "[root    |INFO|deepbiome.py:196]        std : [0.18486145 0.09690316 0.02118913 0.37768738 0.40817982 0.19646624]\n",
      "[root    |INFO|deepbiome.py:197] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:206] Total Computing Ended\n",
      "[root    |INFO|deepbiome.py:207] -----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_evaluation, train_evaluation, network = deepbiome.deepbiome_train(log, warm_start_network_info, path_info, \n",
    "                                                                       number_of_fold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the history plot again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f35cc0acb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('./%s/hist_0.json' % path_info['model_info']['model_dir'], 'r') as f:\n",
    "    history = json.load(f)\n",
    "    \n",
    "plt.plot(history['val_loss'], label='Validation')\n",
    "plt.plot(history['loss'], label='Training')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load the pre-trained network for testing\n",
    "\n",
    "If you want to test the trained model, you can use the `deepbiome_test` function. If you use the index file, this function provide the evaluation using test index (index set not included in the index file) for each fold. If not, this function provide the evaluation using the whole samples. If `number_of_fold` is setted as `k`, the function will test the model only with first `k` folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_network_info = {\n",
    "    'architecture_info': {\n",
    "        'batch_normalization': 'False',\n",
    "        'drop_out': '0',\n",
    "        'weight_initial': 'glorot_uniform',\n",
    "        'weight_l1_penalty':'0.01',\n",
    "        'weight_decay': 'phylogenetic_tree',\n",
    "    },\n",
    "    'model_info': {\n",
    "        'lr': '0.01',\n",
    "        'decay': '0.001',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': 'binary_accuracy, sensitivity, specificity, gmeasure, auc',\n",
    "        'texa_selection_metrics': 'accuracy, sensitivity, specificity, gmeasure',\n",
    "        'network_class': 'DeepBiomeNetwork',\n",
    "        'optimizer': 'adam',\n",
    "        'reader_class': 'MicroBiomeClassificationReader',\n",
    "        'normalizer': 'normalize_minmax',\n",
    "    },\n",
    "    'test_info': {\n",
    "        'batch_size': 'None'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_path_info = {\n",
    "    'data_info': {\n",
    "        'count_list_path': resource_filename('deepbiome', 'tests/data/gcount_list.csv'),\n",
    "        'count_path': resource_filename('deepbiome', 'tests/data/count'),\n",
    "        'data_path': resource_filename('deepbiome', 'tests/data'),\n",
    "        'idx_path': resource_filename('deepbiome', 'tests/data/classification_idx.csv'),\n",
    "        'tree_info_path': resource_filename('deepbiome', 'tests/data/genus48_dic.csv'),\n",
    "        'x_path': '',\n",
    "        'y_path': 'classification_y.csv'\n",
    "    },\n",
    "    'model_info': {\n",
    "        'evaluation': 'eval.npy',\n",
    "        'model_dir': './example_result/',\n",
    "        'weight': 'weight.h5'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|deepbiome.py:262] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:294] Test Evaluation : ['loss' 'binary_accuracy' 'sensitivity' 'specificity' 'gmeasure' 'auc']\n",
      "[root    |INFO|deepbiome.py:296] -------1 fold test start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:306] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:307] Build network for 1 fold testing\n",
      "[root    |INFO|build_network.py:505] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:506] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:510] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:511] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:518]      Genus: 48\n",
      "[root    |INFO|build_network.py:518]     Family: 40\n",
      "[root    |INFO|build_network.py:518]      Order: 23\n",
      "[root    |INFO|build_network.py:518]      Class: 17\n",
      "[root    |INFO|build_network.py:518]     Phylum: 9\n",
      "[root    |INFO|build_network.py:521] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:522] Phylogenetic_tree_dict info: ['Family', 'Class', 'Phylum', 'Number', 'Genus', 'Order']\n",
      "[root    |INFO|build_network.py:523] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:546] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:562] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:563] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:564] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:640] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|deepbiome.py:316] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:317] 1 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:169] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 454us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:174] Evaluation end with time 0.3063983917236328!\n",
      "[root    |INFO|build_network.py:175] Evaluation: [0.14900001883506775, 0.9480000138282776, 0.9704142212867737, 0.9012345671653748, 0.9351849555969238, 0.9853166937828064]\n",
      "[root    |INFO|deepbiome.py:320] \n",
      "[root    |INFO|deepbiome.py:322] Compute time : 1.6649487018585205\n",
      "[root    |INFO|deepbiome.py:323] 1 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:296] -------2 fold test start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:306] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:307] Build network for 2 fold testing\n",
      "[root    |INFO|build_network.py:505] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:506] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:510] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:511] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:518]      Genus: 48\n",
      "[root    |INFO|build_network.py:518]     Family: 40\n",
      "[root    |INFO|build_network.py:518]      Order: 23\n",
      "[root    |INFO|build_network.py:518]      Class: 17\n",
      "[root    |INFO|build_network.py:518]     Phylum: 9\n",
      "[root    |INFO|build_network.py:521] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:522] Phylogenetic_tree_dict info: ['Family', 'Class', 'Phylum', 'Number', 'Genus', 'Order']\n",
      "[root    |INFO|build_network.py:523] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:546] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:562] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:563] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:564] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:640] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|deepbiome.py:316] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:317] 2 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:169] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 270us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:174] Evaluation end with time 0.1646728515625!\n",
      "[root    |INFO|build_network.py:175] Evaluation: [0.37938588857650757, 0.8519999980926514, 0.9482758641242981, 0.6315789222717285, 0.773893415927887, 0.9102767705917358]\n",
      "[root    |INFO|deepbiome.py:320] \n",
      "[root    |INFO|deepbiome.py:322] Compute time : 1.3771986961364746\n",
      "[root    |INFO|deepbiome.py:323] 2 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:296] -------3 fold test start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:306] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:307] Build network for 3 fold testing\n",
      "[root    |INFO|build_network.py:505] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:506] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:510] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:511] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:518]      Genus: 48\n",
      "[root    |INFO|build_network.py:518]     Family: 40\n",
      "[root    |INFO|build_network.py:518]      Order: 23\n",
      "[root    |INFO|build_network.py:518]      Class: 17\n",
      "[root    |INFO|build_network.py:518]     Phylum: 9\n",
      "[root    |INFO|build_network.py:521] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:522] Phylogenetic_tree_dict info: ['Family', 'Class', 'Phylum', 'Number', 'Genus', 'Order']\n",
      "[root    |INFO|build_network.py:523] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:546] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:562] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:563] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:564] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:640] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|deepbiome.py:316] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:317] 3 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:169] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 335us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:174] Evaluation end with time 0.23140716552734375!\n",
      "[root    |INFO|build_network.py:175] Evaluation: [0.6017928123474121, 0.7120000123977661, 1.0, 0.0, 0.0, 0.5361267328262329]\n",
      "[root    |INFO|deepbiome.py:320] \n",
      "[root    |INFO|deepbiome.py:322] Compute time : 1.4677846431732178\n",
      "[root    |INFO|deepbiome.py:323] 3 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:326] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:328] Test Evaluation : ['loss' 'binary_accuracy' 'sensitivity' 'specificity' 'gmeasure' 'auc']\n",
      "[root    |INFO|deepbiome.py:331]       mean : [0.37672624 0.83733334 0.9728967  0.51093783 0.56969279 0.8105734 ]\n",
      "[root    |INFO|deepbiome.py:332]        std : [0.18486145 0.09690316 0.02118913 0.37768738 0.40817982 0.19646624]\n",
      "[root    |INFO|deepbiome.py:333] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:336] Total Computing Ended\n",
      "[root    |INFO|deepbiome.py:337] -----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluation = deepbiome.deepbiome_test(log, test_network_info, test_path_info, number_of_fold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function provides the evaluation result as a numpy array with a shape of (number of fold, number of evaluation measures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  loss binary_accuracy     sensitivity     specificity        gmeasure             auc\n",
      "Mean:           0.3767          0.8373          0.9729          0.5109          0.5697          0.8106\n",
      "Std :           0.1849          0.0969          0.0212          0.3777          0.4082          0.1965\n"
     ]
    }
   ],
   "source": [
    "print('      %s' % ''.join(['%16s'%'loss']+ ['%16s'%s.strip() for s in network_info['model_info']['metrics'].split(',')]))\n",
    "print('Mean: %s' % ''.join(['%16.4f'%v for v in np.mean(evaluation, axis=0)]))\n",
    "print('Std : %s' % ''.join(['%16.4f'%v for v in np.std(evaluation, axis=0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load the pre-trained network for prediction\n",
    "\n",
    "For prediction using the pre-trained model, we can use the `deepbiome_prediction` function. If `number_of_fold` is set to `k`, the function will predict only with first `k` folds sample's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_network_info = {\n",
    "    'architecture_info': {\n",
    "        'batch_normalization': 'False',\n",
    "        'drop_out': '0',\n",
    "        'weight_initial': 'glorot_uniform',\n",
    "        'weight_l1_penalty':'0.01',\n",
    "        'weight_decay': 'phylogenetic_tree',\n",
    "    },\n",
    "    'model_info': {\n",
    "        'decay': '0.001',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'lr': '0.01',\n",
    "        'metrics': 'binary_accuracy, sensitivity, specificity, gmeasure, auc',\n",
    "        'network_class': 'DeepBiomeNetwork',\n",
    "        'normalizer': 'normalize_minmax',\n",
    "        'optimizer': 'adam',\n",
    "        'reader_class': 'MicroBiomeClassificationReader',\n",
    "        'taxa_selection_metrics': 'accuracy, sensitivity, specificity, gmeasure'\n",
    "    },\n",
    "    'test_info': {\n",
    "        'batch_size': 'None'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_path_info = {\n",
    "    'data_info': {\n",
    "        'count_list_path': resource_filename('deepbiome', 'tests/data/gcount_list.csv'),\n",
    "        'count_path': resource_filename('deepbiome', 'tests/data/count'),\n",
    "        'data_path': resource_filename('deepbiome', 'tests/data'),\n",
    "        'tree_info_path': resource_filename('deepbiome', 'tests/data/genus48_dic.csv'),\n",
    "        'x_path': '',\n",
    "    },\n",
    "    'model_info': {\n",
    "        'model_dir': './example_result/',\n",
    "        'weight': 'weight_0.h5'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|deepbiome.py:393] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:420] -------1 th repeatition prediction start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:429] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:430] Build network for 1 fold testing\n",
      "[root    |INFO|build_network.py:505] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:506] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:510] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:511] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:518]      Genus: 48\n",
      "[root    |INFO|build_network.py:518]     Family: 40\n",
      "[root    |INFO|build_network.py:518]      Order: 23\n",
      "[root    |INFO|build_network.py:518]      Class: 17\n",
      "[root    |INFO|build_network.py:518]     Phylum: 9\n",
      "[root    |INFO|build_network.py:521] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:522] Phylogenetic_tree_dict info: ['Family', 'Class', 'Phylum', 'Number', 'Genus', 'Order']\n",
      "[root    |INFO|build_network.py:523] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:546] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:562] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:563] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:564] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:640] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|deepbiome.py:440] -----------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:189] Prediction start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1000/1000 [==============================] - 0s 36us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:194] Prediction end with time 0.03861713409423828!\n",
      "[root    |INFO|deepbiome.py:444] Compute time : 0.978487491607666\n",
      "[root    |INFO|deepbiome.py:445] 1 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:420] -------2 th repeatition prediction start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:429] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:430] Build network for 2 fold testing\n",
      "[root    |INFO|build_network.py:505] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:506] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:510] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:511] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:518]      Genus: 48\n",
      "[root    |INFO|build_network.py:518]     Family: 40\n",
      "[root    |INFO|build_network.py:518]      Order: 23\n",
      "[root    |INFO|build_network.py:518]      Class: 17\n",
      "[root    |INFO|build_network.py:518]     Phylum: 9\n",
      "[root    |INFO|build_network.py:521] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:522] Phylogenetic_tree_dict info: ['Family', 'Class', 'Phylum', 'Number', 'Genus', 'Order']\n",
      "[root    |INFO|build_network.py:523] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:546] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:562] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:563] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:564] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:640] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|deepbiome.py:440] -----------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:189] Prediction start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1000/1000 [==============================] - 0s 42us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:194] Prediction end with time 0.04507732391357422!\n",
      "[root    |INFO|deepbiome.py:444] Compute time : 1.0234794616699219\n",
      "[root    |INFO|deepbiome.py:445] 2 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:420] -------3 th repeatition prediction start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:429] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:430] Build network for 3 fold testing\n",
      "[root    |INFO|build_network.py:505] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:506] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:510] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:511] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:518]      Genus: 48\n",
      "[root    |INFO|build_network.py:518]     Family: 40\n",
      "[root    |INFO|build_network.py:518]      Order: 23\n",
      "[root    |INFO|build_network.py:518]      Class: 17\n",
      "[root    |INFO|build_network.py:518]     Phylum: 9\n",
      "[root    |INFO|build_network.py:521] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:522] Phylogenetic_tree_dict info: ['Family', 'Class', 'Phylum', 'Number', 'Genus', 'Order']\n",
      "[root    |INFO|build_network.py:523] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:533] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:546] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:562] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:563] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:564] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:640] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|deepbiome.py:440] -----------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:189] Prediction start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1000/1000 [==============================] - 0s 45us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:194] Prediction end with time 0.04752993583679199!\n",
      "[root    |INFO|deepbiome.py:444] Compute time : 1.0356991291046143\n",
      "[root    |INFO|deepbiome.py:445] 3 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:449] Total Computing Ended\n",
      "[root    |INFO|deepbiome.py:450] -----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prediction = deepbiome.deepbiome_prediction(log, prediction_network_info, prediction_path_info,\n",
    "                                            num_classes = 1, number_of_fold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1000, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9580059 ],\n",
       "       [0.91303825],\n",
       "       [0.07534188],\n",
       "       [0.10720441],\n",
       "       [1.        ],\n",
       "       [0.93563545],\n",
       "       [0.08312383],\n",
       "       [0.11105517],\n",
       "       [0.9999993 ],\n",
       "       [0.99975   ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0,:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
